{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nn_methods_hw2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZt5vQXWw6WU"
      },
      "source": [
        "Петухова Ксения БКЛ182"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQNxyRkXws62",
        "outputId": "bc3819fe-3291-4675-ad7e-7d3a1eaa155f"
      },
      "source": [
        "!wget https://www.dropbox.com/s/r6u59ljhhjdg6j0/negative.csv\n",
        "!wget https://www.dropbox.com/s/fnpq3z4bcnoktiv/positive.csv "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-11-27 23:35:36--  https://www.dropbox.com/s/r6u59ljhhjdg6j0/negative.csv\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.3.18, 2620:100:601b:18::a27d:812\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.3.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/r6u59ljhhjdg6j0/negative.csv [following]\n",
            "--2021-11-27 23:35:36--  https://www.dropbox.com/s/raw/r6u59ljhhjdg6j0/negative.csv\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 404 Not Found\n",
            "2021-11-27 23:35:36 ERROR 404: Not Found.\n",
            "\n",
            "--2021-11-27 23:35:36--  https://www.dropbox.com/s/fnpq3z4bcnoktiv/positive.csv\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.3.18, 2620:100:6018:18::a27d:312\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.3.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/fnpq3z4bcnoktiv/positive.csv [following]\n",
            "--2021-11-27 23:35:36--  https://www.dropbox.com/s/raw/fnpq3z4bcnoktiv/positive.csv\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 404 Not Found\n",
            "2021-11-27 23:35:36 ERROR 404: Not Found.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5N5lcLOsxsGY",
        "outputId": "9015e48e-bffa-4d30-f24e-04d4d376c27c"
      },
      "source": [
        "!pip3 install torchmetrics"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-0.6.0-py3-none-any.whl (329 kB)\n",
            "\u001b[?25l\r\u001b[K     |█                               | 10 kB 24.0 MB/s eta 0:00:01\r\u001b[K     |██                              | 20 kB 26.7 MB/s eta 0:00:01\r\u001b[K     |███                             | 30 kB 11.8 MB/s eta 0:00:01\r\u001b[K     |████                            | 40 kB 9.4 MB/s eta 0:00:01\r\u001b[K     |█████                           | 51 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |██████                          | 61 kB 5.8 MB/s eta 0:00:01\r\u001b[K     |███████                         | 71 kB 5.8 MB/s eta 0:00:01\r\u001b[K     |████████                        | 81 kB 6.5 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 92 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 102 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 112 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 122 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 133 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 143 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 153 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 163 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 174 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 184 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 194 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 204 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 215 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 225 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 235 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 245 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 256 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 266 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 276 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 286 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 296 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 307 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 317 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 327 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 329 kB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (21.3)\n",
            "Requirement already satisfied: torch>=1.3.1 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.10.0+cu111)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.3.1->torchmetrics) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->torchmetrics) (3.0.6)\n",
            "Installing collected packages: torchmetrics\n",
            "Successfully installed torchmetrics-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 854
        },
        "id": "dmTEYE3Dx0mk",
        "outputId": "b3d4588b-af7c-40e7-8a7d-6ccb2277e8ed"
      },
      "source": [
        "!pip3 install ipdb"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ipdb\n",
            "  Downloading ipdb-0.13.9.tar.gz (16 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from ipdb) (57.4.0)\n",
            "Collecting ipython>=7.17.0\n",
            "  Downloading ipython-7.30.0-py3-none-any.whl (791 kB)\n",
            "\u001b[K     |████████████████████████████████| 791 kB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: toml>=0.10.2 in /usr/local/lib/python3.7/dist-packages (from ipdb) (0.10.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipdb) (4.4.2)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (0.2.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (0.7.5)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (0.1.3)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (2.6.1)\n",
            "Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0\n",
            "  Downloading prompt_toolkit-3.0.23-py3-none-any.whl (374 kB)\n",
            "\u001b[K     |████████████████████████████████| 374 kB 45.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (0.18.1)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (4.8.0)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (5.1.1)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.16->ipython>=7.17.0->ipdb) (0.8.2)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect>4.3->ipython>=7.17.0->ipdb) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.17.0->ipdb) (0.2.5)\n",
            "Building wheels for collected packages: ipdb\n",
            "  Building wheel for ipdb (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ipdb: filename=ipdb-0.13.9-py3-none-any.whl size=11648 sha256=0b100d77f21d7ab7673b7a06f325489da513f74a6c95c5a346a5d840f9e5f4bb\n",
            "  Stored in directory: /root/.cache/pip/wheels/65/cd/cc/aaf92acae337a28fdd2aa4d632196a59745c8c39f76eaeed01\n",
            "Successfully built ipdb\n",
            "Installing collected packages: prompt-toolkit, ipython, ipdb\n",
            "  Attempting uninstall: prompt-toolkit\n",
            "    Found existing installation: prompt-toolkit 1.0.18\n",
            "    Uninstalling prompt-toolkit-1.0.18:\n",
            "      Successfully uninstalled prompt-toolkit-1.0.18\n",
            "  Attempting uninstall: ipython\n",
            "    Found existing installation: ipython 5.5.0\n",
            "    Uninstalling ipython-5.5.0:\n",
            "      Successfully uninstalled ipython-5.5.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "jupyter-console 5.2.0 requires prompt-toolkit<2.0.0,>=1.0.0, but you have prompt-toolkit 3.0.23 which is incompatible.\n",
            "google-colab 1.0.0 requires ipython~=5.5.0, but you have ipython 7.30.0 which is incompatible.\u001b[0m\n",
            "Successfully installed ipdb-0.13.9 ipython-7.30.0 prompt-toolkit-3.0.23\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "IPython",
                  "prompt_toolkit"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GLkUIhKV2z1t",
        "outputId": "2952e3e7-63a1-48bd-f89a-c656a80ec432"
      },
      "source": [
        "!pip3 install pymorphy2"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymorphy2\n",
            "  Downloading pymorphy2-0.9.1-py3-none-any.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 2.5 MB/s \n",
            "\u001b[?25hCollecting pymorphy2-dicts-ru<3.0,>=2.4\n",
            "  Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.2 MB 7.7 MB/s \n",
            "\u001b[?25hCollecting dawg-python>=0.7.1\n",
            "  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.7/dist-packages (from pymorphy2) (0.6.2)\n",
            "Installing collected packages: pymorphy2-dicts-ru, dawg-python, pymorphy2\n",
            "Successfully installed dawg-python-0.7.2 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4mCCKQYxoFw"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from string import punctuation\n",
        "from collections import Counter\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch.optim as optim\n",
        "from torchmetrics import F1\n",
        "from torchmetrics.functional import f1, recall\n",
        "import ipdb\n",
        "import nltk\n",
        "from string import punctuation\n",
        "from tqdm import tqdm\n",
        "import gensim\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmzcobJM2tAf"
      },
      "source": [
        "torch.manual_seed(42)\n",
        "np.random.seed(42)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 324
        },
        "id": "VhOYVI3DyPX2",
        "outputId": "e863b808-930e-4c92-cab6-7ebc81495cac"
      },
      "source": [
        "pos_tweets = pd.read_csv('positive.csv', encoding='utf-8',\n",
        "                         sep=';', header=None,  names=[0,1,2,'text','tone',5,6,7,8,9,10,11])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-bdba17fded76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m pos_tweets = pd.read_csv('positive.csv', encoding='utf-8',\n\u001b[0;32m----> 2\u001b[0;31m                          sep=';', header=None,  names=[0,1,2,'text','tone',5,6,7,8,9,10,11])\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1991\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1992\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1993\u001b[0;31m                 \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1994\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1995\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'positive.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5n7rQKoR6kW2"
      },
      "source": [
        "neg_tweets = pd.read_csv('negative.csv', encoding='utf-8', sep=';', header=None, names=[0,1,2,'text','tone',5,6,7,8,9,10,11] )\n",
        "neg_tweets['tone'] = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJKCTAFB3WlZ"
      },
      "source": [
        "### сайт с датасетом упал, поэтому теперь так:"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Z5TeaEz3bC2"
      },
      "source": [
        "pos_tweets = pd.read_csv('/content/drive/MyDrive/positive.csv', encoding='utf-8',\n",
        "                         sep=';', header=None,  names=[0,1,2,'text','tone',5,6,7,8,9,10,11])"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qu183RFd3iyP"
      },
      "source": [
        "neg_tweets = pd.read_csv('/content/drive/MyDrive/negative.csv', encoding='utf-8', sep=';', header=None, names=[0,1,2,'text','tone',5,6,7,8,9,10,11] )\n",
        "neg_tweets['tone'] = 0"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mqok_6QD6nt0",
        "outputId": "46df8e16-beef-461c-c13b-33639dbed1a4"
      },
      "source": [
        "all_tweets_data = pos_tweets.append(neg_tweets)\n",
        "print(len(all_tweets_data))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "226834\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ay-aTBb_6zr9"
      },
      "source": [
        "tweets_data = shuffle(all_tweets_data[['text','tone']])[:100000]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqey6pvW627M"
      },
      "source": [
        "def preprocess(text):\n",
        "    tokens = text.lower().split()\n",
        "    tokens = [token.strip(punctuation) for token in tokens]\n",
        "    return ' '.join(tokens)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vzh_BcPb8dMV",
        "outputId": "b0dc5c18-bf74-4fcc-938f-0f6e7ba01be8"
      },
      "source": [
        "texts = list(tweets_data.text)\n",
        "preprocessed_texts = [preprocess(text) for text in tqdm(texts)]\n",
        "tweets_data['preprocessed_text'] = preprocessed_texts\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100000/100000 [00:00<00:00, 146653.11it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ij6ap9Ns63EW"
      },
      "source": [
        "train_sentences, val_sentences = train_test_split(tweets_data, test_size=0.1)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "id": "NZWDpFA062_t",
        "outputId": "8a8cc598-265b-488f-f9c1-49fa6f359f7c"
      },
      "source": [
        "train_sentences[:10]"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>tone</th>\n",
              "      <th>preprocessed_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>62759</th>\n",
              "      <td>#нагризлучшая #голос обидно за второе место :(...</td>\n",
              "      <td>0</td>\n",
              "      <td>нагризлучшая голос обидно за второе место  поб...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>61077</th>\n",
              "      <td>сегодня я буду 4 часа пыриться на концерты и к...</td>\n",
              "      <td>1</td>\n",
              "      <td>сегодня я буду 4 часа пыриться на концерты и к...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82338</th>\n",
              "      <td>То что нужно после пар))) #goodys #like #minsk...</td>\n",
              "      <td>1</td>\n",
              "      <td>то что нужно после пар goodys like minsk heine...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>58794</th>\n",
              "      <td>Хоспади, какой же всё-таки неудобный новый инт...</td>\n",
              "      <td>0</td>\n",
              "      <td>хоспади какой же всё-таки неудобный новый инте...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14268</th>\n",
              "      <td>@ichcc @rozhkovsky ну да, я с родителями за ко...</td>\n",
              "      <td>1</td>\n",
              "      <td>ichcc rozhkovsky ну да я с родителями за компанию</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34900</th>\n",
              "      <td>Сегодня специально оделась так,чтобы было прох...</td>\n",
              "      <td>0</td>\n",
              "      <td>сегодня специально оделась так,чтобы было прох...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29686</th>\n",
              "      <td>Еду на работу покопаться в сервере и wi-fi точ...</td>\n",
              "      <td>1</td>\n",
              "      <td>еду на работу покопаться в сервере и wi-fi точ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71607</th>\n",
              "      <td>@yanochka18_96 хотя я не верю в эту дружбу:DD ...</td>\n",
              "      <td>1</td>\n",
              "      <td>yanochka18_96 хотя я не верю в эту дружбу:dd я...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30810</th>\n",
              "      <td>Блин лента... Вы меня не любите, даже писать п...</td>\n",
              "      <td>0</td>\n",
              "      <td>блин лента вы меня не любите даже писать перес...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>81923</th>\n",
              "      <td>Мои любимые носочки))\\nАхаахахх\\nЗамерзла http...</td>\n",
              "      <td>1</td>\n",
              "      <td>мои любимые носочки ахаахахх замерзла http://t...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    text  ...                                  preprocessed_text\n",
              "62759  #нагризлучшая #голос обидно за второе место :(...  ...  нагризлучшая голос обидно за второе место  поб...\n",
              "61077  сегодня я буду 4 часа пыриться на концерты и к...  ...  сегодня я буду 4 часа пыриться на концерты и к...\n",
              "82338  То что нужно после пар))) #goodys #like #minsk...  ...  то что нужно после пар goodys like minsk heine...\n",
              "58794  Хоспади, какой же всё-таки неудобный новый инт...  ...  хоспади какой же всё-таки неудобный новый инте...\n",
              "14268  @ichcc @rozhkovsky ну да, я с родителями за ко...  ...  ichcc rozhkovsky ну да я с родителями за компанию\n",
              "34900  Сегодня специально оделась так,чтобы было прох...  ...  сегодня специально оделась так,чтобы было прох...\n",
              "29686  Еду на работу покопаться в сервере и wi-fi точ...  ...  еду на работу покопаться в сервере и wi-fi точ...\n",
              "71607  @yanochka18_96 хотя я не верю в эту дружбу:DD ...  ...  yanochka18_96 хотя я не верю в эту дружбу:dd я...\n",
              "30810  Блин лента... Вы меня не любите, даже писать п...  ...  блин лента вы меня не любите даже писать перес...\n",
              "81923  Мои любимые носочки))\\nАхаахахх\\nЗамерзла http...  ...  мои любимые носочки ахаахахх замерзла http://t...\n",
              "\n",
              "[10 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5dVVBnf622t",
        "outputId": "98c8a4a6-1282-41bb-f249-88ae3f41f77e"
      },
      "source": [
        "vocab = Counter()\n",
        "\n",
        "for text in tweets_data['preprocessed_text']:\n",
        "    vocab.update(text.split(' '))\n",
        "print('всего уникальных токенов:', len(vocab))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "всего уникальных токенов: 202617\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iw_PvdBO62u6",
        "outputId": "3766f64b-d26b-4ea0-9a28-b694d89c4784"
      },
      "source": [
        "filtered_vocab = set()\n",
        "\n",
        "for word in vocab:\n",
        "    if vocab[word] > 2:\n",
        "        filtered_vocab.add(word)\n",
        "print('уникальных токенов, втретившихся больше 2 раз:', len(filtered_vocab))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "уникальных токенов, втретившихся больше 2 раз: 32481\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-YBcm9v62ql"
      },
      "source": [
        "word2id = {'PAD':0}\n",
        "\n",
        "for word in filtered_vocab:\n",
        "    word2id[word] = len(word2id)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fc4Qgd4u62iN"
      },
      "source": [
        "id2word = {i:word for word, i in word2id.items()}"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XdJGZN_97y3F",
        "outputId": "67a5a138-3c3b-44b2-c910-9c7a3650a0cf"
      },
      "source": [
        "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "DEVICE"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJbUCBig8FxC"
      },
      "source": [
        "### CNN на уровне слов"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4Tt7E_27zYS"
      },
      "source": [
        "class TwitterDataset(Dataset):\n",
        "\n",
        "    def __init__(self, dataset, word2id, DEVICE):\n",
        "        self.dataset = dataset['preprocessed_text'].values\n",
        "        self.word2id = word2id\n",
        "        self.length = dataset.shape[0]\n",
        "        self.target = dataset['tone'].values\n",
        "        self.device = DEVICE\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, index): \n",
        "        words = self.dataset[index].split(' ')\n",
        "        ids = torch.LongTensor([self.word2id[word] for word in words if word in self.word2id])\n",
        "        y = [self.target[index]]\n",
        "        return ids, y\n",
        "\n",
        "    def collate_fn(self, batch): \n",
        "      ids, y = list(zip(*batch))\n",
        "      padded_ids = pad_sequence(ids, batch_first=True).to(self.device)\n",
        "      y = torch.Tensor(y).to(self.device)\n",
        "      return padded_ids, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZ-B0ebKDf3p"
      },
      "source": [
        "train_dataset = TwitterDataset(train_sentences, word2id, DEVICE)\n",
        "train_sampler = RandomSampler(train_dataset)\n",
        "train_iterator = DataLoader(train_dataset, collate_fn = train_dataset.collate_fn, sampler=train_sampler, batch_size=1024)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWto7hj2DmP1"
      },
      "source": [
        "batch = next(iter(train_iterator))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hKUmIJbXD6_X",
        "outputId": "e8a7e398-29da-4725-93c1-f90d56fbf693"
      },
      "source": [
        "batch[0].shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1024, 27])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IG_9GNw-EIad",
        "outputId": "cb7f63f7-30cb-4374-e83e-b3b51c89ff47"
      },
      "source": [
        "[id2word[int(i)] for i in batch[0][0]]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['как',\n",
              " 'говорил',\n",
              " 'самый',\n",
              " 'крутой',\n",
              " 'чувак',\n",
              " 'я',\n",
              " 'самый',\n",
              " 'больной',\n",
              " 'в',\n",
              " 'мире',\n",
              " 'человек',\n",
              " 'и',\n",
              " 'в',\n",
              " 'этот',\n",
              " 'раз',\n",
              " 'я',\n",
              " 'не',\n",
              " 'имею',\n",
              " 'ввиду',\n",
              " 'на',\n",
              " 'голову',\n",
              " 'PAD',\n",
              " 'PAD',\n",
              " 'PAD',\n",
              " 'PAD',\n",
              " 'PAD',\n",
              " 'PAD']"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QuKwNAjmEMYh",
        "outputId": "45c8f3b7-21f9-4da7-b4ff-b86aae4fe0fc"
      },
      "source": [
        "batch[1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        ...,\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [0.]], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cc2oOKbXERWq"
      },
      "source": [
        "val_dataset = TwitterDataset(val_sentences, word2id, DEVICE)\n",
        "val_sampler = SequentialSampler(val_dataset)\n",
        "val_iterator = DataLoader(val_dataset, collate_fn = val_dataset.collate_fn, sampler=val_sampler, batch_size=1024)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tjm0rbI4EghS",
        "outputId": "41f6efb5-b299-492a-84ab-334e38e85631"
      },
      "source": [
        "test_batch = next(iter(val_iterator))\n",
        "test_batch[0].shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1024, 27])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSmxdPLcEjXE",
        "outputId": "e34f0a94-782b-4800-a2d3-dbcdc0399839"
      },
      "source": [
        "%%time\n",
        "w2v = gensim.models.Word2Vec(preprocessed_texts, size=100, window=5, min_count=1)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 35 s, sys: 493 ms, total: 35.4 s\n",
            "Wall time: 20 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Gipb8UiBvBI"
      },
      "source": [
        "weights = np.zeros((len(word2id), 100))\n",
        "count = 0\n",
        "for word, i in word2id.items():\n",
        "    if word == 'PAD':\n",
        "        continue   \n",
        "    try:\n",
        "        weights[i] = w2v.wv[word]    \n",
        "    except KeyError:\n",
        "      count += 1\n",
        "      # oov словам сопоставляем случайный вектор\n",
        "      weights[i] = np.random.normal(0,0.1,100)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0h-uDqbC15Z"
      },
      "source": [
        "class CNN(nn.Module):\n",
        "    \n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.embedding.from_pretrained(torch.tensor(weights), freeze=True)\n",
        "        self.bigrams = nn.Conv1d(in_channels=embedding_dim, out_channels=100, kernel_size=2, padding='same')\n",
        "        self.trigrams = nn.Conv1d(in_channels=embedding_dim, out_channels=80, kernel_size=3, padding='same')\n",
        "        self.concatenated = nn.Conv1d(in_channels=180, out_channels=180, kernel_size=2, padding='same')\n",
        "        self.hidden = nn.Linear(in_features=180, out_features=1)\n",
        "        self.out = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, word):\n",
        "        #batch_size x seq_len\n",
        "        embedded = self.embedding(word)\n",
        "        #batch_size x seq_len x embedding_dim\n",
        "        embedded = embedded.transpose(1,2)\n",
        "        #batch_size x embedding_dim x seq_len\n",
        "        feature_map_bigrams = self.bigrams(embedded)\n",
        "        #batch_size x filter_count2 x seq_len* \n",
        "        feature_map_trigrams = self.trigrams(embedded)\n",
        "        #batch_size x filter_count3 x seq_len*\n",
        "        concat_ngrams = torch.cat((feature_map_bigrams, feature_map_trigrams), 1)\n",
        "        \n",
        "        final_layer = self.concatenated(concat_ngrams)\n",
        "\n",
        "\n",
        "        pooling_final = final_layer.max(2)[0] \n",
        "        logits = self.hidden(pooling_final) \n",
        "        logits = self.out(logits)      \n",
        "        return logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7AdzYDHPr88"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "    epoch_loss = 0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for i, (texts, ys) in enumerate(iterator): \n",
        "        optimizer.zero_grad()\n",
        "        preds = model(texts)  \n",
        "        loss = criterion(preds, ys)  \n",
        "        loss.backward() \n",
        "        optimizer.step() \n",
        "        epoch_loss += loss.item()\n",
        "        if not (i + 1) % int(len(iterator)/5):\n",
        "            print(f'Train loss: {epoch_loss/i}')      \n",
        "    return  epoch_loss / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wSEuWPvT2L-"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    epoch_loss = 0\n",
        "    epoch_metric = 0\n",
        "    model.eval() \n",
        "    with torch.no_grad():\n",
        "        for i, (texts, ys) in enumerate(iterator):   \n",
        "            preds = model(texts)\n",
        "            loss = criterion(preds, ys)\n",
        "            epoch_loss += loss.item()\n",
        "            batch_metric = f1(preds.round().long(), ys.long(), ignore_index=0)\n",
        "            epoch_metric += batch_metric\n",
        "\n",
        "            if not (i + 1) % int(len(iterator)/5):\n",
        "              print(f'Val loss: {epoch_loss/i}, Val f1: {epoch_metric/i}')\n",
        "        \n",
        "    return epoch_metric / len(iterator), epoch_loss / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HC2yvqe0UFeM"
      },
      "source": [
        "model = CNN(len(word2id), 8)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
        "criterion = nn.BCELoss()  \n",
        "\n",
        "# веса модели и значения лосса храним там же, где и все остальные тензоры\n",
        "model = model.to(DEVICE)\n",
        "criterion = criterion.to(DEVICE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SMbm45gDVWvm",
        "outputId": "704d373a-7ddd-4b06-a3ad-7c8829c32dfc"
      },
      "source": [
        "losses = []\n",
        "losses_eval = []\n",
        "f1s = []\n",
        "f1s_eval = []\n",
        "\n",
        "for i in range(20):\n",
        "    print(f'\\nstarting Epoch {i}')\n",
        "    print('Training...')\n",
        "    epoch_loss = train(model, train_iterator, optimizer, criterion)\n",
        "    losses.append(epoch_loss)\n",
        "    print('\\nEvaluating on train...')\n",
        "    f1_on_train,_ = evaluate(model, train_iterator, criterion)\n",
        "    f1s.append(f1_on_train)\n",
        "    print('\\nEvaluating on test...')\n",
        "    f1_on_test, epoch_loss_on_test = evaluate(model, val_iterator, criterion)\n",
        "    losses_eval.append(epoch_loss_on_test)\n",
        "    f1s_eval.append(f1_on_test)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "starting Epoch 0\n",
            "Training...\n",
            "Train loss: 0.7347741425037384\n",
            "Train loss: 0.7059568130608761\n",
            "Train loss: 0.6944504249095916\n",
            "Train loss: 0.6875377003826312\n",
            "Train loss: 0.6826833004043216\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.7022149227559566, Val f1: 0.6004847288131714\n",
            "Val loss: 0.6792925903291414, Val f1: 0.5890041589736938\n",
            "Val loss: 0.6724821412563324, Val f1: 0.5825263261795044\n",
            "Val loss: 0.6693035195122904, Val f1: 0.5783787369728088\n",
            "Val loss: 0.6676760826792035, Val f1: 0.575772225856781\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.325424075126648, Val f1: 1.1345562934875488\n",
            "Val loss: 0.8829265634218851, Val f1: 0.760692298412323\n",
            "Val loss: 0.7950083017349243, Val f1: 0.6795694231987\n",
            "Val loss: 0.7554538079670498, Val f1: 0.6519906520843506\n",
            "Val loss: 0.7350514729817709, Val f1: 0.6334011554718018\n",
            "\n",
            "starting Epoch 1\n",
            "Training...\n",
            "Train loss: 0.6970260329544544\n",
            "Train loss: 0.6741145253181458\n",
            "Train loss: 0.6676842892169952\n",
            "Train loss: 0.6648557586456413\n",
            "Train loss: 0.6622964732703709\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.6912657879292965, Val f1: 0.5841040015220642\n",
            "Val loss: 0.6688277035048513, Val f1: 0.5699830055236816\n",
            "Val loss: 0.6614472186565399, Val f1: 0.5661891102790833\n",
            "Val loss: 0.658429886867751, Val f1: 0.5622788667678833\n",
            "Val loss: 0.6555566787719727, Val f1: 0.5622289776802063\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.3063071966171265, Val f1: 1.1164307594299316\n",
            "Val loss: 0.8701896270116171, Val f1: 0.7376696467399597\n",
            "Val loss: 0.7847814798355103, Val f1: 0.6574401259422302\n",
            "Val loss: 0.7450782826968602, Val f1: 0.6315015554428101\n",
            "Val loss: 0.7253973152902391, Val f1: 0.610739529132843\n",
            "\n",
            "starting Epoch 2\n",
            "Training...\n",
            "Train loss: 0.6855983659625053\n",
            "Train loss: 0.6629899017738573\n",
            "Train loss: 0.6548143434524536\n",
            "Train loss: 0.6514680794815519\n",
            "Train loss: 0.6492305944363276\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.6731486357748508, Val f1: 0.7000806331634521\n",
            "Val loss: 0.6522870876572349, Val f1: 0.6767451167106628\n",
            "Val loss: 0.645311313867569, Val f1: 0.6708083748817444\n",
            "Val loss: 0.6416008739329097, Val f1: 0.6671209335327148\n",
            "Val loss: 0.6393736955665407, Val f1: 0.6658751368522644\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.287194550037384, Val f1: 1.255751371383667\n",
            "Val loss: 0.8563271760940552, Val f1: 0.8469663858413696\n",
            "Val loss: 0.7720662355422974, Val f1: 0.7659317255020142\n",
            "Val loss: 0.7328415513038635, Val f1: 0.7350360751152039\n",
            "Val loss: 0.7135032150480483, Val f1: 0.7148502469062805\n",
            "\n",
            "starting Epoch 3\n",
            "Training...\n",
            "Train loss: 0.6714929416775703\n",
            "Train loss: 0.6499835632064126\n",
            "Train loss: 0.6427720439434051\n",
            "Train loss: 0.6390242799004512\n",
            "Train loss: 0.6380136098180499\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.6584623865783215, Val f1: 0.6917247176170349\n",
            "Val loss: 0.6385081175601843, Val f1: 0.6725872159004211\n",
            "Val loss: 0.632061333656311, Val f1: 0.6647816300392151\n",
            "Val loss: 0.6284473333785783, Val f1: 0.6622695922851562\n",
            "Val loss: 0.627296195143745, Val f1: 0.6590999364852905\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.2705960273742676, Val f1: 1.2548918724060059\n",
            "Val loss: 0.8432677586873373, Val f1: 0.8369501829147339\n",
            "Val loss: 0.7618582010269165, Val f1: 0.7499668002128601\n",
            "Val loss: 0.7226740206990924, Val f1: 0.7250956892967224\n",
            "Val loss: 0.7039393252796597, Val f1: 0.7014820575714111\n",
            "\n",
            "starting Epoch 4\n",
            "Training...\n",
            "Train loss: 0.6587889492511749\n",
            "Train loss: 0.6425268361062715\n",
            "Train loss: 0.6344891786575317\n",
            "Train loss: 0.6307330256077781\n",
            "Train loss: 0.6273222310202462\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.6464210152626038, Val f1: 0.6598584651947021\n",
            "Val loss: 0.6263203115174265, Val f1: 0.6417009234428406\n",
            "Val loss: 0.6229169142246246, Val f1: 0.6309402585029602\n",
            "Val loss: 0.6209814655247019, Val f1: 0.6268837451934814\n",
            "Val loss: 0.6196723878383636, Val f1: 0.6236745119094849\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.2574539184570312, Val f1: 1.1846461296081543\n",
            "Val loss: 0.8336778879165649, Val f1: 0.7940592765808105\n",
            "Val loss: 0.755016815662384, Val f1: 0.7112294435501099\n",
            "Val loss: 0.7162706426211766, Val f1: 0.6822980046272278\n",
            "Val loss: 0.6981396277745565, Val f1: 0.6616838574409485\n",
            "\n",
            "starting Epoch 5\n",
            "Training...\n",
            "Train loss: 0.6507520787417889\n",
            "Train loss: 0.6297705245740486\n",
            "Train loss: 0.6228803074359894\n",
            "Train loss: 0.6183199677894364\n",
            "Train loss: 0.6150504946708679\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.6346589736640453, Val f1: 0.7276017665863037\n",
            "Val loss: 0.6146290374524666, Val f1: 0.7053232192993164\n",
            "Val loss: 0.6084675419330597, Val f1: 0.6975806355476379\n",
            "Val loss: 0.6068952697426525, Val f1: 0.691877007484436\n",
            "Val loss: 0.6050046270801908, Val f1: 0.6897460222244263\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.252686619758606, Val f1: 1.2849979400634766\n",
            "Val loss: 0.8277791738510132, Val f1: 0.8646353483200073\n",
            "Val loss: 0.7486074805259705, Val f1: 0.7807299494743347\n",
            "Val loss: 0.7099992292267936, Val f1: 0.7501593828201294\n",
            "Val loss: 0.6921831899219089, Val f1: 0.7270147204399109\n",
            "\n",
            "starting Epoch 6\n",
            "Training...\n",
            "Train loss: 0.6359788961708546\n",
            "Train loss: 0.6160551652763829\n",
            "Train loss: 0.6110282146930694\n",
            "Train loss: 0.6065291833521714\n",
            "Train loss: 0.6045333111569995\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.6215341575443745, Val f1: 0.7403388619422913\n",
            "Val loss: 0.603483113375577, Val f1: 0.7132409811019897\n",
            "Val loss: 0.5981556892395019, Val f1: 0.7045417428016663\n",
            "Val loss: 0.5956499914624798, Val f1: 0.6998024582862854\n",
            "Val loss: 0.5940170266798565, Val f1: 0.6985017657279968\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.237673580646515, Val f1: 1.3049170970916748\n",
            "Val loss: 0.8163662155469259, Val f1: 0.8749592304229736\n",
            "Val loss: 0.7388661503791809, Val f1: 0.7870398163795471\n",
            "Val loss: 0.7009472336087909, Val f1: 0.7576509714126587\n",
            "Val loss: 0.6833638019031949, Val f1: 0.7335439920425415\n",
            "\n",
            "starting Epoch 7\n",
            "Training...\n",
            "Train loss: 0.6238651759922504\n",
            "Train loss: 0.6058521487496116\n",
            "Train loss: 0.5994735431671142\n",
            "Train loss: 0.5969303552784136\n",
            "Train loss: 0.5945776800314585\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.6147084720432758, Val f1: 0.7222042679786682\n",
            "Val loss: 0.5924571210687811, Val f1: 0.7039648294448853\n",
            "Val loss: 0.5862348997592925, Val f1: 0.6982875466346741\n",
            "Val loss: 0.5846286522808359, Val f1: 0.6924501657485962\n",
            "Val loss: 0.5832193309352511, Val f1: 0.689408540725708\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.2197991013526917, Val f1: 1.268705129623413\n",
            "Val loss: 0.8050974210103353, Val f1: 0.8588448762893677\n",
            "Val loss: 0.729715359210968, Val f1: 0.7718273401260376\n",
            "Val loss: 0.6919071418898446, Val f1: 0.7402857542037964\n",
            "Val loss: 0.6758253905508254, Val f1: 0.7154377102851868\n",
            "\n",
            "starting Epoch 8\n",
            "Training...\n",
            "Train loss: 0.6163095198571682\n",
            "Train loss: 0.5953152540958289\n",
            "Train loss: 0.5875272417068481\n",
            "Train loss: 0.5863264016251066\n",
            "Train loss: 0.5842956801255544\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.6003709509968758, Val f1: 0.7420386075973511\n",
            "Val loss: 0.5806277430418766, Val f1: 0.7192399501800537\n",
            "Val loss: 0.5760144245624542, Val f1: 0.7100669741630554\n",
            "Val loss: 0.574489386224035, Val f1: 0.7056582570075989\n",
            "Val loss: 0.5729463639713469, Val f1: 0.7048077583312988\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.2141308784484863, Val f1: 1.3017792701721191\n",
            "Val loss: 0.8009304602940878, Val f1: 0.876457929611206\n",
            "Val loss: 0.7267415523529053, Val f1: 0.78778076171875\n",
            "Val loss: 0.6880893536976406, Val f1: 0.757476270198822\n",
            "Val loss: 0.6716004345152113, Val f1: 0.7325747609138489\n",
            "\n",
            "starting Epoch 9\n",
            "Training...\n",
            "Train loss: 0.603862825781107\n",
            "Train loss: 0.5847701412258726\n",
            "Train loss: 0.578034600019455\n",
            "Train loss: 0.5761922820290523\n",
            "Train loss: 0.5747826184545245\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5930239334702492, Val f1: 0.752487301826477\n",
            "Val loss: 0.5736759413372386, Val f1: 0.7337911128997803\n",
            "Val loss: 0.5684638011455536, Val f1: 0.7263780236244202\n",
            "Val loss: 0.5657623961790285, Val f1: 0.7230638265609741\n",
            "Val loss: 0.5634691779102597, Val f1: 0.7208598852157593\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.2090031504631042, Val f1: 1.3343536853790283\n",
            "Val loss: 0.796325703461965, Val f1: 0.8968662619590759\n",
            "Val loss: 0.7235650181770324, Val f1: 0.803107738494873\n",
            "Val loss: 0.6848962477275303, Val f1: 0.7722631692886353\n",
            "Val loss: 0.6689107682969835, Val f1: 0.7465803623199463\n",
            "\n",
            "starting Epoch 10\n",
            "Training...\n",
            "Train loss: 0.5948508977890015\n",
            "Train loss: 0.5786573652065161\n",
            "Train loss: 0.5693189060688019\n",
            "Train loss: 0.5671198679440057\n",
            "Train loss: 0.5648858653647559\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5827127881348133, Val f1: 0.7738026976585388\n",
            "Val loss: 0.5622096964807222, Val f1: 0.7527579069137573\n",
            "Val loss: 0.5592897379398346, Val f1: 0.7429201602935791\n",
            "Val loss: 0.5562030183735178, Val f1: 0.7388817667961121\n",
            "Val loss: 0.5544245803640002, Val f1: 0.7364274859428406\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.1993346810340881, Val f1: 1.348795771598816\n",
            "Val loss: 0.790805995464325, Val f1: 0.9065947532653809\n",
            "Val loss: 0.7175289273262024, Val f1: 0.8152777552604675\n",
            "Val loss: 0.679826727935246, Val f1: 0.7824133634567261\n",
            "Val loss: 0.6650306449996101, Val f1: 0.7567332983016968\n",
            "\n",
            "starting Epoch 11\n",
            "Training...\n",
            "Train loss: 0.5844574794173241\n",
            "Train loss: 0.5658340345729481\n",
            "Train loss: 0.5606143081188202\n",
            "Train loss: 0.5569385619305852\n",
            "Train loss: 0.5555971470617113\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.571676716208458, Val f1: 0.7606208920478821\n",
            "Val loss: 0.5568002444325071, Val f1: 0.7349167466163635\n",
            "Val loss: 0.5514565682411194, Val f1: 0.7291396856307983\n",
            "Val loss: 0.5471433676890473, Val f1: 0.7283111810684204\n",
            "Val loss: 0.5448231122323445, Val f1: 0.7258890867233276\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.187059998512268, Val f1: 1.3387805223464966\n",
            "Val loss: 0.7838199138641357, Val f1: 0.8916721343994141\n",
            "Val loss: 0.7129500508308411, Val f1: 0.7988435626029968\n",
            "Val loss: 0.6750811764172145, Val f1: 0.7648931741714478\n",
            "Val loss: 0.6601625813378228, Val f1: 0.738380491733551\n",
            "\n",
            "starting Epoch 12\n",
            "Training...\n",
            "Train loss: 0.5741976760327816\n",
            "Train loss: 0.555882706786647\n",
            "Train loss: 0.5491546642780304\n",
            "Train loss: 0.5476015308010045\n",
            "Train loss: 0.5468156820251828\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5653407648205757, Val f1: 0.7618927359580994\n",
            "Val loss: 0.5479092597961426, Val f1: 0.7407987713813782\n",
            "Val loss: 0.5410684990882874, Val f1: 0.7327442765235901\n",
            "Val loss: 0.5368547172688726, Val f1: 0.7294617891311646\n",
            "Val loss: 0.5362319144464675, Val f1: 0.7264739871025085\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.1843565702438354, Val f1: 1.3288906812667847\n",
            "Val loss: 0.7815337777137756, Val f1: 0.8847557902336121\n",
            "Val loss: 0.7115702867507935, Val f1: 0.7909483313560486\n",
            "Val loss: 0.6732719966343471, Val f1: 0.7612106800079346\n",
            "Val loss: 0.6577194200621711, Val f1: 0.7368288636207581\n",
            "\n",
            "starting Epoch 13\n",
            "Training...\n",
            "Train loss: 0.5601018145680428\n",
            "Train loss: 0.5469832221666971\n",
            "Train loss: 0.5397828447818757\n",
            "Train loss: 0.5387720919367093\n",
            "Train loss: 0.5380511588993526\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5505029652267694, Val f1: 0.7867683172225952\n",
            "Val loss: 0.5346048934893175, Val f1: 0.7638891339302063\n",
            "Val loss: 0.5310286301374435, Val f1: 0.7540608048439026\n",
            "Val loss: 0.5282441809106229, Val f1: 0.7499345541000366\n",
            "Val loss: 0.5265244433567637, Val f1: 0.7477929592132568\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.1770490407943726, Val f1: 1.385262370109558\n",
            "Val loss: 0.7762982050577799, Val f1: 0.9180041551589966\n",
            "Val loss: 0.7056207895278931, Val f1: 0.8185080885887146\n",
            "Val loss: 0.6676949518067496, Val f1: 0.7835542559623718\n",
            "Val loss: 0.6529351870218912, Val f1: 0.7580728530883789\n",
            "\n",
            "starting Epoch 14\n",
            "Training...\n",
            "Train loss: 0.5541383922100067\n",
            "Train loss: 0.538973407311873\n",
            "Train loss: 0.5321922141313553\n",
            "Train loss: 0.5304212512365029\n",
            "Train loss: 0.5300742604193234\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5522075332701206, Val f1: 0.8030034899711609\n",
            "Val loss: 0.5332885682582855, Val f1: 0.7795127630233765\n",
            "Val loss: 0.5262925404310227, Val f1: 0.7731545567512512\n",
            "Val loss: 0.5234436552916, Val f1: 0.7706841230392456\n",
            "Val loss: 0.5227994411474183, Val f1: 0.7668881416320801\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.1863021850585938, Val f1: 1.405144453048706\n",
            "Val loss: 0.7823394735654196, Val f1: 0.9351674318313599\n",
            "Val loss: 0.7104836821556091, Val f1: 0.8385782241821289\n",
            "Val loss: 0.6730440514428275, Val f1: 0.8007124662399292\n",
            "Val loss: 0.658541308508979, Val f1: 0.7755550146102905\n",
            "\n",
            "starting Epoch 15\n",
            "Training...\n",
            "Train loss: 0.5497211404144764\n",
            "Train loss: 0.5277777278062069\n",
            "Train loss: 0.5237618398666382\n",
            "Train loss: 0.5225680826315239\n",
            "Train loss: 0.5198970575417791\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5362759865820408, Val f1: 0.7881063222885132\n",
            "Val loss: 0.5203563923185522, Val f1: 0.76507169008255\n",
            "Val loss: 0.5143287593126297, Val f1: 0.7568023800849915\n",
            "Val loss: 0.5120080878485495, Val f1: 0.7525649666786194\n",
            "Val loss: 0.5097732629094806, Val f1: 0.751356303691864\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.167259931564331, Val f1: 1.3610551357269287\n",
            "Val loss: 0.7716214259465536, Val f1: 0.9075173735618591\n",
            "Val loss: 0.7035358428955079, Val f1: 0.8087461590766907\n",
            "Val loss: 0.6643709710666111, Val f1: 0.7780550122261047\n",
            "Val loss: 0.6495440734757317, Val f1: 0.7521687150001526\n",
            "\n",
            "starting Epoch 16\n",
            "Training...\n",
            "Train loss: 0.5363356247544289\n",
            "Train loss: 0.5213015314304468\n",
            "Train loss: 0.5161711001396179\n",
            "Train loss: 0.5131344439378426\n",
            "Train loss: 0.5123111037980943\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5271836370229721, Val f1: 0.8031343817710876\n",
            "Val loss: 0.510055144627889, Val f1: 0.7785883545875549\n",
            "Val loss: 0.5062425398826599, Val f1: 0.7709985375404358\n",
            "Val loss: 0.5036218757949659, Val f1: 0.7669413089752197\n",
            "Val loss: 0.5007748696066084, Val f1: 0.7658159136772156\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.1602984070777893, Val f1: 1.3886382579803467\n",
            "Val loss: 0.7681830128033956, Val f1: 0.9230322241783142\n",
            "Val loss: 0.6989600777626037, Val f1: 0.8287224173545837\n",
            "Val loss: 0.6612556321280343, Val f1: 0.7933553457260132\n",
            "Val loss: 0.6477805905871921, Val f1: 0.766282320022583\n",
            "\n",
            "starting Epoch 17\n",
            "Training...\n",
            "Train loss: 0.528012964874506\n",
            "Train loss: 0.5107944589672666\n",
            "Train loss: 0.5074065387248993\n",
            "Train loss: 0.5042532553423696\n",
            "Train loss: 0.5049558881492842\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5172036588191986, Val f1: 0.8095449209213257\n",
            "Val loss: 0.5007726586226261, Val f1: 0.7860916256904602\n",
            "Val loss: 0.49548093259334564, Val f1: 0.7781445384025574\n",
            "Val loss: 0.49397611351155524, Val f1: 0.7744157910346985\n",
            "Val loss: 0.49334938930613653, Val f1: 0.7718315124511719\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.160710871219635, Val f1: 1.3930537700653076\n",
            "Val loss: 0.7656054894129435, Val f1: 0.9270645976066589\n",
            "Val loss: 0.698662519454956, Val f1: 0.8298471570014954\n",
            "Val loss: 0.6608467783246722, Val f1: 0.7958587408065796\n",
            "Val loss: 0.6474474668502808, Val f1: 0.7682358026504517\n",
            "\n",
            "starting Epoch 18\n",
            "Training...\n",
            "Train loss: 0.5209214109927416\n",
            "Train loss: 0.5019969940185547\n",
            "Train loss: 0.4971005862951279\n",
            "Train loss: 0.4958104531266796\n",
            "Train loss: 0.4960849827953747\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5117307547479868, Val f1: 0.8232814073562622\n",
            "Val loss: 0.4951402707533403, Val f1: 0.8004318475723267\n",
            "Val loss: 0.49129933476448057, Val f1: 0.7921568155288696\n",
            "Val loss: 0.4872731295094561, Val f1: 0.7884926795959473\n",
            "Val loss: 0.48707113202129093, Val f1: 0.7847913503646851\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.1596855521202087, Val f1: 1.4153801202774048\n",
            "Val loss: 0.7683527668317159, Val f1: 0.9405239820480347\n",
            "Val loss: 0.7002333164215088, Val f1: 0.8413081169128418\n",
            "Val loss: 0.6616803748267037, Val f1: 0.8053322434425354\n",
            "Val loss: 0.6487034625477262, Val f1: 0.7796033024787903\n",
            "\n",
            "starting Epoch 19\n",
            "Training...\n",
            "Train loss: 0.5167431104928255\n",
            "Train loss: 0.5005802349610762\n",
            "Train loss: 0.4960286170244217\n",
            "Train loss: 0.4915892535181188\n",
            "Train loss: 0.49004060065462474\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5065249167382717, Val f1: 0.8267061710357666\n",
            "Val loss: 0.4896234156507434, Val f1: 0.8018655180931091\n",
            "Val loss: 0.4846741819381714, Val f1: 0.7947171330451965\n",
            "Val loss: 0.48242416221704054, Val f1: 0.7914958000183105\n",
            "Val loss: 0.4802380143886521, Val f1: 0.7900623083114624\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.1686750650405884, Val f1: 1.410733938217163\n",
            "Val loss: 0.7712566057840983, Val f1: 0.9394235610961914\n",
            "Val loss: 0.7030170321464538, Val f1: 0.8428747057914734\n",
            "Val loss: 0.6645458340644836, Val f1: 0.8070710897445679\n",
            "Val loss: 0.6512628528806899, Val f1: 0.7803032994270325\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wmRhVA9rf4-h",
        "outputId": "4d3e041e-24a9-4296-a086-32ce3810a2f7"
      },
      "source": [
        "print(\"Loss: \", losses_eval[-1])\n",
        "print(\"f1: \", f1s_eval[-1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss:  0.5861365675926209\n",
            "f1:  tensor(0.7023, device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "BOJ6OmZdXBjJ",
        "outputId": "b765022f-7da8-4a8c-ec1b-7e292b3c68ec"
      },
      "source": [
        "plt.plot(losses)\n",
        "plt.plot(losses_eval)\n",
        "plt.title('BCE loss value')\n",
        "plt.ylabel('BCE loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper right')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORz1nkPqbK-_"
      },
      "source": [
        "модель переобучается на трейне"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "RQQuGY4CXx14",
        "outputId": "c0397213-998f-4b88-8ad6-a324c1a2199e"
      },
      "source": [
        "plt.plot(f1s)\n",
        "plt.plot(f1s_eval)\n",
        "plt.title('f1 value')\n",
        "plt.ylabel('f1 value')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQn2h0dhYdPD"
      },
      "source": [
        "def predict(model, iterator):\n",
        "    model.eval()\n",
        "    fp = []\n",
        "    fn = []\n",
        "    tp = [] \n",
        "    tn = []\n",
        "    with torch.no_grad():\n",
        "        for i, (texts, ys) in enumerate(iterator):   \n",
        "            preds = model(texts)  # делаем предсказания на тесте \n",
        "            for pred, gold, text in zip(preds, ys, texts):\n",
        "              text = ' '.join([id2word[int(symbol)] for symbol in text if symbol !=0])\n",
        "              if round(pred.item()) > gold:\n",
        "                fp.append(text)\n",
        "              elif round(pred.item()) < gold:\n",
        "                fn.append(text)\n",
        "              elif round(pred.item()) == gold == 1:\n",
        "                tp.append(text)\n",
        "              elif round(pred.item()) == gold == 0:\n",
        "                tn.append(text)\n",
        "    return fp, fn, tp, tn"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chk74h4Mab2Q"
      },
      "source": [
        "fp, fn, tp, tn = predict(model, val_iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WAqedUfBaejg",
        "outputId": "68be3279-467b-441d-f08e-aa902deb4e84"
      },
      "source": [
        "print('что правильно предсказываем:', tp[:5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "что правильно предсказываем: ['rt с сегодняшнее утро я решила посвятить гитаре и музыку всем замечательного дня', 'ага уроки х и я устала чот спать хотелось жутко х_х', 'три дня отдыха эти выходные просто замечательные', 'деп сол үшін ғой үшін', 'хорошо что мама не посмотрела журнал на собрании хотя на и так все знает а нет d']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SgqlNaykah1K",
        "outputId": "648f45e6-5590-49df-c4e6-e0f394a7a6ca"
      },
      "source": [
        "print('ошибочно не относим к негативным:', fn[:5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ошибочно не относим к негативным: ['это мероприятие проводит мы площадку участников они собирают', 'с  были весь день заняты полезными делами', 'lulkaaa так что не надо тут включать', 'хахха мне всегда было весело:d', 'нет она просто неудобная  я скучаю по старой на которой букв не видно с']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n72NG6y_08JM",
        "outputId": "18380704-6a7f-47ff-88e3-dcef88efae96"
      },
      "source": [
        "print('ошибочно считаем положительными:', fp[:5])"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ошибочно считаем положительными:  ['поехала на почту рабочий день закончился победитель по жизни', 'reedasha ylianatomlinson kevin_real ann_horan69 а до меня наконец дошло что аську', 'женщины  вы самые существа ппц', 'тупые приоритеты у нынешней фотать всё что только и выкладывать в инстаграм особенно фотографироваться в зеркале о_о', 'мама что мы с настей два ножа когда открывали но так и не открыли']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37j4-o19d80l"
      },
      "source": [
        "Попробуем добавить в нашу модель еще один пулинг, функцию активации reLU и дропаут"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "epJeg-7fck1m"
      },
      "source": [
        "class CNN(nn.Module):\n",
        "    \n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.embedding.from_pretrained(torch.tensor(weights), freeze=True)\n",
        "        self.bigrams = nn.Conv1d(in_channels=embedding_dim, out_channels=100, kernel_size=2, padding='same')\n",
        "        self.trigrams = nn.Conv1d(in_channels=embedding_dim, out_channels=80, kernel_size=3, padding='same')\n",
        "        self.concatenated = nn.Conv1d(in_channels=180, out_channels=180, kernel_size=2, padding='same')\n",
        "        self.pooling = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.hidden = nn.Linear(in_features=180, out_features=1)\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "        self.out = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, word):\n",
        "        #batch_size x seq_len\n",
        "        embedded = self.embedding(word)\n",
        "        #batch_size x seq_len x embedding_dim\n",
        "        embedded = embedded.transpose(1,2)\n",
        "        #batch_size x embedding_dim x seq_len\n",
        "        feature_map_bigrams = self.bigrams(embedded)\n",
        "        #batch_size x filter_count2 x seq_len* \n",
        "        feature_map_trigrams = self.trigrams(embedded)\n",
        "        #batch_size x filter_count3 x seq_len*\n",
        "        concat_ngrams = torch.cat((feature_map_bigrams, feature_map_trigrams), 1)\n",
        "        \n",
        "        final_layer = self.dropout(self.pooling(self.relu(self.concatenated(concat_ngrams))))\n",
        "\n",
        "\n",
        "        pooling_final = final_layer.max(2)[0] \n",
        "        logits = self.hidden(pooling_final) \n",
        "        logits = self.out(logits)      \n",
        "        return logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4Vuz8l7aqwa"
      },
      "source": [
        "model = CNN(len(word2id), 8)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
        "criterion = nn.BCELoss()  \n",
        "\n",
        "# веса модели и значения лосса храним там же, где и все остальные тензоры\n",
        "model = model.to(DEVICE)\n",
        "criterion = criterion.to(DEVICE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tnOWOfbFgsLf",
        "outputId": "dacc033e-a957-413c-8819-72d47577ae13"
      },
      "source": [
        "losses = []\n",
        "losses_eval = []\n",
        "f1s = []\n",
        "f1s_eval = []\n",
        "\n",
        "for i in range(20):\n",
        "    print(f'\\nstarting Epoch {i}')\n",
        "    print('Training...')\n",
        "    epoch_loss = train(model, train_iterator, optimizer, criterion)\n",
        "    losses.append(epoch_loss)\n",
        "    print('\\nEvaluating on train...')\n",
        "    f1_on_train,_ = evaluate(model, train_iterator, criterion)\n",
        "    f1s.append(f1_on_train)\n",
        "    print('\\nEvaluating on test...')\n",
        "    f1_on_test, epoch_loss_on_test = evaluate(model, val_iterator, criterion)\n",
        "    losses_eval.append(epoch_loss_on_test)\n",
        "    f1s_eval.append(f1_on_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "starting Epoch 0\n",
            "Training...\n",
            "Train loss: 0.7471905238926411\n",
            "Train loss: 0.7139828584410928\n",
            "Train loss: 0.702025454044342\n",
            "Train loss: 0.6956649814079057\n",
            "Train loss: 0.6910913224731173\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.710243221372366, Val f1: 0.6252029538154602\n",
            "Val loss: 0.6884555292851997, Val f1: 0.6086114048957825\n",
            "Val loss: 0.6822406351566315, Val f1: 0.6000368595123291\n",
            "Val loss: 0.6788153408178642, Val f1: 0.5968596339225769\n",
            "Val loss: 0.6767997543017069, Val f1: 0.5937218070030212\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.3413613438606262, Val f1: 1.12729811668396\n",
            "Val loss: 0.893420398235321, Val f1: 0.7570306658744812\n",
            "Val loss: 0.8040358066558838, Val f1: 0.6866993308067322\n",
            "Val loss: 0.765936051096235, Val f1: 0.6586481928825378\n",
            "Val loss: 0.7436567544937134, Val f1: 0.6406907439231873\n",
            "\n",
            "starting Epoch 1\n",
            "Training...\n",
            "Train loss: 0.7093343175947666\n",
            "Train loss: 0.6880765990777449\n",
            "Train loss: 0.6803259134292603\n",
            "Train loss: 0.6766210607628325\n",
            "Train loss: 0.6739321244614465\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.6986673772335052, Val f1: 0.658772349357605\n",
            "Val loss: 0.677500965017261, Val f1: 0.6365294456481934\n",
            "Val loss: 0.6707093119621277, Val f1: 0.6310534477233887\n",
            "Val loss: 0.6672173960885005, Val f1: 0.6285278797149658\n",
            "Val loss: 0.6653375788813546, Val f1: 0.6278771758079529\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.3199840188026428, Val f1: 1.21628737449646\n",
            "Val loss: 0.879464586575826, Val f1: 0.8092187643051147\n",
            "Val loss: 0.7920032620429993, Val f1: 0.7251715064048767\n",
            "Val loss: 0.754623464175633, Val f1: 0.6918614506721497\n",
            "Val loss: 0.7325099176830716, Val f1: 0.6750718355178833\n",
            "\n",
            "starting Epoch 2\n",
            "Training...\n",
            "Train loss: 0.7054195627570152\n",
            "Train loss: 0.6794464479793202\n",
            "Train loss: 0.6708058953285218\n",
            "Train loss: 0.6661446050031862\n",
            "Train loss: 0.6632866164048513\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.69098711758852, Val f1: 0.6962906122207642\n",
            "Val loss: 0.6683780695452834, Val f1: 0.6770763993263245\n",
            "Val loss: 0.6619675588607788, Val f1: 0.6695200800895691\n",
            "Val loss: 0.6585831891244917, Val f1: 0.6652882695198059\n",
            "Val loss: 0.656355293733733, Val f1: 0.6631234288215637\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.3078206777572632, Val f1: 1.2768059968948364\n",
            "Val loss: 0.871719241142273, Val f1: 0.8535575270652771\n",
            "Val loss: 0.7849328517913818, Val f1: 0.7673220634460449\n",
            "Val loss: 0.7472221255302429, Val f1: 0.7344803214073181\n",
            "Val loss: 0.7250378264321221, Val f1: 0.716172456741333\n",
            "\n",
            "starting Epoch 3\n",
            "Training...\n",
            "Train loss: 0.6896072290837765\n",
            "Train loss: 0.6677165157867201\n",
            "Train loss: 0.6615658950805664\n",
            "Train loss: 0.6571861656744089\n",
            "Train loss: 0.6543590256146022\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.6781345680356026, Val f1: 0.6570425033569336\n",
            "Val loss: 0.6568214459852739, Val f1: 0.6395127773284912\n",
            "Val loss: 0.6510303831100464, Val f1: 0.6318121552467346\n",
            "Val loss: 0.6480574545575611, Val f1: 0.625834047794342\n",
            "Val loss: 0.6461294364361536, Val f1: 0.6235366463661194\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.2870959043502808, Val f1: 1.1897599697113037\n",
            "Val loss: 0.858366072177887, Val f1: 0.7900679111480713\n",
            "Val loss: 0.7739232182502747, Val f1: 0.7098643183708191\n",
            "Val loss: 0.7362103036471775, Val f1: 0.684264063835144\n",
            "Val loss: 0.7142762740453085, Val f1: 0.6667843461036682\n",
            "\n",
            "starting Epoch 4\n",
            "Training...\n",
            "Train loss: 0.6792391240596771\n",
            "Train loss: 0.6585507392883301\n",
            "Train loss: 0.6512827289104461\n",
            "Train loss: 0.646743865155462\n",
            "Train loss: 0.6435903842960086\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.6694378741085529, Val f1: 0.7103272080421448\n",
            "Val loss: 0.6510448365500479, Val f1: 0.6865238547325134\n",
            "Val loss: 0.6438617444038391, Val f1: 0.6820932626724243\n",
            "Val loss: 0.6400661174930743, Val f1: 0.6780943274497986\n",
            "Val loss: 0.6379479985861551, Val f1: 0.6763158440589905\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.2740927934646606, Val f1: 1.298105239868164\n",
            "Val loss: 0.8498214880625407, Val f1: 0.8656963109970093\n",
            "Val loss: 0.7662566065788269, Val f1: 0.7780353426933289\n",
            "Val loss: 0.7284663319587708, Val f1: 0.7462725043296814\n",
            "Val loss: 0.7068439854515923, Val f1: 0.7278996706008911\n",
            "\n",
            "starting Epoch 5\n",
            "Training...\n",
            "Train loss: 0.6693886108696461\n",
            "Train loss: 0.6513926513267286\n",
            "Train loss: 0.6437460112571717\n",
            "Train loss: 0.6390951880768164\n",
            "Train loss: 0.6371527725741977\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.6617021486163139, Val f1: 0.6497955322265625\n",
            "Val loss: 0.6415005615263274, Val f1: 0.6325863599777222\n",
            "Val loss: 0.6350392627716065, Val f1: 0.6267403364181519\n",
            "Val loss: 0.6320070330776385, Val f1: 0.6232756972312927\n",
            "Val loss: 0.6300720870494843, Val f1: 0.6210107803344727\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.2583898305892944, Val f1: 1.1860588788986206\n",
            "Val loss: 0.8393921057383219, Val f1: 0.7883917093276978\n",
            "Val loss: 0.7578099727630615, Val f1: 0.7058794498443604\n",
            "Val loss: 0.7199387550354004, Val f1: 0.6795744299888611\n",
            "Val loss: 0.6988397969139947, Val f1: 0.6609365940093994\n",
            "\n",
            "starting Epoch 6\n",
            "Training...\n",
            "Train loss: 0.6627710647881031\n",
            "Train loss: 0.6439574487281569\n",
            "Train loss: 0.6352347636222839\n",
            "Train loss: 0.6322214834725679\n",
            "Train loss: 0.6287780453761419\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.6540654450654984, Val f1: 0.7152794599533081\n",
            "Val loss: 0.6352318326632181, Val f1: 0.6907297372817993\n",
            "Val loss: 0.6280633449554444, Val f1: 0.6835235357284546\n",
            "Val loss: 0.6232483538229074, Val f1: 0.6822625398635864\n",
            "Val loss: 0.6213490813970566, Val f1: 0.6800898909568787\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.2464736104011536, Val f1: 1.3042774200439453\n",
            "Val loss: 0.831535816192627, Val f1: 0.8667044639587402\n",
            "Val loss: 0.7503413796424866, Val f1: 0.780821681022644\n",
            "Val loss: 0.7130503058433533, Val f1: 0.7479354739189148\n",
            "Val loss: 0.6918963922394646, Val f1: 0.7286939024925232\n",
            "\n",
            "starting Epoch 7\n",
            "Training...\n",
            "Train loss: 0.6513302475214005\n",
            "Train loss: 0.6312103000554171\n",
            "Train loss: 0.6263502097129822\n",
            "Train loss: 0.6223718096960836\n",
            "Train loss: 0.6201181603329522\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.6435761898756027, Val f1: 0.7116101980209351\n",
            "Val loss: 0.6238050695621606, Val f1: 0.691909909248352\n",
            "Val loss: 0.6178918015956879, Val f1: 0.6842820048332214\n",
            "Val loss: 0.6144568190645816, Val f1: 0.6815589070320129\n",
            "Val loss: 0.6126799654392969, Val f1: 0.6798343658447266\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.2302233576774597, Val f1: 1.3053592443466187\n",
            "Val loss: 0.820651113986969, Val f1: 0.8706902265548706\n",
            "Val loss: 0.7411669611930847, Val f1: 0.7806151509284973\n",
            "Val loss: 0.7040628365107945, Val f1: 0.747809648513794\n",
            "Val loss: 0.6834699776437547, Val f1: 0.729732096195221\n",
            "\n",
            "starting Epoch 8\n",
            "Training...\n",
            "Train loss: 0.6437035612761974\n",
            "Train loss: 0.6240890802759113\n",
            "Train loss: 0.6173766481876374\n",
            "Train loss: 0.6140086704225682\n",
            "Train loss: 0.6125984397672471\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.6357821561396122, Val f1: 0.7063143849372864\n",
            "Val loss: 0.6176287080302383, Val f1: 0.68768709897995\n",
            "Val loss: 0.6110586655139923, Val f1: 0.6807145476341248\n",
            "Val loss: 0.6074637459285224, Val f1: 0.6785181760787964\n",
            "Val loss: 0.6059695978959402, Val f1: 0.6754379868507385\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.2191975712776184, Val f1: 1.2835767269134521\n",
            "Val loss: 0.8129985133806864, Val f1: 0.852418839931488\n",
            "Val loss: 0.7348678350448609, Val f1: 0.7653858065605164\n",
            "Val loss: 0.6983900495937893, Val f1: 0.7346888780593872\n",
            "Val loss: 0.677988330523173, Val f1: 0.7158507108688354\n",
            "\n",
            "starting Epoch 9\n",
            "Training...\n",
            "Train loss: 0.6348630040884018\n",
            "Train loss: 0.6156124526804144\n",
            "Train loss: 0.610071405172348\n",
            "Train loss: 0.60684329538203\n",
            "Train loss: 0.6058631525153205\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.6292649544775486, Val f1: 0.697270393371582\n",
            "Val loss: 0.6097410971468146, Val f1: 0.6802479028701782\n",
            "Val loss: 0.6015703999996185, Val f1: 0.6761713027954102\n",
            "Val loss: 0.599505234120497, Val f1: 0.6711323261260986\n",
            "Val loss: 0.5976991078683308, Val f1: 0.6696558594703674\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.2068194150924683, Val f1: 1.276158094406128\n",
            "Val loss: 0.805036465326945, Val f1: 0.8456530570983887\n",
            "Val loss: 0.728110671043396, Val f1: 0.7595720291137695\n",
            "Val loss: 0.691324668271201, Val f1: 0.7282803058624268\n",
            "Val loss: 0.6714861856566535, Val f1: 0.7092459797859192\n",
            "\n",
            "starting Epoch 10\n",
            "Training...\n",
            "Train loss: 0.6289024353027344\n",
            "Train loss: 0.6092819192192771\n",
            "Train loss: 0.6045589721202851\n",
            "Train loss: 0.6013207524570067\n",
            "Train loss: 0.5985518161739621\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.6211035512387753, Val f1: 0.7260289192199707\n",
            "Val loss: 0.5998796134284048, Val f1: 0.7067283391952515\n",
            "Val loss: 0.5944394016265869, Val f1: 0.6983469128608704\n",
            "Val loss: 0.5911118939741334, Val f1: 0.695749044418335\n",
            "Val loss: 0.5899259043591363, Val f1: 0.6916673183441162\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.1917092204093933, Val f1: 1.306604266166687\n",
            "Val loss: 0.7957140803337097, Val f1: 0.8756093978881836\n",
            "Val loss: 0.7201061606407165, Val f1: 0.7855129837989807\n",
            "Val loss: 0.6843298758779254, Val f1: 0.7526655793190002\n",
            "Val loss: 0.6648860904905531, Val f1: 0.7341769337654114\n",
            "\n",
            "starting Epoch 11\n",
            "Training...\n",
            "Train loss: 0.6222855634987354\n",
            "Train loss: 0.6030345848112395\n",
            "Train loss: 0.5969553673267365\n",
            "Train loss: 0.5944472017572887\n",
            "Train loss: 0.5923042332842237\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.6072345823049545, Val f1: 0.7467445731163025\n",
            "Val loss: 0.591345073598804, Val f1: 0.7197893857955933\n",
            "Val loss: 0.5871353423595429, Val f1: 0.7096390128135681\n",
            "Val loss: 0.5849628457382544, Val f1: 0.704152524471283\n",
            "Val loss: 0.5831503768761953, Val f1: 0.7031240463256836\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.1826305985450745, Val f1: 1.3312838077545166\n",
            "Val loss: 0.7901221315066019, Val f1: 0.8895648717880249\n",
            "Val loss: 0.7148128509521484, Val f1: 0.7997410893440247\n",
            "Val loss: 0.6792996525764465, Val f1: 0.7636341452598572\n",
            "Val loss: 0.6602734790907966, Val f1: 0.7440255284309387\n",
            "\n",
            "starting Epoch 12\n",
            "Training...\n",
            "Train loss: 0.6150336116552353\n",
            "Train loss: 0.5962030002565095\n",
            "Train loss: 0.5891897046566009\n",
            "Train loss: 0.5874377275580791\n",
            "Train loss: 0.5849439835264569\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.6035929545760155, Val f1: 0.7297205328941345\n",
            "Val loss: 0.5867541901993029, Val f1: 0.7075479626655579\n",
            "Val loss: 0.5809047508239746, Val f1: 0.7017846703529358\n",
            "Val loss: 0.5785326842051833, Val f1: 0.6977975368499756\n",
            "Val loss: 0.5759879343566441, Val f1: 0.6972459554672241\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.1710746884346008, Val f1: 1.319469928741455\n",
            "Val loss: 0.7821245590845743, Val f1: 0.8748407363891602\n",
            "Val loss: 0.7081634402275085, Val f1: 0.7854618430137634\n",
            "Val loss: 0.6727485827037266, Val f1: 0.751981794834137\n",
            "Val loss: 0.6542076667149862, Val f1: 0.7323647141456604\n",
            "\n",
            "starting Epoch 13\n",
            "Training...\n",
            "Train loss: 0.5993764437735081\n",
            "Train loss: 0.5832377274831136\n",
            "Train loss: 0.5804175245761871\n",
            "Train loss: 0.577665262257875\n",
            "Train loss: 0.5770859718322754\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5933908596634865, Val f1: 0.7529866099357605\n",
            "Val loss: 0.578317358638301, Val f1: 0.7248274683952332\n",
            "Val loss: 0.5727494609355926, Val f1: 0.7173171639442444\n",
            "Val loss: 0.5699650663048473, Val f1: 0.714819073677063\n",
            "Val loss: 0.5685836083832241, Val f1: 0.712892472743988\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.1609376072883606, Val f1: 1.3368358612060547\n",
            "Val loss: 0.7750950455665588, Val f1: 0.8891317844390869\n",
            "Val loss: 0.701589560508728, Val f1: 0.8005166053771973\n",
            "Val loss: 0.6661939450672695, Val f1: 0.7669515013694763\n",
            "Val loss: 0.6482572555541992, Val f1: 0.7457901239395142\n",
            "\n",
            "starting Epoch 14\n",
            "Training...\n",
            "Train loss: 0.5978795737028122\n",
            "Train loss: 0.5812417341001106\n",
            "Train loss: 0.5769822931289673\n",
            "Train loss: 0.5734526870855644\n",
            "Train loss: 0.5712526880559468\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5922569148242474, Val f1: 0.741805911064148\n",
            "Val loss: 0.5746983741268967, Val f1: 0.7238560914993286\n",
            "Val loss: 0.5680163526535034, Val f1: 0.716370165348053\n",
            "Val loss: 0.564416054469436, Val f1: 0.7135252356529236\n",
            "Val loss: 0.5621405910877955, Val f1: 0.7123849391937256\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.147730529308319, Val f1: 1.3452460765838623\n",
            "Val loss: 0.767273485660553, Val f1: 0.8937869071960449\n",
            "Val loss: 0.6952100157737732, Val f1: 0.8051769137382507\n",
            "Val loss: 0.6601798789841788, Val f1: 0.7707728147506714\n",
            "Val loss: 0.6428582800759209, Val f1: 0.7497892379760742\n",
            "\n",
            "starting Epoch 15\n",
            "Training...\n",
            "Train loss: 0.5923678055405617\n",
            "Train loss: 0.5755394209514965\n",
            "Train loss: 0.5684999072551727\n",
            "Train loss: 0.5659174474317636\n",
            "Train loss: 0.5646962828579403\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5835161805152893, Val f1: 0.7675763964653015\n",
            "Val loss: 0.5664230603160281, Val f1: 0.7416919469833374\n",
            "Val loss: 0.5602202069759369, Val f1: 0.7367396950721741\n",
            "Val loss: 0.5582339016359243, Val f1: 0.7330175042152405\n",
            "Val loss: 0.5560712353104637, Val f1: 0.7315838932991028\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.1418417096138, Val f1: 1.3802677392959595\n",
            "Val loss: 0.7624625364939371, Val f1: 0.917144238948822\n",
            "Val loss: 0.6912626981735229, Val f1: 0.8233656287193298\n",
            "Val loss: 0.6565803630011422, Val f1: 0.7893141508102417\n",
            "Val loss: 0.6393298109372457, Val f1: 0.7667277455329895\n",
            "\n",
            "starting Epoch 16\n",
            "Training...\n",
            "Train loss: 0.5852408036589622\n",
            "Train loss: 0.5689788045305194\n",
            "Train loss: 0.5614734923839569\n",
            "Train loss: 0.5586010262147704\n",
            "Train loss: 0.557823088197481\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5745787248015404, Val f1: 0.7813432812690735\n",
            "Val loss: 0.5574222062573289, Val f1: 0.7567242383956909\n",
            "Val loss: 0.5527946186065674, Val f1: 0.7478600144386292\n",
            "Val loss: 0.5505978367221889, Val f1: 0.7442222833633423\n",
            "Val loss: 0.5495247329984393, Val f1: 0.7405509352684021\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.1337546706199646, Val f1: 1.399972915649414\n",
            "Val loss: 0.7574447592099508, Val f1: 0.9339367151260376\n",
            "Val loss: 0.6856693744659423, Val f1: 0.8347885012626648\n",
            "Val loss: 0.6512975267001561, Val f1: 0.8009653687477112\n",
            "Val loss: 0.634449647532569, Val f1: 0.7780451774597168\n",
            "\n",
            "starting Epoch 17\n",
            "Training...\n",
            "Train loss: 0.5825950056314468\n",
            "Train loss: 0.5636559544187604\n",
            "Train loss: 0.5574968194961548\n",
            "Train loss: 0.5544483101190026\n",
            "Train loss: 0.551414049807049\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5661513917148113, Val f1: 0.7861267328262329\n",
            "Val loss: 0.5533816868608649, Val f1: 0.7579184770584106\n",
            "Val loss: 0.5474954521656037, Val f1: 0.751632809638977\n",
            "Val loss: 0.5440474063602846, Val f1: 0.7479487061500549\n",
            "Val loss: 0.54277132664408, Val f1: 0.745087206363678\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.1232989430427551, Val f1: 1.3967933654785156\n",
            "Val loss: 0.7505844235420227, Val f1: 0.9346725940704346\n",
            "Val loss: 0.6805877566337586, Val f1: 0.8380500078201294\n",
            "Val loss: 0.6464051519121442, Val f1: 0.8039440512657166\n",
            "Val loss: 0.6303545302814908, Val f1: 0.782031238079071\n",
            "\n",
            "starting Epoch 18\n",
            "Training...\n",
            "Train loss: 0.5766389705240726\n",
            "Train loss: 0.5583046439922217\n",
            "Train loss: 0.5544352114200592\n",
            "Train loss: 0.5506807352179912\n",
            "Train loss: 0.5484580887215478\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5631848573684692, Val f1: 0.7704481482505798\n",
            "Val loss: 0.5463151715018533, Val f1: 0.744330108165741\n",
            "Val loss: 0.5417820096015931, Val f1: 0.7358562350273132\n",
            "Val loss: 0.5390647694246092, Val f1: 0.7334284782409668\n",
            "Val loss: 0.536694789926211, Val f1: 0.7323920726776123\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.1121231317520142, Val f1: 1.3639739751815796\n",
            "Val loss: 0.7443428039550781, Val f1: 0.9108402729034424\n",
            "Val loss: 0.6753633975982666, Val f1: 0.8177515268325806\n",
            "Val loss: 0.6412781051227024, Val f1: 0.7825181484222412\n",
            "Val loss: 0.6257568200429281, Val f1: 0.759357750415802\n",
            "\n",
            "starting Epoch 19\n",
            "Training...\n",
            "Train loss: 0.570081789046526\n",
            "Train loss: 0.548725451483871\n",
            "Train loss: 0.5452745008468628\n",
            "Train loss: 0.5430022220113384\n",
            "Train loss: 0.5414870899348032\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.556517269462347, Val f1: 0.7916462421417236\n",
            "Val loss: 0.5393450657526652, Val f1: 0.767746090888977\n",
            "Val loss: 0.5358202695846558, Val f1: 0.7584725022315979\n",
            "Val loss: 0.53335715496718, Val f1: 0.7544192671775818\n",
            "Val loss: 0.5311097602049509, Val f1: 0.7535373568534851\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.1066583395004272, Val f1: 1.4052870273590088\n",
            "Val loss: 0.7409197489420573, Val f1: 0.9419618844985962\n",
            "Val loss: 0.6724528908729553, Val f1: 0.8445386290550232\n",
            "Val loss: 0.6387169786861965, Val f1: 0.808515191078186\n",
            "Val loss: 0.6233325401941935, Val f1: 0.7847010493278503\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WUlDfdaPgy1C",
        "outputId": "06f0085a-2e27-45e2-ef0a-fad1a6e7c865"
      },
      "source": [
        "print(\"Loss: \", losses_eval[-1])\n",
        "print(\"f1: \", f1s_eval[-1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss:  0.5609992861747741\n",
            "f1:  tensor(0.7062, device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "w72h9Hm5g3rM",
        "outputId": "5f3360c1-2879-472f-dc6d-fb235d602ecf"
      },
      "source": [
        "plt.plot(losses)\n",
        "plt.plot(losses_eval)\n",
        "plt.title('BCE loss value')\n",
        "plt.ylabel('BCE loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "pHPhbUD5g-GI",
        "outputId": "1f38f00f-9adc-4a23-c596-e453e4a5a08a"
      },
      "source": [
        "plt.plot(f1s)\n",
        "plt.plot(f1s_eval)\n",
        "plt.title('f1 value')\n",
        "plt.ylabel('f1 value')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GikUI1qOhkia"
      },
      "source": [
        "Как мы видим, модель перестала переобучаться на трейне, f1 мера улучшилась на несколько сотых"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxzrIFeg9aBL"
      },
      "source": [
        "### Комбинация эмбеддингов и символьных признаков"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGfyU4ZOhD25",
        "outputId": "53d2ee96-625b-46a1-995b-5bb3487e7dd2"
      },
      "source": [
        "vocab_sym = Counter()\n",
        "\n",
        "for text in tweets_data['preprocessed_text']:\n",
        "    vocab_sym.update(list(text))\n",
        "print('всего уникальных символов:', len(vocab_sym))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "всего уникальных символов: 294\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xyFQQSKGjRRm",
        "outputId": "032505be-0d98-4722-c75f-d9100d210d4b"
      },
      "source": [
        "filtered_vocab_sym = set()\n",
        "\n",
        "for symbol in vocab_sym:\n",
        "    if vocab_sym[symbol] > 5:\n",
        "        filtered_vocab_sym.add(symbol)\n",
        "print('уникальных символов, втретившихся больше 5 раз:', len(filtered_vocab_sym))\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "уникальных символов, втретившихся больше 5 раз: 139\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oudwG88Vjb6a"
      },
      "source": [
        "#создаем словарь с индексами symbol2id, для спецсимвола паддинга дефолтный индекс - 0\n",
        "symbol2id = {'PAD':0}\n",
        "\n",
        "for symbol in filtered_vocab_sym:\n",
        "    symbol2id[symbol] = len(symbol2id)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqF5sxsPjkjG"
      },
      "source": [
        "id2symbol = {i:symbol for symbol, i in symbol2id.items()}"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TgjO3DQTjlcl"
      },
      "source": [
        "class TwitterDataset(Dataset):\n",
        "\n",
        "    def __init__(self, dataset, word2id, symbol2id, DEVICE):\n",
        "        self.dataset = dataset['preprocessed_text'].values\n",
        "        self.word2id = word2id\n",
        "        self.symbol2id = symbol2id\n",
        "        self.length = dataset.shape[0]\n",
        "        self.target = dataset['tone'].values\n",
        "        self.device = DEVICE\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, index): \n",
        "        words = self.dataset[index].split(' ')\n",
        "        ids = torch.LongTensor([self.word2id[word] for word in words if word in self.word2id])\n",
        "        symbols = list(self.dataset[index])\n",
        "        ids_sym = torch.LongTensor([self.symbol2id[symbol] for symbol in symbols if symbol in self.symbol2id])\n",
        "        y = [self.target[index]]\n",
        "        return ids, ids_sym, y\n",
        "\n",
        "    def collate_fn(self, batch): \n",
        "      ids, ids_sym, y  = list(zip(*batch))\n",
        "      padded_ids = pad_sequence(ids, batch_first=True).to(self.device)\n",
        "      padded_ids_sym = pad_sequence(ids_sym, batch_first=True).to(self.device)\n",
        "      y = torch.Tensor(y).to(self.device)\n",
        "      return padded_ids, padded_ids_sym, y"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDKyc___ktj5"
      },
      "source": [
        "train_dataset = TwitterDataset(train_sentences, word2id, symbol2id, DEVICE)\n",
        "train_sampler = RandomSampler(train_dataset)\n",
        "train_iterator = DataLoader(train_dataset, collate_fn = train_dataset.collate_fn, sampler=train_sampler, batch_size=1024)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LiYITZ0jk2yY"
      },
      "source": [
        "batch = next(iter(train_iterator))"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8Jp3KrTlFh9"
      },
      "source": [
        "val_dataset = TwitterDataset(val_sentences, word2id, symbol2id, DEVICE)\n",
        "val_sampler = SequentialSampler(val_dataset)\n",
        "val_iterator = DataLoader(val_dataset, collate_fn = val_dataset.collate_fn, sampler=val_sampler, batch_size=1024)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vm6bMv49lgd9",
        "outputId": "dac25730-ce39-4b36-a337-69c9ea9b409d"
      },
      "source": [
        "test_batch = next(iter(val_iterator))\n",
        "test_batch[0].shape"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1024, 26])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DhsqV24lhgH"
      },
      "source": [
        "class CNN(nn.Module):\n",
        "    \n",
        "    def __init__(self, vocab_size, vocab_size_sym, embedding_dim):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, 100)\n",
        "        self.embedding.from_pretrained(torch.tensor(weights), freeze=True)\n",
        "        self.embedding_sym = nn.Embedding(vocab_size_sym, embedding_dim)\n",
        "\n",
        "        self.bigrams = nn.Conv1d(in_channels=embedding_dim, out_channels=100, kernel_size=2, padding='same')\n",
        "        self.trigrams = nn.Conv1d(in_channels=embedding_dim, out_channels=80, kernel_size=3, padding='same')\n",
        "\n",
        "        self.hidden = nn.Linear(in_features=100, out_features=100)\n",
        "        self.hidden_sym = nn.Linear(in_features=180, out_features=100)\n",
        "        self.hidden_final = nn.Linear(in_features=200, out_features=1)\n",
        "        self.out = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, word, sym):\n",
        "        embedded = self.embedding(word)\n",
        "\n",
        "        embedded_sent = torch.mean(embedded, dim=1)\n",
        "\n",
        "        linear_lay = self.hidden(embedded_sent)\n",
        "\n",
        "        embedded_sym = self.embedding_sym(sym)\n",
        "        embedded_sym = embedded_sym.transpose(1,2)\n",
        "\n",
        "        feature_map_bigrams = self.bigrams(embedded_sym)\n",
        "        feature_map_trigrams = self.trigrams(embedded_sym)\n",
        "        concat_ngrams = torch.cat((feature_map_bigrams, feature_map_trigrams), 1)\n",
        "\n",
        "\n",
        "        pooling = concat_ngrams.max(2)[0]\n",
        "        linear_sym = self.hidden_sym(pooling)\n",
        "\n",
        "\n",
        "        concat = torch.cat((linear_lay, linear_sym), 1)\n",
        "        \n",
        "        logits = self.hidden_final(concat)\n",
        "        logits = self.out(logits)      \n",
        "        return logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvSfUWPwr9NY"
      },
      "source": [
        "model = CNN(len(word2id), len(symbol2id), 8)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
        "criterion = nn.BCELoss()  \n",
        "\n",
        "# веса модели и значения лосса храним там же, где и все остальные тензоры\n",
        "model = model.to(DEVICE)\n",
        "criterion = criterion.to(DEVICE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usiKueagtHbJ"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "    epoch_loss = 0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for i, (texts, symbols, ys) in enumerate(iterator): \n",
        "        optimizer.zero_grad()\n",
        "        preds = model(texts, symbols)  \n",
        "        loss = criterion(preds, ys)  \n",
        "        loss.backward() \n",
        "        optimizer.step() \n",
        "        epoch_loss += loss.item()\n",
        "        if not (i + 1) % int(len(iterator)/5):\n",
        "            print(f'Train loss: {epoch_loss/i}')      \n",
        "    return  epoch_loss / len(iterator)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLgfX7QT1wL1"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    epoch_loss = 0\n",
        "    epoch_metric = 0\n",
        "    model.eval() \n",
        "    with torch.no_grad():\n",
        "        for i, (texts, symbols, ys) in enumerate(iterator):   \n",
        "            preds = model(texts, symbols)\n",
        "            loss = criterion(preds, ys)\n",
        "            epoch_loss += loss.item()\n",
        "            batch_metric = f1(preds.round().long(), ys.long(), ignore_index=0)\n",
        "            epoch_metric += batch_metric\n",
        "\n",
        "            if not (i + 1) % int(len(iterator)/5):\n",
        "              print(f'Val loss: {epoch_loss/i}, Val f1: {epoch_metric/i}')\n",
        "        \n",
        "    return epoch_metric / len(iterator), epoch_loss / len(iterator)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "puMvdFDXsx5n",
        "outputId": "8a11b9b7-6e8a-44f1-990c-18324204ab27"
      },
      "source": [
        "losses = []\n",
        "losses_eval = []\n",
        "f1s = []\n",
        "f1s_eval = []\n",
        "\n",
        "for i in range(20):\n",
        "    print(f'\\nstarting Epoch {i}')\n",
        "    print('Training...')\n",
        "    epoch_loss = train(model, train_iterator, optimizer, criterion)\n",
        "    losses.append(epoch_loss)\n",
        "    print('\\nEvaluating on train...')\n",
        "    f1_on_train,_ = evaluate(model, train_iterator, criterion)\n",
        "    f1s.append(f1_on_train)\n",
        "    print('\\nEvaluating on test...')\n",
        "    f1_on_test, epoch_loss_on_test = evaluate(model, val_iterator, criterion)\n",
        "    losses_eval.append(epoch_loss_on_test)\n",
        "    f1s_eval.append(f1_on_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "starting Epoch 0\n",
            "Training...\n",
            "Train loss: 0.733752179890871\n",
            "Train loss: 0.7059228998242002\n",
            "Train loss: 0.6937021410465241\n",
            "Train loss: 0.6852269919950571\n",
            "Train loss: 0.6782749550683158\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.6895610503852367, Val f1: 0.6328274011611938\n",
            "Val loss: 0.6666343808174133, Val f1: 0.6121730804443359\n",
            "Val loss: 0.659312025308609, Val f1: 0.6057159900665283\n",
            "Val loss: 0.6564290372293387, Val f1: 0.600706934928894\n",
            "Val loss: 0.6540357172489166, Val f1: 0.600382924079895\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.2746126651763916, Val f1: 1.2393996715545654\n",
            "Val loss: 0.8562382062276205, Val f1: 0.8123142123222351\n",
            "Val loss: 0.7736100077629089, Val f1: 0.7241352200508118\n",
            "Val loss: 0.7380890931401934, Val f1: 0.6848635673522949\n",
            "Val loss: 0.7180861102210151, Val f1: 0.6619582772254944\n",
            "\n",
            "starting Epoch 1\n",
            "Training...\n",
            "Train loss: 0.681204441934824\n",
            "Train loss: 0.657556156317393\n",
            "Train loss: 0.6459033811092376\n",
            "Train loss: 0.6391550569391963\n",
            "Train loss: 0.6334273332641238\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.6422958634793758, Val f1: 0.7455739378929138\n",
            "Val loss: 0.6225485314022411, Val f1: 0.7223609685897827\n",
            "Val loss: 0.6166694664955139, Val f1: 0.7163731455802917\n",
            "Val loss: 0.6135341614039976, Val f1: 0.7137345671653748\n",
            "Val loss: 0.6110535491080511, Val f1: 0.7114613056182861\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.1850271821022034, Val f1: 1.422583818435669\n",
            "Val loss: 0.7981512745221456, Val f1: 0.946601927280426\n",
            "Val loss: 0.7231373071670533, Val f1: 0.8455950021743774\n",
            "Val loss: 0.6906363793781826, Val f1: 0.8000090718269348\n",
            "Val loss: 0.6733854346805148, Val f1: 0.7783939838409424\n",
            "\n",
            "starting Epoch 2\n",
            "Training...\n",
            "Train loss: 0.6372608095407486\n",
            "Train loss: 0.6159385605291887\n",
            "Train loss: 0.6053814315795898\n",
            "Train loss: 0.5983491148521651\n",
            "Train loss: 0.5942360765877224\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.6025331355631351, Val f1: 0.7441203594207764\n",
            "Val loss: 0.5848348447770784, Val f1: 0.7208817601203918\n",
            "Val loss: 0.5789972352981567, Val f1: 0.7142753601074219\n",
            "Val loss: 0.5758203027853325, Val f1: 0.7116938233375549\n",
            "Val loss: 0.5738407479865211, Val f1: 0.7095659971237183\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.11703360080719, Val f1: 1.3985538482666016\n",
            "Val loss: 0.7527636686960856, Val f1: 0.9345868229866028\n",
            "Val loss: 0.6833879709243774, Val f1: 0.8365025520324707\n",
            "Val loss: 0.6518659506525312, Val f1: 0.7935875654220581\n",
            "Val loss: 0.6364439460966322, Val f1: 0.7721892595291138\n",
            "\n",
            "starting Epoch 3\n",
            "Training...\n",
            "Train loss: 0.5992432720959187\n",
            "Train loss: 0.5782504424904332\n",
            "Train loss: 0.5728354406356811\n",
            "Train loss: 0.5679850756232419\n",
            "Train loss: 0.565279911671366\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5752404257655144, Val f1: 0.7648321390151978\n",
            "Val loss: 0.5561787702820518, Val f1: 0.7431776523590088\n",
            "Val loss: 0.5511577820777893, Val f1: 0.7343128323554993\n",
            "Val loss: 0.5486838559606182, Val f1: 0.7308650016784668\n",
            "Val loss: 0.5471900098380589, Val f1: 0.72770756483078\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.0783538818359375, Val f1: 1.4252443313598633\n",
            "Val loss: 0.7265312274297079, Val f1: 0.9503979086875916\n",
            "Val loss: 0.6589228749275208, Val f1: 0.8513553738594055\n",
            "Val loss: 0.6289134110723223, Val f1: 0.808890163898468\n",
            "Val loss: 0.613864799340566, Val f1: 0.7865569591522217\n",
            "\n",
            "starting Epoch 4\n",
            "Training...\n",
            "Train loss: 0.5764009281992912\n",
            "Train loss: 0.5583576758702596\n",
            "Train loss: 0.5484260821342468\n",
            "Train loss: 0.5434484152651545\n",
            "Train loss: 0.5387752623785109\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5505043845623732, Val f1: 0.7858473062515259\n",
            "Val loss: 0.5315362937522657, Val f1: 0.7645397782325745\n",
            "Val loss: 0.5271241593360901, Val f1: 0.7557597160339355\n",
            "Val loss: 0.5238153836620387, Val f1: 0.7516657710075378\n",
            "Val loss: 0.5229367443493435, Val f1: 0.748425304889679\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.0464960932731628, Val f1: 1.4452967643737793\n",
            "Val loss: 0.7054535746574402, Val f1: 0.9646396636962891\n",
            "Val loss: 0.6387880325317383, Val f1: 0.8631588220596313\n",
            "Val loss: 0.6100918650627136, Val f1: 0.8212199211120605\n",
            "Val loss: 0.5949000716209412, Val f1: 0.7991522550582886\n",
            "\n",
            "starting Epoch 5\n",
            "Training...\n",
            "Train loss: 0.5483338292688131\n",
            "Train loss: 0.5308627034678604\n",
            "Train loss: 0.5242942988872528\n",
            "Train loss: 0.5175728575507207\n",
            "Train loss: 0.5145033658260391\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.531720781698823, Val f1: 0.7593132853507996\n",
            "Val loss: 0.5151362057888147, Val f1: 0.7416400909423828\n",
            "Val loss: 0.5119217628240585, Val f1: 0.7334948182106018\n",
            "Val loss: 0.5096874828658887, Val f1: 0.7300645112991333\n",
            "Val loss: 0.5075298728687423, Val f1: 0.7282897233963013\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.0354139804840088, Val f1: 1.3710724115371704\n",
            "Val loss: 0.6993913054466248, Val f1: 0.9220740795135498\n",
            "Val loss: 0.6340491533279419, Val f1: 0.8292576670646667\n",
            "Val loss: 0.6048085944993156, Val f1: 0.7890918850898743\n",
            "Val loss: 0.589244888888465, Val f1: 0.769329309463501\n",
            "\n",
            "starting Epoch 6\n",
            "Training...\n",
            "Train loss: 0.5272482484579086\n",
            "Train loss: 0.5072217421098189\n",
            "Train loss: 0.5018311828374863\n",
            "Train loss: 0.4974902939440599\n",
            "Train loss: 0.4949111225349562\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.49956007301807404, Val f1: 0.8213977813720703\n",
            "Val loss: 0.48636159842664545, Val f1: 0.7968347072601318\n",
            "Val loss: 0.48143342792987825, Val f1: 0.789074182510376\n",
            "Val loss: 0.47966088198903784, Val f1: 0.7845386266708374\n",
            "Val loss: 0.478100329992317, Val f1: 0.781830906867981\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.9985929429531097, Val f1: 1.4822781085968018\n",
            "Val loss: 0.6744730770587921, Val f1: 0.9914088249206543\n",
            "Val loss: 0.6083622395992279, Val f1: 0.8869062662124634\n",
            "Val loss: 0.5814438164234161, Val f1: 0.8430243730545044\n",
            "Val loss: 0.56549358036783, Val f1: 0.8208341002464294\n",
            "\n",
            "starting Epoch 7\n",
            "Training...\n",
            "Train loss: 0.49600780569016933\n",
            "Train loss: 0.4832187900037477\n",
            "Train loss: 0.47748569786548617\n",
            "Train loss: 0.47440677881240845\n",
            "Train loss: 0.47317610113393693\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.48244126327335835, Val f1: 0.838253378868103\n",
            "Val loss: 0.4649714231491089, Val f1: 0.8133440017700195\n",
            "Val loss: 0.4637162035703659, Val f1: 0.8025074005126953\n",
            "Val loss: 0.46073364188422017, Val f1: 0.799192488193512\n",
            "Val loss: 0.45954936565387816, Val f1: 0.7971674799919128\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.9822183847427368, Val f1: 1.503296136856079\n",
            "Val loss: 0.6657173037528992, Val f1: 1.0054817199707031\n",
            "Val loss: 0.5990602195262908, Val f1: 0.9013668894767761\n",
            "Val loss: 0.5729813703468868, Val f1: 0.8558014035224915\n",
            "Val loss: 0.5564477774831984, Val f1: 0.8336947560310364\n",
            "\n",
            "starting Epoch 8\n",
            "Training...\n",
            "Train loss: 0.4879526365548372\n",
            "Train loss: 0.4666734157186566\n",
            "Train loss: 0.46046409249305725\n",
            "Train loss: 0.45766660482136173\n",
            "Train loss: 0.4538783924210639\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.4616902284324169, Val f1: 0.838388204574585\n",
            "Val loss: 0.447554276748137, Val f1: 0.8142490386962891\n",
            "Val loss: 0.44370902061462403, Val f1: 0.8052669763565063\n",
            "Val loss: 0.4416006985884994, Val f1: 0.8021498322486877\n",
            "Val loss: 0.4398811392131306, Val f1: 0.7999083995819092\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.9618764519691467, Val f1: 1.4893558025360107\n",
            "Val loss: 0.6525951027870178, Val f1: 0.9927668571472168\n",
            "Val loss: 0.5875946462154389, Val f1: 0.8915811777114868\n",
            "Val loss: 0.5621098535401481, Val f1: 0.8454689383506775\n",
            "Val loss: 0.545475529299842, Val f1: 0.8239924907684326\n",
            "\n",
            "starting Epoch 9\n",
            "Training...\n",
            "Train loss: 0.46116957254707813\n",
            "Train loss: 0.446964319005157\n",
            "Train loss: 0.44219809889793393\n",
            "Train loss: 0.43967408964883037\n",
            "Train loss: 0.43662758952095393\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.4491334296762943, Val f1: 0.840828001499176\n",
            "Val loss: 0.4394684605526202, Val f1: 0.8097892999649048\n",
            "Val loss: 0.4336712282896042, Val f1: 0.8029546141624451\n",
            "Val loss: 0.4306975380698247, Val f1: 0.7991571426391602\n",
            "Val loss: 0.4287422448396683, Val f1: 0.7979381084442139\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.9628134369850159, Val f1: 1.4749456644058228\n",
            "Val loss: 0.6543469130992889, Val f1: 0.9853233695030212\n",
            "Val loss: 0.5896748960018158, Val f1: 0.8835352063179016\n",
            "Val loss: 0.5641339932169233, Val f1: 0.8372856378555298\n",
            "Val loss: 0.5467239452732934, Val f1: 0.8168765306472778\n",
            "\n",
            "starting Epoch 10\n",
            "Training...\n",
            "Train loss: 0.4516962543129921\n",
            "Train loss: 0.437134917938348\n",
            "Train loss: 0.42842528522014617\n",
            "Train loss: 0.4242773020445411\n",
            "Train loss: 0.42300378247385934\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.4311813395470381, Val f1: 0.8673470616340637\n",
            "Val loss: 0.4186584750811259, Val f1: 0.8404932022094727\n",
            "Val loss: 0.41296364545822145, Val f1: 0.8332271575927734\n",
            "Val loss: 0.4111719643002126, Val f1: 0.8281036019325256\n",
            "Val loss: 0.4097633163134257, Val f1: 0.8255162835121155\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.9453862905502319, Val f1: 1.5248947143554688\n",
            "Val loss: 0.6433647672335306, Val f1: 1.0154300928115845\n",
            "Val loss: 0.5767126739025116, Val f1: 0.9134420156478882\n",
            "Val loss: 0.552992582321167, Val f1: 0.8653112649917603\n",
            "Val loss: 0.5352237522602081, Val f1: 0.8436765670776367\n",
            "\n",
            "starting Epoch 11\n",
            "Training...\n",
            "Train loss: 0.4280921034514904\n",
            "Train loss: 0.4145910252224315\n",
            "Train loss: 0.41259429514408114\n",
            "Train loss: 0.41070996602969384\n",
            "Train loss: 0.4098891122710137\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.41866106167435646, Val f1: 0.8803586959838867\n",
            "Val loss: 0.40750360669511737, Val f1: 0.8508545756340027\n",
            "Val loss: 0.4047940158843994, Val f1: 0.8411690592765808\n",
            "Val loss: 0.4012583356295059, Val f1: 0.8371142745018005\n",
            "Val loss: 0.39931878199179965, Val f1: 0.8350271582603455\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.947497695684433, Val f1: 1.5307278633117676\n",
            "Val loss: 0.6455682019392649, Val f1: 1.0231902599334717\n",
            "Val loss: 0.5770345151424408, Val f1: 0.9222297668457031\n",
            "Val loss: 0.5544149535042899, Val f1: 0.877153217792511\n",
            "Val loss: 0.536055886083179, Val f1: 0.8560009002685547\n",
            "\n",
            "starting Epoch 12\n",
            "Training...\n",
            "Train loss: 0.42259999550879\n",
            "Train loss: 0.40907856822013855\n",
            "Train loss: 0.40311937510967255\n",
            "Train loss: 0.40030734040843907\n",
            "Train loss: 0.3991092081580843\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.41930404491722584, Val f1: 0.8816112279891968\n",
            "Val loss: 0.40614297173239966, Val f1: 0.8568405508995056\n",
            "Val loss: 0.4034461438655853, Val f1: 0.8461704850196838\n",
            "Val loss: 0.40219372510910034, Val f1: 0.8412523865699768\n",
            "Val loss: 0.4016065260484105, Val f1: 0.8371800184249878\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.9727773368358612, Val f1: 1.5455853939056396\n",
            "Val loss: 0.6643482347329458, Val f1: 1.0284165143966675\n",
            "Val loss: 0.5917069554328919, Val f1: 0.9285715818405151\n",
            "Val loss: 0.5688377363341195, Val f1: 0.882243275642395\n",
            "Val loss: 0.5493812958399454, Val f1: 0.8614547252655029\n",
            "\n",
            "starting Epoch 13\n",
            "Training...\n",
            "Train loss: 0.4066224582493305\n",
            "Train loss: 0.3948252490072539\n",
            "Train loss: 0.3919003641605377\n",
            "Train loss: 0.38794412541745316\n",
            "Train loss: 0.3849948499174345\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.39432414807379246, Val f1: 0.8930115103721619\n",
            "Val loss: 0.3859018286069234, Val f1: 0.8650957345962524\n",
            "Val loss: 0.38326457917690276, Val f1: 0.8534954786300659\n",
            "Val loss: 0.38080706080394006, Val f1: 0.8493912220001221\n",
            "Val loss: 0.38016234657594133, Val f1: 0.8465938568115234\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.9512020647525787, Val f1: 1.5496289730072021\n",
            "Val loss: 0.6503723760445913, Val f1: 1.030709147453308\n",
            "Val loss: 0.5792017996311187, Val f1: 0.929654598236084\n",
            "Val loss: 0.5575807648045676, Val f1: 0.8823599219322205\n",
            "Val loss: 0.5383394161860148, Val f1: 0.8617508411407471\n",
            "\n",
            "starting Epoch 14\n",
            "Training...\n",
            "Train loss: 0.39873853512108326\n",
            "Train loss: 0.3855892537218152\n",
            "Train loss: 0.38009350538253783\n",
            "Train loss: 0.37560695899066643\n",
            "Train loss: 0.3751173274857657\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.389800526201725, Val f1: 0.8873573541641235\n",
            "Val loss: 0.3756343830715526, Val f1: 0.8585200309753418\n",
            "Val loss: 0.36905353724956513, Val f1: 0.8504447340965271\n",
            "Val loss: 0.3661243608638422, Val f1: 0.84607994556427\n",
            "Val loss: 0.36468028880300973, Val f1: 0.8451924324035645\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.9382558763027191, Val f1: 1.5256481170654297\n",
            "Val loss: 0.6424801548322042, Val f1: 1.0129133462905884\n",
            "Val loss: 0.5752536058425903, Val f1: 0.9093416929244995\n",
            "Val loss: 0.5531071977955955, Val f1: 0.8608147501945496\n",
            "Val loss: 0.533939990732405, Val f1: 0.8411398530006409\n",
            "\n",
            "starting Epoch 15\n",
            "Training...\n",
            "Train loss: 0.3869278561323881\n",
            "Train loss: 0.37098439624815277\n",
            "Train loss: 0.366114621758461\n",
            "Train loss: 0.3646239473748563\n",
            "Train loss: 0.36401700547763277\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.37662272714078426, Val f1: 0.8867621421813965\n",
            "Val loss: 0.3608901491670897, Val f1: 0.8614113926887512\n",
            "Val loss: 0.35891735017299653, Val f1: 0.8524523973464966\n",
            "Val loss: 0.3605888579318772, Val f1: 0.8472616076469421\n",
            "Val loss: 0.35859883292799904, Val f1: 0.8451369404792786\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.9464274644851685, Val f1: 1.5204668045043945\n",
            "Val loss: 0.6492719451586405, Val f1: 1.0042227506637573\n",
            "Val loss: 0.5821067333221436, Val f1: 0.9039652943611145\n",
            "Val loss: 0.560106805392674, Val f1: 0.855197548866272\n",
            "Val loss: 0.5405317776732974, Val f1: 0.8356539607048035\n",
            "\n",
            "starting Epoch 16\n",
            "Training...\n",
            "Train loss: 0.3777743689715862\n",
            "Train loss: 0.3586357001102332\n",
            "Train loss: 0.3559353744983673\n",
            "Train loss: 0.354130061704721\n",
            "Train loss: 0.35468246113686336\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.36573381908237934, Val f1: 0.897792398929596\n",
            "Val loss: 0.3522065019968784, Val f1: 0.8714496493339539\n",
            "Val loss: 0.34844442486763, Val f1: 0.8636263012886047\n",
            "Val loss: 0.3469934912759866, Val f1: 0.859710156917572\n",
            "Val loss: 0.34604630193540026, Val f1: 0.8570252656936646\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.9421026408672333, Val f1: 1.523507833480835\n",
            "Val loss: 0.6483291288216909, Val f1: 1.013660192489624\n",
            "Val loss: 0.5790723204612732, Val f1: 0.9133580923080444\n",
            "Val loss: 0.5580127154077802, Val f1: 0.866436779499054\n",
            "Val loss: 0.5380584299564362, Val f1: 0.8457318544387817\n",
            "\n",
            "starting Epoch 17\n",
            "Training...\n",
            "Train loss: 0.36463239789009094\n",
            "Train loss: 0.35270311073823407\n",
            "Train loss: 0.349654997587204\n",
            "Train loss: 0.3467459229391013\n",
            "Train loss: 0.34628497773692724\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.35709428414702415, Val f1: 0.895897388458252\n",
            "Val loss: 0.34891402992335235, Val f1: 0.8677549362182617\n",
            "Val loss: 0.3456231224536896, Val f1: 0.8589483499526978\n",
            "Val loss: 0.3421770489927548, Val f1: 0.8554357886314392\n",
            "Val loss: 0.3433610342797779, Val f1: 0.8514921069145203\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.9574041664600372, Val f1: 1.508001685142517\n",
            "Val loss: 0.6585701803366343, Val f1: 0.9999213218688965\n",
            "Val loss: 0.5901773869991302, Val f1: 0.901441752910614\n",
            "Val loss: 0.5687796516077859, Val f1: 0.8538941740989685\n",
            "Val loss: 0.5485140350129869, Val f1: 0.8339249491691589\n",
            "\n",
            "starting Epoch 18\n",
            "Training...\n",
            "Train loss: 0.35900128819048405\n",
            "Train loss: 0.3473411933942275\n",
            "Train loss: 0.34133425116539\n",
            "Train loss: 0.3410008269459454\n",
            "Train loss: 0.33945474809124354\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.3593872133642435, Val f1: 0.8918019533157349\n",
            "Val loss: 0.3487888448166125, Val f1: 0.8636386394500732\n",
            "Val loss: 0.34657060146331786, Val f1: 0.8550888299942017\n",
            "Val loss: 0.3442204901531561, Val f1: 0.8515453338623047\n",
            "Val loss: 0.34169062901110875, Val f1: 0.8488850593566895\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.9736645817756653, Val f1: 1.4952099323272705\n",
            "Val loss: 0.6703193386395773, Val f1: 0.9922618865966797\n",
            "Val loss: 0.6015563130378723, Val f1: 0.8929578065872192\n",
            "Val loss: 0.5797981875283378, Val f1: 0.8465656638145447\n",
            "Val loss: 0.5590696665975783, Val f1: 0.8266403675079346\n",
            "\n",
            "starting Epoch 19\n",
            "Training...\n",
            "Train loss: 0.34383910335600376\n",
            "Train loss: 0.3350513776143392\n",
            "Train loss: 0.33412670850753784\n",
            "Train loss: 0.3334040744091148\n",
            "Train loss: 0.33236076895679745\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.34343361109495163, Val f1: 0.9085479974746704\n",
            "Val loss: 0.32844863335291546, Val f1: 0.883578896522522\n",
            "Val loss: 0.3241757720708847, Val f1: 0.875205397605896\n",
            "Val loss: 0.3250259685872206, Val f1: 0.8693128824234009\n",
            "Val loss: 0.323998660558746, Val f1: 0.8664246797561646\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.9597476720809937, Val f1: 1.530957818031311\n",
            "Val loss: 0.6632272998491923, Val f1: 1.0149179697036743\n",
            "Val loss: 0.5928470611572265, Val f1: 0.912273108959198\n",
            "Val loss: 0.5724905984742301, Val f1: 0.865175724029541\n",
            "Val loss: 0.5516089234087203, Val f1: 0.8447167873382568\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yfAIhD9cvZ4w",
        "outputId": "50d9aafe-d8ff-438e-b224-d3c3d5fa6ca0"
      },
      "source": [
        "print(\"Loss: \", losses_eval[-1])\n",
        "print(\"f1: \", f1s_eval[-1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss:  0.4964480310678482\n",
            "f1:  tensor(0.7602)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "hVcHKjji2HL-",
        "outputId": "e757f9bc-c0ad-4e22-976e-2dea2d34dc84"
      },
      "source": [
        "plt.plot(losses)\n",
        "plt.plot(losses_eval)\n",
        "plt.title('BCE loss value')\n",
        "plt.ylabel('BCE loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "mfzcvnbE2REk",
        "outputId": "f4f8c610-7ec3-436b-e00d-6af3808550ad"
      },
      "source": [
        "plt.plot(f1s)\n",
        "plt.plot(f1s_eval)\n",
        "plt.title('f1 value')\n",
        "plt.ylabel('f1 value')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwQLDFXU5Tfu"
      },
      "source": [
        "Попробуем добавить в нашу модель еще один пулинг, функцию активации reLU и дропаут"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vXUyDTU2ZlI"
      },
      "source": [
        "class CNN(nn.Module):\n",
        "    \n",
        "    def __init__(self, vocab_size, vocab_size_sym, embedding_dim):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, 100)\n",
        "        self.embedding.from_pretrained(torch.tensor(weights), freeze=True)\n",
        "        self.embedding_sym = nn.Embedding(vocab_size_sym, embedding_dim)\n",
        "\n",
        "        self.bigrams = nn.Conv1d(in_channels=embedding_dim, out_channels=100, kernel_size=2, padding='same')\n",
        "        self.trigrams = nn.Conv1d(in_channels=embedding_dim, out_channels=80, kernel_size=3, padding='same')\n",
        "        self.pooling = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "\n",
        "        self.hidden = nn.Linear(in_features=100, out_features=100)\n",
        "        self.hidden_sym = nn.Linear(in_features=180, out_features=100)\n",
        "        self.hidden_final = nn.Linear(in_features=200, out_features=1)\n",
        "        self.out = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, word, sym):\n",
        "        embedded = self.embedding(word)\n",
        "\n",
        "        embedded_sent = torch.mean(embedded, dim=1)\n",
        "\n",
        "        linear_lay = self.hidden(embedded_sent)\n",
        "\n",
        "        embedded_sym = self.embedding_sym(sym)\n",
        "        embedded_sym = embedded_sym.transpose(1,2)\n",
        "\n",
        "        feature_map_bigrams = self.dropout(self.pooling(self.relu(self.bigrams(embedded_sym))))\n",
        "        feature_map_trigrams = self.dropout(self.pooling(self.relu(self.trigrams(embedded_sym))))\n",
        "        concat_ngrams = torch.cat((feature_map_bigrams, feature_map_trigrams), 1)\n",
        "\n",
        "\n",
        "        pooling = concat_ngrams.max(2)[0]\n",
        "        linear_sym = self.hidden_sym(pooling)\n",
        "\n",
        "\n",
        "        concat = torch.cat((linear_lay, linear_sym), 1)\n",
        "        \n",
        "        logits = self.hidden_final(concat)\n",
        "        logits = self.out(logits)      \n",
        "        return logits"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAhGw3z-6Ln1"
      },
      "source": [
        "model = CNN(len(word2id), len(symbol2id), 8)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
        "criterion = nn.BCELoss()  \n",
        "\n",
        "# веса модели и значения лосса храним там же, где и все остальные тензоры\n",
        "model = model.to(DEVICE)\n",
        "criterion = criterion.to(DEVICE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QoNjHWTR6Vaj",
        "outputId": "bf1d75e0-e147-4ae3-9031-ae6dbcb0fc9e"
      },
      "source": [
        "losses = []\n",
        "losses_eval = []\n",
        "f1s = []\n",
        "f1s_eval = []\n",
        "\n",
        "for i in range(20):\n",
        "    print(f'\\nstarting Epoch {i}')\n",
        "    print('Training...')\n",
        "    epoch_loss = train(model, train_iterator, optimizer, criterion)\n",
        "    losses.append(epoch_loss)\n",
        "    print('\\nEvaluating on train...')\n",
        "    f1_on_train,_ = evaluate(model, train_iterator, criterion)\n",
        "    f1s.append(f1_on_train)\n",
        "    print('\\nEvaluating on test...')\n",
        "    f1_on_test, epoch_loss_on_test = evaluate(model, val_iterator, criterion)\n",
        "    losses_eval.append(epoch_loss_on_test)\n",
        "    f1s_eval.append(f1_on_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "starting Epoch 0\n",
            "Training...\n",
            "Train loss: 0.7482596412301064\n",
            "Train loss: 0.714286435734142\n",
            "Train loss: 0.7013940274715423\n",
            "Train loss: 0.6928700093013137\n",
            "Train loss: 0.686528293859391\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.7082443721592426, Val f1: 0.6006394624710083\n",
            "Val loss: 0.6877013101722255, Val f1: 0.5767652988433838\n",
            "Val loss: 0.6808474433422088, Val f1: 0.5696582198143005\n",
            "Val loss: 0.6775960975618505, Val f1: 0.5651649236679077\n",
            "Val loss: 0.6756237198909124, Val f1: 0.5631484985351562\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.3315457701683044, Val f1: 1.1448811292648315\n",
            "Val loss: 0.8901570836702982, Val f1: 0.7576505541801453\n",
            "Val loss: 0.801436722278595, Val f1: 0.6722220182418823\n",
            "Val loss: 0.7633493031774249, Val f1: 0.6391071081161499\n",
            "Val loss: 0.7420355081558228, Val f1: 0.6207618713378906\n",
            "\n",
            "starting Epoch 1\n",
            "Training...\n",
            "Train loss: 0.700221698731184\n",
            "Train loss: 0.6754381981762972\n",
            "Train loss: 0.6656170642375946\n",
            "Train loss: 0.659931085002956\n",
            "Train loss: 0.6564695480323973\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.6888697780668736, Val f1: 0.6259151101112366\n",
            "Val loss: 0.6666248260122357, Val f1: 0.6115030646324158\n",
            "Val loss: 0.6596323812007904, Val f1: 0.6076040863990784\n",
            "Val loss: 0.656406846509051, Val f1: 0.6040502190589905\n",
            "Val loss: 0.6542822363830748, Val f1: 0.6024068593978882\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.2882345914840698, Val f1: 1.21608567237854\n",
            "Val loss: 0.862697184085846, Val f1: 0.8083117604255676\n",
            "Val loss: 0.7765098452568054, Val f1: 0.7219184637069702\n",
            "Val loss: 0.7396824104445321, Val f1: 0.6828898191452026\n",
            "Val loss: 0.7191982732878791, Val f1: 0.6634129285812378\n",
            "\n",
            "starting Epoch 2\n",
            "Training...\n",
            "Train loss: 0.6721598654985428\n",
            "Train loss: 0.6500988963878516\n",
            "Train loss: 0.6406670904159546\n",
            "Train loss: 0.6341934969176107\n",
            "Train loss: 0.6293655229466302\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.6536321602761745, Val f1: 0.7523935437202454\n",
            "Val loss: 0.633195450811675, Val f1: 0.7306493520736694\n",
            "Val loss: 0.6277676594257354, Val f1: 0.7245035767555237\n",
            "Val loss: 0.6245999069356206, Val f1: 0.7204810380935669\n",
            "Val loss: 0.6228102835870925, Val f1: 0.7171227335929871\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.2300788164138794, Val f1: 1.4180570840835571\n",
            "Val loss: 0.8237939675649008, Val f1: 0.9410261511802673\n",
            "Val loss: 0.7400163292884827, Val f1: 0.846985936164856\n",
            "Val loss: 0.7062131251607623, Val f1: 0.8032470941543579\n",
            "Val loss: 0.6867905259132385, Val f1: 0.7795892357826233\n",
            "\n",
            "starting Epoch 3\n",
            "Training...\n",
            "Train loss: 0.6527259796857834\n",
            "Train loss: 0.6227251851197445\n",
            "Train loss: 0.6117519116401673\n",
            "Train loss: 0.6060630394451654\n",
            "Train loss: 0.6029207756121954\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.6371248587965965, Val f1: 0.6402880549430847\n",
            "Val loss: 0.6159573576667092, Val f1: 0.6280601024627686\n",
            "Val loss: 0.6098838126659394, Val f1: 0.6272403597831726\n",
            "Val loss: 0.6061735526839299, Val f1: 0.624620258808136\n",
            "Val loss: 0.6043101407232738, Val f1: 0.6231145858764648\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.2046610713005066, Val f1: 1.1915596723556519\n",
            "Val loss: 0.80806831518809, Val f1: 0.7981804013252258\n",
            "Val loss: 0.7263959288597107, Val f1: 0.7271954417228699\n",
            "Val loss: 0.6921425887516567, Val f1: 0.6888341903686523\n",
            "Val loss: 0.6733327243063185, Val f1: 0.668409526348114\n",
            "\n",
            "starting Epoch 4\n",
            "Training...\n",
            "Train loss: 0.6108696572482586\n",
            "Train loss: 0.5904766736608563\n",
            "Train loss: 0.5833530759811402\n",
            "Train loss: 0.5770752510028099\n",
            "Train loss: 0.5740560591220856\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5945805609226227, Val f1: 0.7519069314002991\n",
            "Val loss: 0.574274176901037, Val f1: 0.7294930815696716\n",
            "Val loss: 0.5703800857067108, Val f1: 0.7180588245391846\n",
            "Val loss: 0.567067277965261, Val f1: 0.7158954739570618\n",
            "Val loss: 0.5650022072451455, Val f1: 0.7132803797721863\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.1399597525596619, Val f1: 1.3640477657318115\n",
            "Val loss: 0.7639931837717692, Val f1: 0.912833034992218\n",
            "Val loss: 0.6853356242179871, Val f1: 0.8270443081855774\n",
            "Val loss: 0.6546432375907898, Val f1: 0.7864841818809509\n",
            "Val loss: 0.6367179089122348, Val f1: 0.7629121541976929\n",
            "\n",
            "starting Epoch 5\n",
            "Training...\n",
            "Train loss: 0.5824073255062103\n",
            "Train loss: 0.565755439527107\n",
            "Train loss: 0.5553603786230087\n",
            "Train loss: 0.549720576894817\n",
            "Train loss: 0.5461882200269472\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5766786187887192, Val f1: 0.7349750399589539\n",
            "Val loss: 0.5618332227071127, Val f1: 0.7060849070549011\n",
            "Val loss: 0.5545087456703186, Val f1: 0.7001064419746399\n",
            "Val loss: 0.5519385738159294, Val f1: 0.6950816512107849\n",
            "Val loss: 0.5498943123079482, Val f1: 0.6916926503181458\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.128317654132843, Val f1: 1.3085355758666992\n",
            "Val loss: 0.7560067176818848, Val f1: 0.8754352927207947\n",
            "Val loss: 0.6784496784210206, Val f1: 0.7929887771606445\n",
            "Val loss: 0.6481200286320278, Val f1: 0.7525637745857239\n",
            "Val loss: 0.630074143409729, Val f1: 0.7323609590530396\n",
            "\n",
            "starting Epoch 6\n",
            "Training...\n",
            "Train loss: 0.5610318370163441\n",
            "Train loss: 0.5372396526914655\n",
            "Train loss: 0.5300253134965897\n",
            "Train loss: 0.5251513799624656\n",
            "Train loss: 0.5209231369552159\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5591661538928747, Val f1: 0.7301424145698547\n",
            "Val loss: 0.5421198507150015, Val f1: 0.7080123424530029\n",
            "Val loss: 0.5364935404062271, Val f1: 0.7013387084007263\n",
            "Val loss: 0.5328755294209095, Val f1: 0.698052704334259\n",
            "Val loss: 0.5311219688682329, Val f1: 0.6970988512039185\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.1132739782333374, Val f1: 1.2934091091156006\n",
            "Val loss: 0.7451022267341614, Val f1: 0.8697819113731384\n",
            "Val loss: 0.668956971168518, Val f1: 0.7877622246742249\n",
            "Val loss: 0.6393850445747375, Val f1: 0.7495169639587402\n",
            "Val loss: 0.6212796237733629, Val f1: 0.7327209115028381\n",
            "\n",
            "starting Epoch 7\n",
            "Training...\n",
            "Train loss: 0.5285349618643522\n",
            "Train loss: 0.5101748235297926\n",
            "Train loss: 0.5047172504663467\n",
            "Train loss: 0.5028768768951074\n",
            "Train loss: 0.4995122885420209\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.532027630135417, Val f1: 0.7565895318984985\n",
            "Val loss: 0.518087240782651, Val f1: 0.734684407711029\n",
            "Val loss: 0.5140640264749528, Val f1: 0.7272196412086487\n",
            "Val loss: 0.5113353511290764, Val f1: 0.7233865857124329\n",
            "Val loss: 0.5096824162063145, Val f1: 0.7206997871398926\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.0967810153961182, Val f1: 1.3311526775360107\n",
            "Val loss: 0.7329796155293783, Val f1: 0.8866727948188782\n",
            "Val loss: 0.6572245717048645, Val f1: 0.8029481172561646\n",
            "Val loss: 0.6289931961468288, Val f1: 0.7620893120765686\n",
            "Val loss: 0.6105512181917826, Val f1: 0.7458947896957397\n",
            "\n",
            "starting Epoch 8\n",
            "Training...\n",
            "Train loss: 0.504888629540801\n",
            "Train loss: 0.48961432684551587\n",
            "Train loss: 0.4825089603662491\n",
            "Train loss: 0.47761995000625723\n",
            "Train loss: 0.47736030994426637\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5057467333972454, Val f1: 0.794589638710022\n",
            "Val loss: 0.4911080174373858, Val f1: 0.7701522707939148\n",
            "Val loss: 0.485447102189064, Val f1: 0.7625816464424133\n",
            "Val loss: 0.4824064013673298, Val f1: 0.7590262293815613\n",
            "Val loss: 0.48158582725695204, Val f1: 0.7573860883712769\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.065091609954834, Val f1: 1.37595796585083\n",
            "Val loss: 0.7116589148839315, Val f1: 0.9265920519828796\n",
            "Val loss: 0.6370095252990723, Val f1: 0.839011013507843\n",
            "Val loss: 0.6106344205992562, Val f1: 0.795530378818512\n",
            "Val loss: 0.5920933485031128, Val f1: 0.7801324725151062\n",
            "\n",
            "starting Epoch 9\n",
            "Training...\n",
            "Train loss: 0.4806222189217806\n",
            "Train loss: 0.4657847321394718\n",
            "Train loss: 0.4639160805940628\n",
            "Train loss: 0.46263755924666106\n",
            "Train loss: 0.46015982010534834\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5229814723134041, Val f1: 0.7420692443847656\n",
            "Val loss: 0.5075154160008286, Val f1: 0.7196235656738281\n",
            "Val loss: 0.502326717376709, Val f1: 0.7138998508453369\n",
            "Val loss: 0.4982007386079475, Val f1: 0.7118088006973267\n",
            "Val loss: 0.49704450652712867, Val f1: 0.7098417282104492\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.1208805441856384, Val f1: 1.2797529697418213\n",
            "Val loss: 0.7477235396703085, Val f1: 0.8584343791007996\n",
            "Val loss: 0.6721709489822387, Val f1: 0.7729891538619995\n",
            "Val loss: 0.6431885191372463, Val f1: 0.7339600920677185\n",
            "Val loss: 0.6237209704187181, Val f1: 0.7178080081939697\n",
            "\n",
            "starting Epoch 10\n",
            "Training...\n",
            "Train loss: 0.47166479378938675\n",
            "Train loss: 0.45477434450929816\n",
            "Train loss: 0.44832476794719694\n",
            "Train loss: 0.44468094163866184\n",
            "Train loss: 0.4428418654771078\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.4740769751369953, Val f1: 0.808659553527832\n",
            "Val loss: 0.459663731582237, Val f1: 0.787715494632721\n",
            "Val loss: 0.45424935579299924, Val f1: 0.7825521230697632\n",
            "Val loss: 0.4522274901617819, Val f1: 0.7796441912651062\n",
            "Val loss: 0.4513789213129452, Val f1: 0.776546835899353\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.0507442951202393, Val f1: 1.4067556858062744\n",
            "Val loss: 0.7025970816612244, Val f1: 0.9434705376625061\n",
            "Val loss: 0.6291231274604797, Val f1: 0.8509887456893921\n",
            "Val loss: 0.6037852338382176, Val f1: 0.8054829835891724\n",
            "Val loss: 0.5846516059504615, Val f1: 0.7874332666397095\n",
            "\n",
            "starting Epoch 11\n",
            "Training...\n",
            "Train loss: 0.45267415046691895\n",
            "Train loss: 0.4392419529683662\n",
            "Train loss: 0.432871111035347\n",
            "Train loss: 0.4284520989923335\n",
            "Train loss: 0.42724278072516125\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.4769125487655401, Val f1: 0.7942766547203064\n",
            "Val loss: 0.46394435835607123, Val f1: 0.7704945206642151\n",
            "Val loss: 0.45880572497844696, Val f1: 0.7618377208709717\n",
            "Val loss: 0.454727336986741, Val f1: 0.7626714110374451\n",
            "Val loss: 0.45316477297317415, Val f1: 0.7603713870048523\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.0754219889640808, Val f1: 1.3594281673431396\n",
            "Val loss: 0.7185502847035726, Val f1: 0.9130415320396423\n",
            "Val loss: 0.6454282522201538, Val f1: 0.8214866518974304\n",
            "Val loss: 0.6189640504973275, Val f1: 0.7816473245620728\n",
            "Val loss: 0.5994996792740293, Val f1: 0.7631351351737976\n",
            "\n",
            "starting Epoch 12\n",
            "Training...\n",
            "Train loss: 0.44035034254193306\n",
            "Train loss: 0.42462734471667896\n",
            "Train loss: 0.41887002050876615\n",
            "Train loss: 0.41748783926465616\n",
            "Train loss: 0.4153254936848368\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.4738075006753206, Val f1: 0.7922633290290833\n",
            "Val loss: 0.46480054476044397, Val f1: 0.7631701231002808\n",
            "Val loss: 0.46127995193004606, Val f1: 0.754050076007843\n",
            "Val loss: 0.461411626926109, Val f1: 0.7478744387626648\n",
            "Val loss: 0.45907191861243474, Val f1: 0.7454598546028137\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.111083447933197, Val f1: 1.3207640647888184\n",
            "Val loss: 0.7422322432200114, Val f1: 0.8921071887016296\n",
            "Val loss: 0.6681139826774597, Val f1: 0.7990186214447021\n",
            "Val loss: 0.6406158975192479, Val f1: 0.7578595280647278\n",
            "Val loss: 0.620506571398841, Val f1: 0.7396185994148254\n",
            "\n",
            "starting Epoch 13\n",
            "Training...\n",
            "Train loss: 0.4307597391307354\n",
            "Train loss: 0.4174371741034768\n",
            "Train loss: 0.40944419920444486\n",
            "Train loss: 0.40544142562951613\n",
            "Train loss: 0.4031233326310203\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.4523636344820261, Val f1: 0.8178443312644958\n",
            "Val loss: 0.4370616511865096, Val f1: 0.7929275035858154\n",
            "Val loss: 0.43238947927951815, Val f1: 0.7870209217071533\n",
            "Val loss: 0.42893771627056065, Val f1: 0.7829535007476807\n",
            "Val loss: 0.43019172868558336, Val f1: 0.7800455689430237\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.0721908807754517, Val f1: 1.3874046802520752\n",
            "Val loss: 0.7183534900347391, Val f1: 0.9302014708518982\n",
            "Val loss: 0.6447109103202819, Val f1: 0.8358634114265442\n",
            "Val loss: 0.6194392357553754, Val f1: 0.7937881350517273\n",
            "Val loss: 0.5996126731236776, Val f1: 0.774719774723053\n",
            "\n",
            "starting Epoch 14\n",
            "Training...\n",
            "Train loss: 0.41116813570261\n",
            "Train loss: 0.39776545311465406\n",
            "Train loss: 0.3948617368936539\n",
            "Train loss: 0.3928428932802001\n",
            "Train loss: 0.39082929775828407\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.417904419824481, Val f1: 0.8527582883834839\n",
            "Val loss: 0.4072178647373662, Val f1: 0.8270602226257324\n",
            "Val loss: 0.403808970451355, Val f1: 0.8208661079406738\n",
            "Val loss: 0.40225935649515976, Val f1: 0.8172160387039185\n",
            "Val loss: 0.40114702922957285, Val f1: 0.8146932721138\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.0338845252990723, Val f1: 1.443561315536499\n",
            "Val loss: 0.6951155265172323, Val f1: 0.9625119566917419\n",
            "Val loss: 0.6214937090873718, Val f1: 0.870444118976593\n",
            "Val loss: 0.5985078300748553, Val f1: 0.826658308506012\n",
            "Val loss: 0.5788221193684472, Val f1: 0.8070356249809265\n",
            "\n",
            "starting Epoch 15\n",
            "Training...\n",
            "Train loss: 0.39777666330337524\n",
            "Train loss: 0.38892279520179285\n",
            "Train loss: 0.38299393832683565\n",
            "Train loss: 0.3816012273083872\n",
            "Train loss: 0.3806435948326474\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.42080552130937576, Val f1: 0.8436094522476196\n",
            "Val loss: 0.41063381054184656, Val f1: 0.8187357187271118\n",
            "Val loss: 0.4049096751213074, Val f1: 0.8122109770774841\n",
            "Val loss: 0.40298626182684255, Val f1: 0.8075204491615295\n",
            "Val loss: 0.40156249701976776, Val f1: 0.8064473271369934\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.053658664226532, Val f1: 1.4226064682006836\n",
            "Val loss: 0.7094356616338094, Val f1: 0.9471728801727295\n",
            "Val loss: 0.6359037399291992, Val f1: 0.8516073226928711\n",
            "Val loss: 0.6122294238635472, Val f1: 0.8081035614013672\n",
            "Val loss: 0.5923650860786438, Val f1: 0.7891291379928589\n",
            "\n",
            "starting Epoch 16\n",
            "Training...\n",
            "Train loss: 0.3821596521884203\n",
            "Train loss: 0.37428424846042285\n",
            "Train loss: 0.37203279316425325\n",
            "Train loss: 0.3701130583215116\n",
            "Train loss: 0.37080942058847066\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.44044329039752483, Val f1: 0.818517804145813\n",
            "Val loss: 0.42943527752702887, Val f1: 0.7941017746925354\n",
            "Val loss: 0.4264553594589233, Val f1: 0.7866955399513245\n",
            "Val loss: 0.4229619356233682, Val f1: 0.7819312810897827\n",
            "Val loss: 0.4207659782398315, Val f1: 0.7805399298667908\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.1139895915985107, Val f1: 1.3410345315933228\n",
            "Val loss: 0.7502627968788147, Val f1: 0.907985508441925\n",
            "Val loss: 0.6751426696777344, Val f1: 0.814125657081604\n",
            "Val loss: 0.6493955765451703, Val f1: 0.7713571190834045\n",
            "Val loss: 0.6288573212093778, Val f1: 0.7534372806549072\n",
            "\n",
            "starting Epoch 17\n",
            "Training...\n",
            "Train loss: 0.38612446188926697\n",
            "Train loss: 0.36890727733120776\n",
            "Train loss: 0.36533326506614683\n",
            "Train loss: 0.36340221095440994\n",
            "Train loss: 0.3631412454304241\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.4254365377128124, Val f1: 0.8336419463157654\n",
            "Val loss: 0.41247879736351245, Val f1: 0.8100996017456055\n",
            "Val loss: 0.4082543057203293, Val f1: 0.8028373122215271\n",
            "Val loss: 0.40856001759642985, Val f1: 0.7955363988876343\n",
            "Val loss: 0.40543743542262484, Val f1: 0.7942928075790405\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.1025364995002747, Val f1: 1.360568642616272\n",
            "Val loss: 0.744242250919342, Val f1: 0.9182205200195312\n",
            "Val loss: 0.6693865895271301, Val f1: 0.8241297006607056\n",
            "Val loss: 0.6445444737161908, Val f1: 0.7815303802490234\n",
            "Val loss: 0.6239171226819357, Val f1: 0.7635270953178406\n",
            "\n",
            "starting Epoch 18\n",
            "Training...\n",
            "Train loss: 0.3667408153414726\n",
            "Train loss: 0.354212915355509\n",
            "Train loss: 0.35577852368354795\n",
            "Train loss: 0.35391190559116764\n",
            "Train loss: 0.35334535546246026\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.4135322328656912, Val f1: 0.8516684174537659\n",
            "Val loss: 0.39731627793023083, Val f1: 0.8264503479003906\n",
            "Val loss: 0.3914127230644226, Val f1: 0.8186720013618469\n",
            "Val loss: 0.38910974851295127, Val f1: 0.8144161105155945\n",
            "Val loss: 0.3882442240913709, Val f1: 0.8119769096374512\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.085523247718811, Val f1: 1.3958032131195068\n",
            "Val loss: 0.7350050409634908, Val f1: 0.9318012595176697\n",
            "Val loss: 0.6594826936721802, Val f1: 0.8374580144882202\n",
            "Val loss: 0.6360924073628017, Val f1: 0.7961010932922363\n",
            "Val loss: 0.6155285967720879, Val f1: 0.7781116366386414\n",
            "\n",
            "starting Epoch 19\n",
            "Training...\n",
            "Train loss: 0.3680516555905342\n",
            "Train loss: 0.35283583041393396\n",
            "Train loss: 0.3472150725126266\n",
            "Train loss: 0.3473620321323623\n",
            "Train loss: 0.34863356962090447\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.40768993832170963, Val f1: 0.8498736023902893\n",
            "Val loss: 0.3960184996778315, Val f1: 0.8217073082923889\n",
            "Val loss: 0.3933341056108475, Val f1: 0.8134214282035828\n",
            "Val loss: 0.39271578504078425, Val f1: 0.8088130354881287\n",
            "Val loss: 0.3896280452609062, Val f1: 0.8067395091056824\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.1074992418289185, Val f1: 1.3763060569763184\n",
            "Val loss: 0.7505817612012228, Val f1: 0.9210991263389587\n",
            "Val loss: 0.6748016357421875, Val f1: 0.8270342946052551\n",
            "Val loss: 0.6507324320929391, Val f1: 0.7862001657485962\n",
            "Val loss: 0.629870679643419, Val f1: 0.7687196135520935\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ev6TacC96W8r",
        "outputId": "d8c0616f-af42-4da9-a5cf-31dbb4f35f56"
      },
      "source": [
        "print(\"Loss: \", losses_eval[-1])\n",
        "print(\"f1: \", f1s_eval[-1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss:  0.5668836116790772\n",
            "f1:  tensor(0.6918)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "yffndiKK6hjb",
        "outputId": "80d5600a-979b-4024-fde8-6a80cbf5a795"
      },
      "source": [
        "plt.plot(losses)\n",
        "plt.plot(losses_eval)\n",
        "plt.title('BCE loss value')\n",
        "plt.ylabel('BCE loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "4oXuxrfn6iXB",
        "outputId": "0db07828-c861-44c5-ccff-0d6dd812b8c7"
      },
      "source": [
        "plt.plot(f1s)\n",
        "plt.plot(f1s_eval)\n",
        "plt.title('f1 value')\n",
        "plt.ylabel('f1 value')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQsrefbXKHIr"
      },
      "source": [
        "Как мы видим, качество работы модели ухудшилось, поэтому попробуем сделать что-нибудь еще. Можно, например, покрутить гиперпараметры"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCy0JbSQMZWR"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    epoch_loss = 0\n",
        "    epoch_metric = 0\n",
        "    model.eval() \n",
        "    with torch.no_grad():\n",
        "        for i, (texts, symbols, ys) in enumerate(iterator):   \n",
        "            preds = model(texts, symbols)\n",
        "            loss = criterion(preds, ys)\n",
        "            epoch_loss += loss.item()\n",
        "            batch_metric = f1(preds.round().long(), ys.long(), ignore_index=0)\n",
        "            epoch_metric += batch_metric\n",
        "\n",
        "            # if not (i + 1) % int(len(iterator)/5):\n",
        "              # print(f'Val loss: {epoch_loss/i}, Val f1: {epoch_metric/i}')\n",
        "        \n",
        "    return epoch_metric / len(iterator), epoch_loss / len(iterator)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gXrrz5ALLa-j",
        "outputId": "15817c36-5c5f-48d3-b463-dac3e992aa66"
      },
      "source": [
        "for i in [10, 20, 30, 40, 50, 60, 70, 80, 90]:\n",
        "  model = CNN(len(word2id), len(symbol2id), i)\n",
        "  optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
        "  criterion = nn.BCELoss()  \n",
        "\n",
        "  # веса модели и значения лосса храним там же, где и все остальные тензоры\n",
        "  model = model.to(DEVICE)\n",
        "  criterion = criterion.to(DEVICE)\n",
        "  losses = []\n",
        "  losses_eval = []\n",
        "  f1s = []\n",
        "  f1s_eval = []\n",
        "\n",
        "  for y in range(20):\n",
        "      print(f'\\nstarting Epoch {y}')\n",
        "      # print('Training...')\n",
        "      epoch_loss = train(model, train_iterator, optimizer, criterion)\n",
        "      losses.append(epoch_loss)\n",
        "      # print('\\nEvaluating on train...')\n",
        "      f1_on_train,_ = evaluate(model, train_iterator, criterion)\n",
        "      f1s.append(f1_on_train)\n",
        "      # print('\\nEvaluating on test...')\n",
        "      f1_on_test, epoch_loss_on_test = evaluate(model, val_iterator, criterion)\n",
        "      losses_eval.append(epoch_loss_on_test)\n",
        "      f1s_eval.append(f1_on_test)\n",
        "      print('embedding_dim = ', i, '; f1 = ', f1_on_test, '; loss = ', epoch_loss_on_test)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "starting Epoch 0\n",
            "Train loss: 0.7524196729063988\n",
            "Train loss: 0.7195435094110894\n",
            "Train loss: 0.7057993733882904\n",
            "Train loss: 0.6962288306720221\n",
            "Train loss: 0.6886791402385348\n",
            "embedding_dim =  10 ; f1 =  tensor(0.5297, device='cuda:0') ; loss =  0.6647065699100494\n",
            "\n",
            "starting Epoch 1\n",
            "Train loss: 0.6920825950801373\n",
            "Train loss: 0.6695371667544047\n",
            "Train loss: 0.6591951358318329\n",
            "Train loss: 0.6518963139448593\n",
            "Train loss: 0.6469437863145556\n",
            "embedding_dim =  10 ; f1 =  tensor(0.6151, device='cuda:0') ; loss =  0.6334054052829743\n",
            "\n",
            "starting Epoch 2\n",
            "Train loss: 0.6629443690180779\n",
            "Train loss: 0.6381992878335895\n",
            "Train loss: 0.6291533851623535\n",
            "Train loss: 0.6232151317952285\n",
            "Train loss: 0.6175318679639271\n",
            "embedding_dim =  10 ; f1 =  tensor(0.6332, device='cuda:0') ; loss =  0.6085733354091645\n",
            "\n",
            "starting Epoch 3\n",
            "Train loss: 0.6266495659947395\n",
            "Train loss: 0.6062353018558386\n",
            "Train loss: 0.5984465050697326\n",
            "Train loss: 0.5931078658175113\n",
            "Train loss: 0.5883338082404364\n",
            "embedding_dim =  10 ; f1 =  tensor(0.6607, device='cuda:0') ; loss =  0.5845271408557892\n",
            "\n",
            "starting Epoch 4\n",
            "Train loss: 0.6011471636593342\n",
            "Train loss: 0.5813172640222491\n",
            "Train loss: 0.5719155430793762\n",
            "Train loss: 0.5664066610051625\n",
            "Train loss: 0.5616874496142069\n",
            "embedding_dim =  10 ; f1 =  tensor(0.7055, device='cuda:0') ; loss =  0.560811024904251\n",
            "\n",
            "starting Epoch 5\n",
            "Train loss: 0.5770623721182346\n",
            "Train loss: 0.5571611975178574\n",
            "Train loss: 0.546685299873352\n",
            "Train loss: 0.5407922792790542\n",
            "Train loss: 0.5355614303123384\n",
            "embedding_dim =  10 ; f1 =  tensor(0.6713, device='cuda:0') ; loss =  0.5568410694599152\n",
            "\n",
            "starting Epoch 6\n",
            "Train loss: 0.5527115147560835\n",
            "Train loss: 0.5252854652477034\n",
            "Train loss: 0.51626540184021\n",
            "Train loss: 0.5128581199183393\n",
            "Train loss: 0.5105313491963205\n",
            "embedding_dim =  10 ; f1 =  tensor(0.6576, device='cuda:0') ; loss =  0.5535036206245423\n",
            "\n",
            "starting Epoch 7\n",
            "Train loss: 0.5196906849741936\n",
            "Train loss: 0.5019121413881128\n",
            "Train loss: 0.49423965454101565\n",
            "Train loss: 0.4904261355969443\n",
            "Train loss: 0.4891845395877248\n",
            "embedding_dim =  10 ; f1 =  tensor(0.7114, device='cuda:0') ; loss =  0.5289106070995331\n",
            "\n",
            "starting Epoch 8\n",
            "Train loss: 0.49529800564050674\n",
            "Train loss: 0.4806599300919157\n",
            "Train loss: 0.47364008784294126\n",
            "Train loss: 0.4697995995407674\n",
            "Train loss: 0.4692025486202467\n",
            "embedding_dim =  10 ; f1 =  tensor(0.7266, device='cuda:0') ; loss =  0.5173366874456405\n",
            "\n",
            "starting Epoch 9\n",
            "Train loss: 0.4799414984881878\n",
            "Train loss: 0.467317928870519\n",
            "Train loss: 0.457718790769577\n",
            "Train loss: 0.45564240617538565\n",
            "Train loss: 0.4540180471681413\n",
            "embedding_dim =  10 ; f1 =  tensor(0.7125, device='cuda:0') ; loss =  0.519740667939186\n",
            "\n",
            "starting Epoch 10\n",
            "Train loss: 0.46119384467601776\n",
            "Train loss: 0.4453670861143054\n",
            "Train loss: 0.44246280550956724\n",
            "Train loss: 0.43957586163905127\n",
            "Train loss: 0.43636227682942436\n",
            "embedding_dim =  10 ; f1 =  tensor(0.7247, device='cuda:0') ; loss =  0.5121727138757706\n",
            "\n",
            "starting Epoch 11\n",
            "Train loss: 0.446407338604331\n",
            "Train loss: 0.4310165192141677\n",
            "Train loss: 0.42601770520210264\n",
            "Train loss: 0.4231478047015062\n",
            "Train loss: 0.42251411648023696\n",
            "embedding_dim =  10 ; f1 =  tensor(0.6965, device='cuda:0') ; loss =  0.5288789451122284\n",
            "\n",
            "starting Epoch 12\n",
            "Train loss: 0.4323268234729767\n",
            "Train loss: 0.42032434994524176\n",
            "Train loss: 0.416429563164711\n",
            "Train loss: 0.41291099564353034\n",
            "Train loss: 0.4106605180672237\n",
            "embedding_dim =  10 ; f1 =  tensor(0.7034, device='cuda:0') ; loss =  0.5234078407287598\n",
            "\n",
            "starting Epoch 13\n",
            "Train loss: 0.4236497413367033\n",
            "Train loss: 0.409432486151204\n",
            "Train loss: 0.40161614000797274\n",
            "Train loss: 0.3995012738811436\n",
            "Train loss: 0.3977629091768038\n",
            "embedding_dim =  10 ; f1 =  tensor(0.7045, device='cuda:0') ; loss =  0.5245270490646362\n",
            "\n",
            "starting Epoch 14\n",
            "Train loss: 0.40453947335481644\n",
            "Train loss: 0.3913315344940532\n",
            "Train loss: 0.38901564240455627\n",
            "Train loss: 0.3888218576338754\n",
            "Train loss: 0.38801259582950953\n",
            "embedding_dim =  10 ; f1 =  tensor(0.7106, device='cuda:0') ; loss =  0.523770409822464\n",
            "\n",
            "starting Epoch 15\n",
            "Train loss: 0.3969848807901144\n",
            "Train loss: 0.38237567742665607\n",
            "Train loss: 0.3791496032476425\n",
            "Train loss: 0.3784937164676723\n",
            "Train loss: 0.3768249952367374\n",
            "embedding_dim =  10 ; f1 =  tensor(0.6883, device='cuda:0') ; loss =  0.5471762537956237\n",
            "\n",
            "starting Epoch 16\n",
            "Train loss: 0.3927293624728918\n",
            "Train loss: 0.37744373805595166\n",
            "Train loss: 0.3720499187707901\n",
            "Train loss: 0.37017727698852765\n",
            "Train loss: 0.36937207728624344\n",
            "embedding_dim =  10 ; f1 =  tensor(0.7228, device='cuda:0') ; loss =  0.5233367562294007\n",
            "\n",
            "starting Epoch 17\n",
            "Train loss: 0.37460577115416527\n",
            "Train loss: 0.36630554632707074\n",
            "Train loss: 0.3632645761966705\n",
            "Train loss: 0.36227567783042564\n",
            "Train loss: 0.3607500979588145\n",
            "embedding_dim =  10 ; f1 =  tensor(0.6755, device='cuda:0') ; loss =  0.5721455454826355\n",
            "\n",
            "starting Epoch 18\n",
            "Train loss: 0.36717696860432625\n",
            "Train loss: 0.35950654022621387\n",
            "Train loss: 0.3577461540699005\n",
            "Train loss: 0.35618059359379667\n",
            "Train loss: 0.35571435732500895\n",
            "embedding_dim =  10 ; f1 =  tensor(0.7213, device='cuda:0') ; loss =  0.5339609742164612\n",
            "\n",
            "starting Epoch 19\n",
            "Train loss: 0.36373299174010754\n",
            "Train loss: 0.3501565167398164\n",
            "Train loss: 0.3470413076877594\n",
            "Train loss: 0.34620880769259893\n",
            "Train loss: 0.3455177392987978\n",
            "embedding_dim =  10 ; f1 =  tensor(0.7187, device='cuda:0') ; loss =  0.5428301900625229\n",
            "\n",
            "starting Epoch 0\n",
            "Train loss: 0.7459561564028263\n",
            "Train loss: 0.7101968237847993\n",
            "Train loss: 0.6942102682590484\n",
            "Train loss: 0.6834400011532342\n",
            "Train loss: 0.674840122461319\n",
            "embedding_dim =  20 ; f1 =  tensor(0.6223, device='cuda:0') ; loss =  0.6476629793643951\n",
            "\n",
            "starting Epoch 1\n",
            "Train loss: 0.6722758114337921\n",
            "Train loss: 0.6485373070745757\n",
            "Train loss: 0.6376770222187043\n",
            "Train loss: 0.6309903290734362\n",
            "Train loss: 0.6258157314289183\n",
            "embedding_dim =  20 ; f1 =  tensor(0.6685, device='cuda:0') ; loss =  0.6086063146591186\n",
            "\n",
            "starting Epoch 2\n",
            "Train loss: 0.6299551762640476\n",
            "Train loss: 0.6091545675740098\n",
            "Train loss: 0.6006077814102173\n",
            "Train loss: 0.595506719688871\n",
            "Train loss: 0.5914211585408166\n",
            "embedding_dim =  20 ; f1 =  tensor(0.6974, device='cuda:0') ; loss =  0.5816855609416962\n",
            "\n",
            "starting Epoch 3\n",
            "Train loss: 0.6001178696751595\n",
            "Train loss: 0.5756124637343667\n",
            "Train loss: 0.5688565015792847\n",
            "Train loss: 0.565901734046082\n",
            "Train loss: 0.5625359160559518\n",
            "embedding_dim =  20 ; f1 =  tensor(0.7282, device='cuda:0') ; loss =  0.5613371014595032\n",
            "\n",
            "starting Epoch 4\n",
            "Train loss: 0.576015442609787\n",
            "Train loss: 0.5539347384915208\n",
            "Train loss: 0.5452393674850464\n",
            "Train loss: 0.5403251567883278\n",
            "Train loss: 0.5380740151518867\n",
            "embedding_dim =  20 ; f1 =  tensor(0.7157, device='cuda:0') ; loss =  0.5422007262706756\n",
            "\n",
            "starting Epoch 5\n",
            "Train loss: 0.5533903874456882\n",
            "Train loss: 0.5304650366306305\n",
            "Train loss: 0.5240315014123916\n",
            "Train loss: 0.5200957558048305\n",
            "Train loss: 0.5176297717151188\n",
            "embedding_dim =  20 ; f1 =  tensor(0.7279, device='cuda:0') ; loss =  0.5271288752555847\n",
            "\n",
            "starting Epoch 6\n",
            "Train loss: 0.5273364298045635\n",
            "Train loss: 0.5067157510555151\n",
            "Train loss: 0.502184442281723\n",
            "Train loss: 0.49941731344408063\n",
            "Train loss: 0.49866549919048947\n",
            "embedding_dim =  20 ; f1 =  tensor(0.7535, device='cuda:0') ; loss =  0.520865672826767\n",
            "\n",
            "starting Epoch 7\n",
            "Train loss: 0.5087500736117363\n",
            "Train loss: 0.49041659543008514\n",
            "Train loss: 0.4847955644130707\n",
            "Train loss: 0.48136043815470453\n",
            "Train loss: 0.47794152122168315\n",
            "embedding_dim =  20 ; f1 =  tensor(0.7267, device='cuda:0') ; loss =  0.5086720734834671\n",
            "\n",
            "starting Epoch 8\n",
            "Train loss: 0.4829905666410923\n",
            "Train loss: 0.47126585786992853\n",
            "Train loss: 0.46556687653064727\n",
            "Train loss: 0.4632562354429444\n",
            "Train loss: 0.4608432146764937\n",
            "embedding_dim =  20 ; f1 =  tensor(0.7333, device='cuda:0') ; loss =  0.5023811131715774\n",
            "\n",
            "starting Epoch 9\n",
            "Train loss: 0.4659132603555918\n",
            "Train loss: 0.4498638023029674\n",
            "Train loss: 0.4443672430515289\n",
            "Train loss: 0.44169883808093285\n",
            "Train loss: 0.442154599797158\n",
            "embedding_dim =  20 ; f1 =  tensor(0.7179, device='cuda:0') ; loss =  0.5027300477027893\n",
            "\n",
            "starting Epoch 10\n",
            "Train loss: 0.4512437507510185\n",
            "Train loss: 0.436274298212745\n",
            "Train loss: 0.43052551031112674\n",
            "Train loss: 0.4276243403776368\n",
            "Train loss: 0.4266979268618992\n",
            "embedding_dim =  20 ; f1 =  tensor(0.7052, device='cuda:0') ; loss =  0.5095891833305359\n",
            "\n",
            "starting Epoch 11\n",
            "Train loss: 0.43974590860307217\n",
            "Train loss: 0.42499893361871893\n",
            "Train loss: 0.4206485229730606\n",
            "Train loss: 0.4166368072602286\n",
            "Train loss: 0.41375630987542017\n",
            "embedding_dim =  20 ; f1 =  tensor(0.7130, device='cuda:0') ; loss =  0.5046554327011108\n",
            "\n",
            "starting Epoch 12\n",
            "Train loss: 0.4249247405678034\n",
            "Train loss: 0.40900417349555274\n",
            "Train loss: 0.4037941724061966\n",
            "Train loss: 0.40161753145616447\n",
            "Train loss: 0.3996915707275981\n",
            "embedding_dim =  20 ; f1 =  tensor(0.7032, device='cuda:0') ; loss =  0.5132257521152497\n",
            "\n",
            "starting Epoch 13\n",
            "Train loss: 0.4077645316720009\n",
            "Train loss: 0.4004277397285808\n",
            "Train loss: 0.39513741493225096\n",
            "Train loss: 0.3922506255000385\n",
            "Train loss: 0.39203787382159916\n",
            "embedding_dim =  20 ; f1 =  tensor(0.7365, device='cuda:0') ; loss =  0.49369530081748964\n",
            "\n",
            "starting Epoch 14\n",
            "Train loss: 0.39709279872477055\n",
            "Train loss: 0.38583773013317224\n",
            "Train loss: 0.3835629826784134\n",
            "Train loss: 0.3806183440471763\n",
            "Train loss: 0.37925203499339877\n",
            "embedding_dim =  20 ; f1 =  tensor(0.7334, device='cuda:0') ; loss =  0.49944896399974825\n",
            "\n",
            "starting Epoch 15\n",
            "Train loss: 0.3873746879398823\n",
            "Train loss: 0.3728733180147229\n",
            "Train loss: 0.3707625305652618\n",
            "Train loss: 0.37097277925975286\n",
            "Train loss: 0.36846045688504264\n",
            "embedding_dim =  20 ; f1 =  tensor(0.7281, device='cuda:0') ; loss =  0.5075644791126251\n",
            "\n",
            "starting Epoch 16\n",
            "Train loss: 0.37517365999519825\n",
            "Train loss: 0.36554610909837665\n",
            "Train loss: 0.3632297444343567\n",
            "Train loss: 0.36111359053583286\n",
            "Train loss: 0.36040936907132465\n",
            "embedding_dim =  20 ; f1 =  tensor(0.7467, device='cuda:0') ; loss =  0.49695282280445097\n",
            "\n",
            "starting Epoch 17\n",
            "Train loss: 0.3668820671737194\n",
            "Train loss: 0.35453918305310334\n",
            "Train loss: 0.35108653247356414\n",
            "Train loss: 0.35106760041037605\n",
            "Train loss: 0.34988613923390705\n",
            "embedding_dim =  20 ; f1 =  tensor(0.7448, device='cuda:0') ; loss =  0.5015934407711029\n",
            "\n",
            "starting Epoch 18\n",
            "Train loss: 0.3593153730034828\n",
            "Train loss: 0.35315886681730096\n",
            "Train loss: 0.3492864382266998\n",
            "Train loss: 0.34723127511010243\n",
            "Train loss: 0.34452860554059345\n",
            "embedding_dim =  20 ; f1 =  tensor(0.7548, device='cuda:0') ; loss =  0.4980500161647797\n",
            "\n",
            "starting Epoch 19\n",
            "Train loss: 0.3502737581729889\n",
            "Train loss: 0.3406651977336768\n",
            "Train loss: 0.3372429025173187\n",
            "Train loss: 0.33467663224063704\n",
            "Train loss: 0.3354687389163744\n",
            "embedding_dim =  20 ; f1 =  tensor(0.7515, device='cuda:0') ; loss =  0.5022777795791626\n",
            "\n",
            "starting Epoch 0\n",
            "Train loss: 0.8362693525850773\n",
            "Train loss: 0.7622964237675522\n",
            "Train loss: 0.7330181276798249\n",
            "Train loss: 0.716503200246327\n",
            "Train loss: 0.7049362205323719\n",
            "embedding_dim =  30 ; f1 =  tensor(0.6401, device='cuda:0') ; loss =  0.663467288017273\n",
            "\n",
            "starting Epoch 1\n",
            "Train loss: 0.6914135254919529\n",
            "Train loss: 0.6660059726599491\n",
            "Train loss: 0.6545860350131989\n",
            "Train loss: 0.6480302259103575\n",
            "Train loss: 0.6419281711181005\n",
            "embedding_dim =  30 ; f1 =  tensor(0.6478, device='cuda:0') ; loss =  0.6261054337024688\n",
            "\n",
            "starting Epoch 2\n",
            "Train loss: 0.6461280100047588\n",
            "Train loss: 0.6219045555952823\n",
            "Train loss: 0.6105682635307312\n",
            "Train loss: 0.6033990543280074\n",
            "Train loss: 0.5985940198103586\n",
            "embedding_dim =  30 ; f1 =  tensor(0.6548, device='cuda:0') ; loss =  0.5880858123302459\n",
            "\n",
            "starting Epoch 3\n",
            "Train loss: 0.606963824480772\n",
            "Train loss: 0.5864495598908627\n",
            "Train loss: 0.575172153711319\n",
            "Train loss: 0.5680797384746039\n",
            "Train loss: 0.563266213451113\n",
            "embedding_dim =  30 ; f1 =  tensor(0.7210, device='cuda:0') ; loss =  0.5553175568580627\n",
            "\n",
            "starting Epoch 4\n",
            "Train loss: 0.5696888007223606\n",
            "Train loss: 0.5501978379307371\n",
            "Train loss: 0.5412733995914459\n",
            "Train loss: 0.5365371877577767\n",
            "Train loss: 0.5326103032344863\n",
            "embedding_dim =  30 ; f1 =  tensor(0.7224, device='cuda:0') ; loss =  0.5335304737091064\n",
            "\n",
            "starting Epoch 5\n",
            "Train loss: 0.5458818636834621\n",
            "Train loss: 0.5252236145915408\n",
            "Train loss: 0.5178488957881927\n",
            "Train loss: 0.5143977190131572\n",
            "Train loss: 0.5109759916861852\n",
            "embedding_dim =  30 ; f1 =  tensor(0.7415, device='cuda:0') ; loss =  0.5175128042697906\n",
            "\n",
            "starting Epoch 6\n",
            "Train loss: 0.5251206252723932\n",
            "Train loss: 0.503403795487953\n",
            "Train loss: 0.4962779897451401\n",
            "Train loss: 0.4914412823185992\n",
            "Train loss: 0.4879485631272906\n",
            "embedding_dim =  30 ; f1 =  tensor(0.7391, device='cuda:0') ; loss =  0.5044641613960266\n",
            "\n",
            "starting Epoch 7\n",
            "Train loss: 0.4917432349175215\n",
            "Train loss: 0.4771755906668576\n",
            "Train loss: 0.47265968322753904\n",
            "Train loss: 0.46972883325904163\n",
            "Train loss: 0.46641956163304193\n",
            "embedding_dim =  30 ; f1 =  tensor(0.7246, device='cuda:0') ; loss =  0.5010994374752045\n",
            "\n",
            "starting Epoch 8\n",
            "Train loss: 0.4764871597290039\n",
            "Train loss: 0.4561055555488124\n",
            "Train loss: 0.4533123034238815\n",
            "Train loss: 0.4500809974634825\n",
            "Train loss: 0.4477241032180332\n",
            "embedding_dim =  30 ; f1 =  tensor(0.7339, device='cuda:0') ; loss =  0.49263112246990204\n",
            "\n",
            "starting Epoch 9\n",
            "Train loss: 0.45215840823948383\n",
            "Train loss: 0.44663729992779816\n",
            "Train loss: 0.4396609461307526\n",
            "Train loss: 0.4375247292554201\n",
            "Train loss: 0.4348812170681499\n",
            "embedding_dim =  30 ; f1 =  tensor(0.7544, device='cuda:0') ; loss =  0.4814542531967163\n",
            "\n",
            "starting Epoch 10\n",
            "Train loss: 0.43853782676160336\n",
            "Train loss: 0.4263755805564649\n",
            "Train loss: 0.4185298693180084\n",
            "Train loss: 0.4176631280735357\n",
            "Train loss: 0.4159576612569037\n",
            "embedding_dim =  30 ; f1 =  tensor(0.7204, device='cuda:0') ; loss =  0.49828003346920013\n",
            "\n",
            "starting Epoch 11\n",
            "Train loss: 0.42765617556869984\n",
            "Train loss: 0.4117486260154031\n",
            "Train loss: 0.4074479591846466\n",
            "Train loss: 0.4038365892509916\n",
            "Train loss: 0.40315459704115275\n",
            "embedding_dim =  30 ; f1 =  tensor(0.7316, device='cuda:0') ; loss =  0.49264669716358184\n",
            "\n",
            "starting Epoch 12\n",
            "Train loss: 0.4207972902804613\n",
            "Train loss: 0.4033768195094484\n",
            "Train loss: 0.39966278433799746\n",
            "Train loss: 0.39587677503699686\n",
            "Train loss: 0.39325866245088126\n",
            "embedding_dim =  30 ; f1 =  tensor(0.7624, device='cuda:0') ; loss =  0.47799397110939024\n",
            "\n",
            "starting Epoch 13\n",
            "Train loss: 0.40484909899532795\n",
            "Train loss: 0.38981759096636914\n",
            "Train loss: 0.3846255534887314\n",
            "Train loss: 0.38410203270058135\n",
            "Train loss: 0.3822967825191362\n",
            "embedding_dim =  30 ; f1 =  tensor(0.7554, device='cuda:0') ; loss =  0.48152503073215486\n",
            "\n",
            "starting Epoch 14\n",
            "Train loss: 0.3951420243829489\n",
            "Train loss: 0.3819186299136191\n",
            "Train loss: 0.37742243230342865\n",
            "Train loss: 0.3743627369403839\n",
            "Train loss: 0.37142655962989446\n",
            "embedding_dim =  30 ; f1 =  tensor(0.7600, device='cuda:0') ; loss =  0.4811979055404663\n",
            "\n",
            "starting Epoch 15\n",
            "Train loss: 0.37757118605077267\n",
            "Train loss: 0.37080899784059235\n",
            "Train loss: 0.3668235874176025\n",
            "Train loss: 0.3656402617248137\n",
            "Train loss: 0.36407746835833504\n",
            "embedding_dim =  30 ; f1 =  tensor(0.7690, device='cuda:0') ; loss =  0.4815451860427856\n",
            "\n",
            "starting Epoch 16\n",
            "Train loss: 0.37435023859143257\n",
            "Train loss: 0.35717726295644586\n",
            "Train loss: 0.3546441948413849\n",
            "Train loss: 0.35351927244841164\n",
            "Train loss: 0.3532751106790134\n",
            "embedding_dim =  30 ; f1 =  tensor(0.7556, device='cuda:0') ; loss =  0.4894509017467499\n",
            "\n",
            "starting Epoch 17\n",
            "Train loss: 0.3629295639693737\n",
            "Train loss: 0.34964439995361096\n",
            "Train loss: 0.34668390154838563\n",
            "Train loss: 0.3436810712316143\n",
            "Train loss: 0.3431893147173382\n",
            "embedding_dim =  30 ; f1 =  tensor(0.7463, device='cuda:0') ; loss =  0.49881117045879364\n",
            "\n",
            "starting Epoch 18\n",
            "Train loss: 0.3551664873957634\n",
            "Train loss: 0.34198648189053393\n",
            "Train loss: 0.33922421336174013\n",
            "Train loss: 0.33928566859729253\n",
            "Train loss: 0.3389837621223359\n",
            "embedding_dim =  30 ; f1 =  tensor(0.7587, device='cuda:0') ; loss =  0.4959186941385269\n",
            "\n",
            "starting Epoch 19\n",
            "Train loss: 0.3400757182389498\n",
            "Train loss: 0.3333653554771886\n",
            "Train loss: 0.33302184522151945\n",
            "Train loss: 0.33000965483153044\n",
            "Train loss: 0.329362833074161\n",
            "embedding_dim =  30 ; f1 =  tensor(0.7597, device='cuda:0') ; loss =  0.49879114627838134\n",
            "\n",
            "starting Epoch 0\n",
            "Train loss: 0.7392865978181362\n",
            "Train loss: 0.7028172359322057\n",
            "Train loss: 0.6842358005046845\n",
            "Train loss: 0.6748006210398318\n",
            "Train loss: 0.6677170346180598\n",
            "embedding_dim =  40 ; f1 =  tensor(0.6570, device='cuda:0') ; loss =  0.6397887647151947\n",
            "\n",
            "starting Epoch 1\n",
            "Train loss: 0.6638638339936733\n",
            "Train loss: 0.6369788321581754\n",
            "Train loss: 0.6255162358283997\n",
            "Train loss: 0.6180850925730236\n",
            "Train loss: 0.6127241353193918\n",
            "embedding_dim =  40 ; f1 =  tensor(0.6690, device='cuda:0') ; loss =  0.5989516258239747\n",
            "\n",
            "starting Epoch 2\n",
            "Train loss: 0.6199004352092743\n",
            "Train loss: 0.5971028985399188\n",
            "Train loss: 0.5901919662952423\n",
            "Train loss: 0.5854844228545232\n",
            "Train loss: 0.5808064405407224\n",
            "embedding_dim =  40 ; f1 =  tensor(0.7123, device='cuda:0') ; loss =  0.5725200951099396\n",
            "\n",
            "starting Epoch 3\n",
            "Train loss: 0.5941025577485561\n",
            "Train loss: 0.570411042733626\n",
            "Train loss: 0.5622953963279724\n",
            "Train loss: 0.5571277862164512\n",
            "Train loss: 0.5519391944011053\n",
            "embedding_dim =  40 ; f1 =  tensor(0.7325, device='cuda:0') ; loss =  0.5500031232833862\n",
            "\n",
            "starting Epoch 4\n",
            "Train loss: 0.55965481325984\n",
            "Train loss: 0.5412768179720099\n",
            "Train loss: 0.5342186856269836\n",
            "Train loss: 0.5307661459517123\n",
            "Train loss: 0.5274032364998545\n",
            "embedding_dim =  40 ; f1 =  tensor(0.7488, device='cuda:0') ; loss =  0.5342352271080018\n",
            "\n",
            "starting Epoch 5\n",
            "Train loss: 0.5411928743124008\n",
            "Train loss: 0.5262654974605098\n",
            "Train loss: 0.5182537484169006\n",
            "Train loss: 0.5123415197899093\n",
            "Train loss: 0.5081332155636379\n",
            "embedding_dim =  40 ; f1 =  tensor(0.7548, device='cuda:0') ; loss =  0.51861172914505\n",
            "\n",
            "starting Epoch 6\n",
            "Train loss: 0.5142978075891733\n",
            "Train loss: 0.49873275287223584\n",
            "Train loss: 0.4908618438243866\n",
            "Train loss: 0.4866213086825698\n",
            "Train loss: 0.4845670458106768\n",
            "embedding_dim =  40 ; f1 =  tensor(0.7416, device='cuda:0') ; loss =  0.5035351306200028\n",
            "\n",
            "starting Epoch 7\n",
            "Train loss: 0.4928096681833267\n",
            "Train loss: 0.47783237424763764\n",
            "Train loss: 0.47293888330459594\n",
            "Train loss: 0.4696608270282176\n",
            "Train loss: 0.4670690287436758\n",
            "embedding_dim =  40 ; f1 =  tensor(0.7537, device='cuda:0') ; loss =  0.493537962436676\n",
            "\n",
            "starting Epoch 8\n",
            "Train loss: 0.46928439661860466\n",
            "Train loss: 0.4572084058414806\n",
            "Train loss: 0.45285943150520325\n",
            "Train loss: 0.45085640763168905\n",
            "Train loss: 0.44845859032301677\n",
            "embedding_dim =  40 ; f1 =  tensor(0.7562, device='cuda:0') ; loss =  0.4878628671169281\n",
            "\n",
            "starting Epoch 9\n",
            "Train loss: 0.4524590168148279\n",
            "Train loss: 0.4382989054376429\n",
            "Train loss: 0.433876296877861\n",
            "Train loss: 0.43249945765110986\n",
            "Train loss: 0.429546279211839\n",
            "embedding_dim =  40 ; f1 =  tensor(0.7513, device='cuda:0') ; loss =  0.48373710215091703\n",
            "\n",
            "starting Epoch 10\n",
            "Train loss: 0.4418954383581877\n",
            "Train loss: 0.42603550444949756\n",
            "Train loss: 0.4197317081689835\n",
            "Train loss: 0.4174942267474844\n",
            "Train loss: 0.41654472585235325\n",
            "embedding_dim =  40 ; f1 =  tensor(0.7714, device='cuda:0') ; loss =  0.49327218532562256\n",
            "\n",
            "starting Epoch 11\n",
            "Train loss: 0.42938223108649254\n",
            "Train loss: 0.4185442373608098\n",
            "Train loss: 0.4106799602508545\n",
            "Train loss: 0.40700193483438063\n",
            "Train loss: 0.4052923123041789\n",
            "embedding_dim =  40 ; f1 =  tensor(0.7527, device='cuda:0') ; loss =  0.4820766389369965\n",
            "\n",
            "starting Epoch 12\n",
            "Train loss: 0.4089057333767414\n",
            "Train loss: 0.4012143774466081\n",
            "Train loss: 0.39485423028469085\n",
            "Train loss: 0.3926201404920265\n",
            "Train loss: 0.3911065994983628\n",
            "embedding_dim =  40 ; f1 =  tensor(0.7680, device='cuda:0') ; loss =  0.48232280313968656\n",
            "\n",
            "starting Epoch 13\n",
            "Train loss: 0.39550646767020226\n",
            "Train loss: 0.3856337070465088\n",
            "Train loss: 0.38166006445884704\n",
            "Train loss: 0.38017027458148217\n",
            "Train loss: 0.3788160518521354\n",
            "embedding_dim =  40 ; f1 =  tensor(0.7625, device='cuda:0') ; loss =  0.48109333515167235\n",
            "\n",
            "starting Epoch 14\n",
            "Train loss: 0.38215321861207485\n",
            "Train loss: 0.37329453229904175\n",
            "Train loss: 0.3722150868177414\n",
            "Train loss: 0.3705076642000853\n",
            "Train loss: 0.36939842502276105\n",
            "embedding_dim =  40 ; f1 =  tensor(0.7507, device='cuda:0') ; loss =  0.48780797719955443\n",
            "\n",
            "starting Epoch 15\n",
            "Train loss: 0.3899887092411518\n",
            "Train loss: 0.3773365896759611\n",
            "Train loss: 0.3715100461244583\n",
            "Train loss: 0.3668070609000192\n",
            "Train loss: 0.36328410924900145\n",
            "embedding_dim =  40 ; f1 =  tensor(0.7606, device='cuda:0') ; loss =  0.48478240668773653\n",
            "\n",
            "starting Epoch 16\n",
            "Train loss: 0.3713216036558151\n",
            "Train loss: 0.3587308101581805\n",
            "Train loss: 0.3566495084762573\n",
            "Train loss: 0.3546205883595481\n",
            "Train loss: 0.3535458463288489\n",
            "embedding_dim =  40 ; f1 =  tensor(0.7533, device='cuda:0') ; loss =  0.4914600789546967\n",
            "\n",
            "starting Epoch 17\n",
            "Train loss: 0.3597676809877157\n",
            "Train loss: 0.3477680267709674\n",
            "Train loss: 0.34516915440559387\n",
            "Train loss: 0.34270375210847426\n",
            "Train loss: 0.3440978456111181\n",
            "embedding_dim =  40 ; f1 =  tensor(0.7473, device='cuda:0') ; loss =  0.5009305000305175\n",
            "\n",
            "starting Epoch 18\n",
            "Train loss: 0.35100026801228523\n",
            "Train loss: 0.34260518984361127\n",
            "Train loss: 0.3406569516658783\n",
            "Train loss: 0.33758547412815376\n",
            "Train loss: 0.33656316498915356\n",
            "embedding_dim =  40 ; f1 =  tensor(0.7535, device='cuda:0') ; loss =  0.5019289702177048\n",
            "\n",
            "starting Epoch 19\n",
            "Train loss: 0.34789119847118855\n",
            "Train loss: 0.3364704531250578\n",
            "Train loss: 0.33067186415195465\n",
            "Train loss: 0.32963494355998824\n",
            "Train loss: 0.32927571591876803\n",
            "embedding_dim =  40 ; f1 =  tensor(0.7571, device='cuda:0') ; loss =  0.4995883971452713\n",
            "\n",
            "starting Epoch 0\n",
            "Train loss: 0.7364406287670135\n",
            "Train loss: 0.6993357828169158\n",
            "Train loss: 0.6814554381370544\n",
            "Train loss: 0.6678122628980608\n",
            "Train loss: 0.6569505511295228\n",
            "embedding_dim =  50 ; f1 =  tensor(0.6400, device='cuda:0') ; loss =  0.6172311782836915\n",
            "\n",
            "starting Epoch 1\n",
            "Train loss: 0.6302217803895473\n",
            "Train loss: 0.6078014608585474\n",
            "Train loss: 0.6014070153236389\n",
            "Train loss: 0.5946019220708022\n",
            "Train loss: 0.5889283766349157\n",
            "embedding_dim =  50 ; f1 =  tensor(0.6956, device='cuda:0') ; loss =  0.5779974460601807\n",
            "\n",
            "starting Epoch 2\n",
            "Train loss: 0.5982553027570248\n",
            "Train loss: 0.5729265790997129\n",
            "Train loss: 0.56226287484169\n",
            "Train loss: 0.5569454129062482\n",
            "Train loss: 0.5519918203353882\n",
            "embedding_dim =  50 ; f1 =  tensor(0.7351, device='cuda:0') ; loss =  0.5502737879753112\n",
            "\n",
            "starting Epoch 3\n",
            "Train loss: 0.5680196210741997\n",
            "Train loss: 0.5483312390067361\n",
            "Train loss: 0.540187132358551\n",
            "Train loss: 0.5348294065959418\n",
            "Train loss: 0.5304570630902335\n",
            "embedding_dim =  50 ; f1 =  tensor(0.7269, device='cuda:0') ; loss =  0.5329141736030578\n",
            "\n",
            "starting Epoch 4\n",
            "Train loss: 0.5431596040725708\n",
            "Train loss: 0.5260163599794562\n",
            "Train loss: 0.5219859713315964\n",
            "Train loss: 0.5157593599895933\n",
            "Train loss: 0.5115838281455494\n",
            "embedding_dim =  50 ; f1 =  tensor(0.7391, device='cuda:0') ; loss =  0.5207068026065826\n",
            "\n",
            "starting Epoch 5\n",
            "Train loss: 0.5188080091029406\n",
            "Train loss: 0.5017611727570043\n",
            "Train loss: 0.4973198753595352\n",
            "Train loss: 0.4945920603488808\n",
            "Train loss: 0.49188336197819027\n",
            "embedding_dim =  50 ; f1 =  tensor(0.7262, device='cuda:0') ; loss =  0.5113380163908005\n",
            "\n",
            "starting Epoch 6\n",
            "Train loss: 0.49775114841759205\n",
            "Train loss: 0.48602805715618713\n",
            "Train loss: 0.4791338622570038\n",
            "Train loss: 0.4772411722745468\n",
            "Train loss: 0.474037932852904\n",
            "embedding_dim =  50 ; f1 =  tensor(0.7151, device='cuda:0') ; loss =  0.5052574694156646\n",
            "\n",
            "starting Epoch 7\n",
            "Train loss: 0.48598216846585274\n",
            "Train loss: 0.4664137824015184\n",
            "Train loss: 0.46219332456588746\n",
            "Train loss: 0.4603209335412552\n",
            "Train loss: 0.4590433701163247\n",
            "embedding_dim =  50 ; f1 =  tensor(0.7240, device='cuda:0') ; loss =  0.49698545336723327\n",
            "\n",
            "starting Epoch 8\n",
            "Train loss: 0.46990581043064594\n",
            "Train loss: 0.453435556455092\n",
            "Train loss: 0.4485522896051407\n",
            "Train loss: 0.446469184178025\n",
            "Train loss: 0.445484812770571\n",
            "embedding_dim =  50 ; f1 =  tensor(0.7361, device='cuda:0') ; loss =  0.4901056468486786\n",
            "\n",
            "starting Epoch 9\n",
            "Train loss: 0.4584350176155567\n",
            "Train loss: 0.4416290720303853\n",
            "Train loss: 0.4363820111751556\n",
            "Train loss: 0.43284458767122297\n",
            "Train loss: 0.43047511613085154\n",
            "embedding_dim =  50 ; f1 =  tensor(0.7138, device='cuda:0') ; loss =  0.49780298173427584\n",
            "\n",
            "starting Epoch 10\n",
            "Train loss: 0.43345053493976593\n",
            "Train loss: 0.4204746855027748\n",
            "Train loss: 0.4170362222194672\n",
            "Train loss: 0.41491716788775884\n",
            "Train loss: 0.4138673905815397\n",
            "embedding_dim =  50 ; f1 =  tensor(0.7436, device='cuda:0') ; loss =  0.48404177725315095\n",
            "\n",
            "starting Epoch 11\n",
            "Train loss: 0.42141471430659294\n",
            "Train loss: 0.4058082410783479\n",
            "Train loss: 0.40301749169826506\n",
            "Train loss: 0.40077898573519577\n",
            "Train loss: 0.4000810526666187\n",
            "embedding_dim =  50 ; f1 =  tensor(0.7353, device='cuda:0') ; loss =  0.4881829500198364\n",
            "\n",
            "starting Epoch 12\n",
            "Train loss: 0.41089710034430027\n",
            "Train loss: 0.3975176621567119\n",
            "Train loss: 0.3933803242444992\n",
            "Train loss: 0.3938741852988058\n",
            "Train loss: 0.39059172038521084\n",
            "embedding_dim =  50 ; f1 =  tensor(0.7022, device='cuda:0') ; loss =  0.5118453919887542\n",
            "\n",
            "starting Epoch 13\n",
            "Train loss: 0.40039041452109814\n",
            "Train loss: 0.3903763158754869\n",
            "Train loss: 0.383453261256218\n",
            "Train loss: 0.38105142561357414\n",
            "Train loss: 0.37862166655915125\n",
            "embedding_dim =  50 ; f1 =  tensor(0.7065, device='cuda:0') ; loss =  0.5111475139856339\n",
            "\n",
            "starting Epoch 14\n",
            "Train loss: 0.3881590235978365\n",
            "Train loss: 0.37394632986097626\n",
            "Train loss: 0.37193666100502015\n",
            "Train loss: 0.36979138984609006\n",
            "Train loss: 0.3693750929974374\n",
            "embedding_dim =  50 ; f1 =  tensor(0.7074, device='cuda:0') ; loss =  0.5156331539154053\n",
            "\n",
            "starting Epoch 15\n",
            "Train loss: 0.38105629943311214\n",
            "Train loss: 0.36960344061707007\n",
            "Train loss: 0.3685137647390366\n",
            "Train loss: 0.36647245688224905\n",
            "Train loss: 0.36300885890211376\n",
            "embedding_dim =  50 ; f1 =  tensor(0.7071, device='cuda:0') ; loss =  0.5208926796913147\n",
            "\n",
            "starting Epoch 16\n",
            "Train loss: 0.36073626577854156\n",
            "Train loss: 0.3540887010819984\n",
            "Train loss: 0.3511099553108215\n",
            "Train loss: 0.35204250065248405\n",
            "Train loss: 0.3525998042452903\n",
            "embedding_dim =  50 ; f1 =  tensor(0.6752, device='cuda:0') ; loss =  0.5604440569877625\n",
            "\n",
            "starting Epoch 17\n",
            "Train loss: 0.3556516543030739\n",
            "Train loss: 0.3484341890522928\n",
            "Train loss: 0.3452235347032547\n",
            "Train loss: 0.34581598728450375\n",
            "Train loss: 0.34482491122824804\n",
            "embedding_dim =  50 ; f1 =  tensor(0.7306, device='cuda:0') ; loss =  0.5107235729694366\n",
            "\n",
            "starting Epoch 18\n",
            "Train loss: 0.35042101331055164\n",
            "Train loss: 0.3373206013982946\n",
            "Train loss: 0.33609387516975403\n",
            "Train loss: 0.33602013828149485\n",
            "Train loss: 0.33523868804886225\n",
            "embedding_dim =  50 ; f1 =  tensor(0.7221, device='cuda:0') ; loss =  0.5250767409801483\n",
            "\n",
            "starting Epoch 19\n",
            "Train loss: 0.34872822277247906\n",
            "Train loss: 0.3343393802642822\n",
            "Train loss: 0.33184410333633424\n",
            "Train loss: 0.3289040504996456\n",
            "Train loss: 0.3284859763724463\n",
            "embedding_dim =  50 ; f1 =  tensor(0.7298, device='cuda:0') ; loss =  0.5211957663297653\n",
            "\n",
            "starting Epoch 0\n",
            "Train loss: 0.7379537262022495\n",
            "Train loss: 0.6978382287603436\n",
            "Train loss: 0.6775173044204712\n",
            "Train loss: 0.663697188469901\n",
            "Train loss: 0.652338540979794\n",
            "embedding_dim =  60 ; f1 =  tensor(0.6758, device='cuda:0') ; loss =  0.6102292656898498\n",
            "\n",
            "starting Epoch 1\n",
            "Train loss: 0.6366451531648636\n",
            "Train loss: 0.6084851315527251\n",
            "Train loss: 0.596108740568161\n",
            "Train loss: 0.5877169468509618\n",
            "Train loss: 0.5819070679800851\n",
            "embedding_dim =  60 ; f1 =  tensor(0.7159, device='cuda:0') ; loss =  0.5670414328575134\n",
            "\n",
            "starting Epoch 2\n",
            "Train loss: 0.5883271284401417\n",
            "Train loss: 0.563637805707527\n",
            "Train loss: 0.5560631895065308\n",
            "Train loss: 0.5519261813875455\n",
            "Train loss: 0.5485692088093076\n",
            "embedding_dim =  60 ; f1 =  tensor(0.7455, device='cuda:0') ; loss =  0.5511101901531219\n",
            "\n",
            "starting Epoch 3\n",
            "Train loss: 0.5559440925717354\n",
            "Train loss: 0.5385607769995024\n",
            "Train loss: 0.5332759892940522\n",
            "Train loss: 0.53158660077337\n",
            "Train loss: 0.5296776372761953\n",
            "embedding_dim =  60 ; f1 =  tensor(0.7441, device='cuda:0') ; loss =  0.5344661474227905\n",
            "\n",
            "starting Epoch 4\n",
            "Train loss: 0.5451331548392773\n",
            "Train loss: 0.5260142060843381\n",
            "Train loss: 0.5174619334936142\n",
            "Train loss: 0.5144802118415264\n",
            "Train loss: 0.512036257201717\n",
            "embedding_dim =  60 ; f1 =  tensor(0.7535, device='cuda:0') ; loss =  0.5282883405685425\n",
            "\n",
            "starting Epoch 5\n",
            "Train loss: 0.5234592854976654\n",
            "Train loss: 0.5056214486107682\n",
            "Train loss: 0.5005545550584793\n",
            "Train loss: 0.49697248215105994\n",
            "Train loss: 0.4944543182140305\n",
            "embedding_dim =  60 ; f1 =  tensor(0.7558, device='cuda:0') ; loss =  0.5101369440555572\n",
            "\n",
            "starting Epoch 6\n",
            "Train loss: 0.5048084706068039\n",
            "Train loss: 0.4881422321001689\n",
            "Train loss: 0.4821486192941666\n",
            "Train loss: 0.4791139239695535\n",
            "Train loss: 0.47789264044591356\n",
            "embedding_dim =  60 ; f1 =  tensor(0.7568, device='cuda:0') ; loss =  0.5018671184778214\n",
            "\n",
            "starting Epoch 7\n",
            "Train loss: 0.48424300365149975\n",
            "Train loss: 0.4710495282303203\n",
            "Train loss: 0.46333979845046996\n",
            "Train loss: 0.46102972217460175\n",
            "Train loss: 0.4575948143998782\n",
            "embedding_dim =  60 ; f1 =  tensor(0.7584, device='cuda:0') ; loss =  0.4933195382356644\n",
            "\n",
            "starting Epoch 8\n",
            "Train loss: 0.4699676427990198\n",
            "Train loss: 0.45609658324357233\n",
            "Train loss: 0.44789870262146\n",
            "Train loss: 0.4446313145445354\n",
            "Train loss: 0.44121469131537844\n",
            "embedding_dim =  60 ; f1 =  tensor(0.7567, device='cuda:0') ; loss =  0.48842128813266755\n",
            "\n",
            "starting Epoch 9\n",
            "Train loss: 0.44904567301273346\n",
            "Train loss: 0.43522915785962885\n",
            "Train loss: 0.43105952739715575\n",
            "Train loss: 0.4272135972087063\n",
            "Train loss: 0.4256189461974871\n",
            "embedding_dim =  60 ; f1 =  tensor(0.7666, device='cuda:0') ; loss =  0.48936366736888887\n",
            "\n",
            "starting Epoch 10\n",
            "Train loss: 0.4360917769372463\n",
            "Train loss: 0.4186157114578016\n",
            "Train loss: 0.4139308190345764\n",
            "Train loss: 0.41286912960792654\n",
            "Train loss: 0.41042321778479074\n",
            "embedding_dim =  60 ; f1 =  tensor(0.7660, device='cuda:0') ; loss =  0.4845747411251068\n",
            "\n",
            "starting Epoch 11\n",
            "Train loss: 0.41686707362532616\n",
            "Train loss: 0.40476753946506616\n",
            "Train loss: 0.40076202630996705\n",
            "Train loss: 0.39839568022471755\n",
            "Train loss: 0.39749455735796974\n",
            "embedding_dim =  60 ; f1 =  tensor(0.7597, device='cuda:0') ; loss =  0.48301722705364225\n",
            "\n",
            "starting Epoch 12\n",
            "Train loss: 0.39720303378999233\n",
            "Train loss: 0.38907114574403473\n",
            "Train loss: 0.3906923186779022\n",
            "Train loss: 0.3927320789045362\n",
            "Train loss: 0.39211752123775934\n",
            "embedding_dim =  60 ; f1 =  tensor(0.7600, device='cuda:0') ; loss =  0.48396158814430235\n",
            "\n",
            "starting Epoch 13\n",
            "Train loss: 0.39371392875909805\n",
            "Train loss: 0.38139310479164124\n",
            "Train loss: 0.37867246329784393\n",
            "Train loss: 0.37705077297651945\n",
            "Train loss: 0.37429098323697135\n",
            "embedding_dim =  60 ; f1 =  tensor(0.7665, device='cuda:0') ; loss =  0.484252056479454\n",
            "\n",
            "starting Epoch 14\n",
            "Train loss: 0.3866229075938463\n",
            "Train loss: 0.3776440105654977\n",
            "Train loss: 0.3743050354719162\n",
            "Train loss: 0.3690468495461478\n",
            "Train loss: 0.36633499356962385\n",
            "embedding_dim =  60 ; f1 =  tensor(0.7625, device='cuda:0') ; loss =  0.4863391101360321\n",
            "\n",
            "starting Epoch 15\n",
            "Train loss: 0.37263449653983116\n",
            "Train loss: 0.36188241478168603\n",
            "Train loss: 0.35770759999752044\n",
            "Train loss: 0.35498635315183386\n",
            "Train loss: 0.3544069510840234\n",
            "embedding_dim =  60 ; f1 =  tensor(0.7602, device='cuda:0') ; loss =  0.49028439819812775\n",
            "\n",
            "starting Epoch 16\n",
            "Train loss: 0.3722011838108301\n",
            "Train loss: 0.35741825356627954\n",
            "Train loss: 0.35459744453430175\n",
            "Train loss: 0.3501758904599432\n",
            "Train loss: 0.34824424059618087\n",
            "embedding_dim =  60 ; f1 =  tensor(0.7532, device='cuda:0') ; loss =  0.49740505814552305\n",
            "\n",
            "starting Epoch 17\n",
            "Train loss: 0.35503924638032913\n",
            "Train loss: 0.3446712132656213\n",
            "Train loss: 0.33882598400115965\n",
            "Train loss: 0.3380444859390828\n",
            "Train loss: 0.336461892085416\n",
            "embedding_dim =  60 ; f1 =  tensor(0.7597, device='cuda:0') ; loss =  0.4984375923871994\n",
            "\n",
            "starting Epoch 18\n",
            "Train loss: 0.3370912540704012\n",
            "Train loss: 0.33372261397766345\n",
            "Train loss: 0.3310601568222046\n",
            "Train loss: 0.3298278744540997\n",
            "Train loss: 0.3304210401007107\n",
            "embedding_dim =  60 ; f1 =  tensor(0.7542, device='cuda:0') ; loss =  0.506191286444664\n",
            "\n",
            "starting Epoch 19\n",
            "Train loss: 0.34443720057606697\n",
            "Train loss: 0.334001775040771\n",
            "Train loss: 0.32978006660938264\n",
            "Train loss: 0.32824961640941563\n",
            "Train loss: 0.3255706677834193\n",
            "embedding_dim =  60 ; f1 =  tensor(0.7593, device='cuda:0') ; loss =  0.5059072494506835\n",
            "\n",
            "starting Epoch 0\n",
            "Train loss: 0.7344733402132988\n",
            "Train loss: 0.6967420812809106\n",
            "Train loss: 0.6783831512928009\n",
            "Train loss: 0.666387809746301\n",
            "Train loss: 0.655822020911035\n",
            "embedding_dim =  70 ; f1 =  tensor(0.6314, device='cuda:0') ; loss =  0.6193637788295746\n",
            "\n",
            "starting Epoch 1\n",
            "Train loss: 0.6386116407811642\n",
            "Train loss: 0.6130932027643378\n",
            "Train loss: 0.602148541212082\n",
            "Train loss: 0.5952979112738994\n",
            "Train loss: 0.5903866993529456\n",
            "embedding_dim =  70 ; f1 =  tensor(0.6692, device='cuda:0') ; loss =  0.5769567728042603\n",
            "\n",
            "starting Epoch 2\n",
            "Train loss: 0.588343296200037\n",
            "Train loss: 0.5674037535985311\n",
            "Train loss: 0.5605039942264557\n",
            "Train loss: 0.5557800042095469\n",
            "Train loss: 0.5522160253354481\n",
            "embedding_dim =  70 ; f1 =  tensor(0.7448, device='cuda:0') ; loss =  0.5616459608078003\n",
            "\n",
            "starting Epoch 3\n",
            "Train loss: 0.5651346743106842\n",
            "Train loss: 0.5427676710215482\n",
            "Train loss: 0.5365079027414322\n",
            "Train loss: 0.5302797569267785\n",
            "Train loss: 0.529203648013728\n",
            "embedding_dim =  70 ; f1 =  tensor(0.7519, device='cuda:0') ; loss =  0.5430996060371399\n",
            "\n",
            "starting Epoch 4\n",
            "Train loss: 0.5482360869646072\n",
            "Train loss: 0.5284117577653943\n",
            "Train loss: 0.5242501699924469\n",
            "Train loss: 0.5204898439236542\n",
            "Train loss: 0.5182454820190158\n",
            "embedding_dim =  70 ; f1 =  tensor(0.7466, device='cuda:0') ; loss =  0.5227230608463287\n",
            "\n",
            "starting Epoch 5\n",
            "Train loss: 0.5295599550008774\n",
            "Train loss: 0.5090362384463801\n",
            "Train loss: 0.50149305164814\n",
            "Train loss: 0.49741150416545016\n",
            "Train loss: 0.49509107179584955\n",
            "embedding_dim =  70 ; f1 =  tensor(0.7516, device='cuda:0') ; loss =  0.5091899454593658\n",
            "\n",
            "starting Epoch 6\n",
            "Train loss: 0.5088608413934708\n",
            "Train loss: 0.49220855579231726\n",
            "Train loss: 0.48410873174667357\n",
            "Train loss: 0.47988944044753684\n",
            "Train loss: 0.47748417655626935\n",
            "embedding_dim =  70 ; f1 =  tensor(0.7606, device='cuda:0') ; loss =  0.5059759050607682\n",
            "\n",
            "starting Epoch 7\n",
            "Train loss: 0.4875349700450897\n",
            "Train loss: 0.47338965715784015\n",
            "Train loss: 0.46617442309856416\n",
            "Train loss: 0.46391294474032385\n",
            "Train loss: 0.4621131256932304\n",
            "embedding_dim =  70 ; f1 =  tensor(0.7595, device='cuda:0') ; loss =  0.5109314560890198\n",
            "\n",
            "starting Epoch 8\n",
            "Train loss: 0.47736234590411186\n",
            "Train loss: 0.45979655421141424\n",
            "Train loss: 0.45342188119888305\n",
            "Train loss: 0.4493015977873731\n",
            "Train loss: 0.4481960776306334\n",
            "embedding_dim =  70 ; f1 =  tensor(0.7649, device='cuda:0') ; loss =  0.4991637885570526\n",
            "\n",
            "starting Epoch 9\n",
            "Train loss: 0.45408271066844463\n",
            "Train loss: 0.44073169881647284\n",
            "Train loss: 0.43532139003276826\n",
            "Train loss: 0.4325616466465281\n",
            "Train loss: 0.4311265136514391\n",
            "embedding_dim =  70 ; f1 =  tensor(0.7704, device='cuda:0') ; loss =  0.48189170062541964\n",
            "\n",
            "starting Epoch 10\n",
            "Train loss: 0.4336639381945133\n",
            "Train loss: 0.4219028696869359\n",
            "Train loss: 0.41680708050727844\n",
            "Train loss: 0.4153841796206005\n",
            "Train loss: 0.4127538998921712\n",
            "embedding_dim =  70 ; f1 =  tensor(0.7669, device='cuda:0') ; loss =  0.4779820591211319\n",
            "\n",
            "starting Epoch 11\n",
            "Train loss: 0.4262523874640465\n",
            "Train loss: 0.4087101663603927\n",
            "Train loss: 0.40274700701236726\n",
            "Train loss: 0.40109246124082537\n",
            "Train loss: 0.4008000046014786\n",
            "embedding_dim =  70 ; f1 =  tensor(0.7687, device='cuda:0') ; loss =  0.4874733775854111\n",
            "\n",
            "starting Epoch 12\n",
            "Train loss: 0.40619273856282234\n",
            "Train loss: 0.393653951811068\n",
            "Train loss: 0.39302208602428435\n",
            "Train loss: 0.38982601219148777\n",
            "Train loss: 0.388267097728593\n",
            "embedding_dim =  70 ; f1 =  tensor(0.7670, device='cuda:0') ; loss =  0.47294890880584717\n",
            "\n",
            "starting Epoch 13\n",
            "Train loss: 0.40140463784337044\n",
            "Train loss: 0.3851439121997718\n",
            "Train loss: 0.3791362887620926\n",
            "Train loss: 0.37846526370119693\n",
            "Train loss: 0.37749135458753225\n",
            "embedding_dim =  70 ; f1 =  tensor(0.7720, device='cuda:0') ; loss =  0.47709360122680666\n",
            "\n",
            "starting Epoch 14\n",
            "Train loss: 0.3842289410531521\n",
            "Train loss: 0.374163110147823\n",
            "Train loss: 0.37089473247528076\n",
            "Train loss: 0.3683049807797617\n",
            "Train loss: 0.36710334037031445\n",
            "embedding_dim =  70 ; f1 =  tensor(0.7711, device='cuda:0') ; loss =  0.4765053361654282\n",
            "\n",
            "starting Epoch 15\n",
            "Train loss: 0.3762021102011204\n",
            "Train loss: 0.36667516285722906\n",
            "Train loss: 0.3624970489740372\n",
            "Train loss: 0.35967128312409813\n",
            "Train loss: 0.3570082166365215\n",
            "embedding_dim =  70 ; f1 =  tensor(0.7620, device='cuda:0') ; loss =  0.4801941990852356\n",
            "\n",
            "starting Epoch 16\n",
            "Train loss: 0.36034844256937504\n",
            "Train loss: 0.3586811658107873\n",
            "Train loss: 0.35459638118743897\n",
            "Train loss: 0.3505804943504618\n",
            "Train loss: 0.34888959924379986\n",
            "embedding_dim =  70 ; f1 =  tensor(0.7703, device='cuda:0') ; loss =  0.48284161686897276\n",
            "\n",
            "starting Epoch 17\n",
            "Train loss: 0.3602532856166363\n",
            "Train loss: 0.3486643135547638\n",
            "Train loss: 0.34224335551261903\n",
            "Train loss: 0.3409628045203081\n",
            "Train loss: 0.34039792241085143\n",
            "embedding_dim =  70 ; f1 =  tensor(0.7393, device='cuda:0') ; loss =  0.5022818833589554\n",
            "\n",
            "starting Epoch 18\n",
            "Train loss: 0.3493118956685066\n",
            "Train loss: 0.3383646146817641\n",
            "Train loss: 0.3382053256034851\n",
            "Train loss: 0.33436217770647647\n",
            "Train loss: 0.33349728903600145\n",
            "embedding_dim =  70 ; f1 =  tensor(0.7487, device='cuda:0') ; loss =  0.5004582524299621\n",
            "\n",
            "starting Epoch 19\n",
            "Train loss: 0.34075579047203064\n",
            "Train loss: 0.3286147912343343\n",
            "Train loss: 0.3262580645084381\n",
            "Train loss: 0.32613713794679783\n",
            "Train loss: 0.32418306207373027\n",
            "embedding_dim =  70 ; f1 =  tensor(0.7628, device='cuda:0') ; loss =  0.49701435267925265\n",
            "\n",
            "starting Epoch 0\n",
            "Train loss: 0.7340685874223709\n",
            "Train loss: 0.6943466843980731\n",
            "Train loss: 0.675685909986496\n",
            "Train loss: 0.6628448206986954\n",
            "Train loss: 0.6517500416153953\n",
            "embedding_dim =  80 ; f1 =  tensor(0.5553, device='cuda:0') ; loss =  0.6236125290393829\n",
            "\n",
            "starting Epoch 1\n",
            "Train loss: 0.6370838358998299\n",
            "Train loss: 0.614169388106375\n",
            "Train loss: 0.6007340002059937\n",
            "Train loss: 0.5911987201491399\n",
            "Train loss: 0.5842006731600988\n",
            "embedding_dim =  80 ; f1 =  tensor(0.6931, device='cuda:0') ; loss =  0.5668449699878693\n",
            "\n",
            "starting Epoch 2\n",
            "Train loss: 0.5792868174612522\n",
            "Train loss: 0.5609268726724567\n",
            "Train loss: 0.5547165107727051\n",
            "Train loss: 0.5493559410322958\n",
            "Train loss: 0.545414245554379\n",
            "embedding_dim =  80 ; f1 =  tensor(0.7422, device='cuda:0') ; loss =  0.5452545166015625\n",
            "\n",
            "starting Epoch 3\n",
            "Train loss: 0.5513057745993137\n",
            "Train loss: 0.5339638753370806\n",
            "Train loss: 0.5291123002767563\n",
            "Train loss: 0.5250973483519767\n",
            "Train loss: 0.5224963547218413\n",
            "embedding_dim =  80 ; f1 =  tensor(0.7465, device='cuda:0') ; loss =  0.5365845918655395\n",
            "\n",
            "starting Epoch 4\n",
            "Train loss: 0.5303488243371248\n",
            "Train loss: 0.515079165949966\n",
            "Train loss: 0.5096935367584229\n",
            "Train loss: 0.5074493555880305\n",
            "Train loss: 0.5053499812881151\n",
            "embedding_dim =  80 ; f1 =  tensor(0.7509, device='cuda:0') ; loss =  0.5250473618507385\n",
            "\n",
            "starting Epoch 5\n",
            "Train loss: 0.5180392451584339\n",
            "Train loss: 0.5008729127320376\n",
            "Train loss: 0.5008062124252319\n",
            "Train loss: 0.4956297549738813\n",
            "Train loss: 0.4920631870627403\n",
            "embedding_dim =  80 ; f1 =  tensor(0.7534, device='cuda:0') ; loss =  0.5275416254997254\n",
            "\n",
            "starting Epoch 6\n",
            "Train loss: 0.5035956781357527\n",
            "Train loss: 0.4862283964951833\n",
            "Train loss: 0.48185148715972903\n",
            "Train loss: 0.4773324398852106\n",
            "Train loss: 0.47470255231573466\n",
            "embedding_dim =  80 ; f1 =  tensor(0.7544, device='cuda:0') ; loss =  0.5017951339483261\n",
            "\n",
            "starting Epoch 7\n",
            "Train loss: 0.48536071740090847\n",
            "Train loss: 0.46875568501877063\n",
            "Train loss: 0.4640702909231186\n",
            "Train loss: 0.46015619742336555\n",
            "Train loss: 0.4576293563558942\n",
            "embedding_dim =  80 ; f1 =  tensor(0.7604, device='cuda:0') ; loss =  0.49891493618488314\n",
            "\n",
            "starting Epoch 8\n",
            "Train loss: 0.47058931551873684\n",
            "Train loss: 0.45490428444110986\n",
            "Train loss: 0.4496660315990448\n",
            "Train loss: 0.44422699622253875\n",
            "Train loss: 0.4434843662948835\n",
            "embedding_dim =  80 ; f1 =  tensor(0.7626, device='cuda:0') ; loss =  0.4947821617126465\n",
            "\n",
            "starting Epoch 9\n",
            "Train loss: 0.44773539900779724\n",
            "Train loss: 0.4365050205678651\n",
            "Train loss: 0.43142434060573576\n",
            "Train loss: 0.4289542254227311\n",
            "Train loss: 0.4265619172226815\n",
            "embedding_dim =  80 ; f1 =  tensor(0.7648, device='cuda:0') ; loss =  0.4894996166229248\n",
            "\n",
            "starting Epoch 10\n",
            "Train loss: 0.4285620879381895\n",
            "Train loss: 0.4149776850685929\n",
            "Train loss: 0.41081995129585264\n",
            "Train loss: 0.4111027490736833\n",
            "Train loss: 0.4118923775496937\n",
            "embedding_dim =  80 ; f1 =  tensor(0.7716, device='cuda:0') ; loss =  0.48979560732841493\n",
            "\n",
            "starting Epoch 11\n",
            "Train loss: 0.4308059886097908\n",
            "Train loss: 0.4141274221015699\n",
            "Train loss: 0.40758283257484434\n",
            "Train loss: 0.40431573408753124\n",
            "Train loss: 0.40151588760671164\n",
            "embedding_dim =  80 ; f1 =  tensor(0.7692, device='cuda:0') ; loss =  0.47866265177726747\n",
            "\n",
            "starting Epoch 12\n",
            "Train loss: 0.4088827967643738\n",
            "Train loss: 0.3954688608646393\n",
            "Train loss: 0.39186915636062625\n",
            "Train loss: 0.3892335362398802\n",
            "Train loss: 0.3878901469565573\n",
            "embedding_dim =  80 ; f1 =  tensor(0.7670, device='cuda:0') ; loss =  0.4780478239059448\n",
            "\n",
            "starting Epoch 13\n",
            "Train loss: 0.3952835611999035\n",
            "Train loss: 0.3829895212794795\n",
            "Train loss: 0.37962989091873167\n",
            "Train loss: 0.37720324966444896\n",
            "Train loss: 0.3765378881778036\n",
            "embedding_dim =  80 ; f1 =  tensor(0.7745, device='cuda:0') ; loss =  0.49017558693885804\n",
            "\n",
            "starting Epoch 14\n",
            "Train loss: 0.383637398481369\n",
            "Train loss: 0.3686969569235137\n",
            "Train loss: 0.36536345303058626\n",
            "Train loss: 0.3656322239939846\n",
            "Train loss: 0.36544270707028254\n",
            "embedding_dim =  80 ; f1 =  tensor(0.7696, device='cuda:0') ; loss =  0.4816806524991989\n",
            "\n",
            "starting Epoch 15\n",
            "Train loss: 0.38056232035160065\n",
            "Train loss: 0.3687303833889239\n",
            "Train loss: 0.36245889484882354\n",
            "Train loss: 0.3608545883377986\n",
            "Train loss: 0.35919766021626337\n",
            "embedding_dim =  80 ; f1 =  tensor(0.7731, device='cuda:0') ; loss =  0.48525148034095766\n",
            "\n",
            "starting Epoch 16\n",
            "Train loss: 0.36928845942020416\n",
            "Train loss: 0.35248409618030896\n",
            "Train loss: 0.3489207601547241\n",
            "Train loss: 0.3492964070234726\n",
            "Train loss: 0.3485385591075534\n",
            "embedding_dim =  80 ; f1 =  tensor(0.7690, device='cuda:0') ; loss =  0.4874489724636078\n",
            "\n",
            "starting Epoch 17\n",
            "Train loss: 0.3536974675953388\n",
            "Train loss: 0.3439281844731533\n",
            "Train loss: 0.34070202767848967\n",
            "Train loss: 0.34013948734126875\n",
            "Train loss: 0.3403063916734287\n",
            "embedding_dim =  80 ; f1 =  tensor(0.7713, device='cuda:0') ; loss =  0.4927581936120987\n",
            "\n",
            "starting Epoch 18\n",
            "Train loss: 0.3594025243073702\n",
            "Train loss: 0.3456916935516126\n",
            "Train loss: 0.33880510568618777\n",
            "Train loss: 0.3372926022579421\n",
            "Train loss: 0.3358579689548129\n",
            "embedding_dim =  80 ; f1 =  tensor(0.7607, device='cuda:0') ; loss =  0.49613846838474274\n",
            "\n",
            "starting Epoch 19\n",
            "Train loss: 0.34462411515414715\n",
            "Train loss: 0.33456329414338776\n",
            "Train loss: 0.32839371025562286\n",
            "Train loss: 0.3269454365345969\n",
            "Train loss: 0.3269165594662939\n",
            "embedding_dim =  80 ; f1 =  tensor(0.7642, device='cuda:0') ; loss =  0.49862594306468966\n",
            "\n",
            "starting Epoch 0\n",
            "Train loss: 0.7540699206292629\n",
            "Train loss: 0.7104159412962018\n",
            "Train loss: 0.6890474319458008\n",
            "Train loss: 0.6750157555537437\n",
            "Train loss: 0.6635677339065642\n",
            "embedding_dim =  90 ; f1 =  tensor(0.6210, device='cuda:0') ; loss =  0.6190013110637664\n",
            "\n",
            "starting Epoch 1\n",
            "Train loss: 0.6350498907268047\n",
            "Train loss: 0.610873717250246\n",
            "Train loss: 0.5987419676780701\n",
            "Train loss: 0.5899190279974866\n",
            "Train loss: 0.5842409630616506\n",
            "embedding_dim =  90 ; f1 =  tensor(0.7342, device='cuda:0') ; loss =  0.5679038524627685\n",
            "\n",
            "starting Epoch 2\n",
            "Train loss: 0.5861613899469376\n",
            "Train loss: 0.568359866286769\n",
            "Train loss: 0.5584937429428101\n",
            "Train loss: 0.5514041888180063\n",
            "Train loss: 0.5469930959599358\n",
            "embedding_dim =  90 ; f1 =  tensor(0.7302, device='cuda:0') ; loss =  0.5430741250514984\n",
            "\n",
            "starting Epoch 3\n",
            "Train loss: 0.5605339221656322\n",
            "Train loss: 0.5450600764968179\n",
            "Train loss: 0.5377516758441925\n",
            "Train loss: 0.531076088770112\n",
            "Train loss: 0.5284493508793059\n",
            "embedding_dim =  90 ; f1 =  tensor(0.7225, device='cuda:0') ; loss =  0.5278529524803162\n",
            "\n",
            "starting Epoch 4\n",
            "Train loss: 0.5422894917428493\n",
            "Train loss: 0.5238079690571987\n",
            "Train loss: 0.5180328214168548\n",
            "Train loss: 0.5136347052766316\n",
            "Train loss: 0.5118150746538526\n",
            "embedding_dim =  90 ; f1 =  tensor(0.7459, device='cuda:0') ; loss =  0.5149342060089112\n",
            "\n",
            "starting Epoch 5\n",
            "Train loss: 0.5197118669748306\n",
            "Train loss: 0.498781577204213\n",
            "Train loss: 0.4945245748758316\n",
            "Train loss: 0.4933185110341257\n",
            "Train loss: 0.4922424732219605\n",
            "embedding_dim =  90 ; f1 =  tensor(0.7517, device='cuda:0') ; loss =  0.5065529674291611\n",
            "\n",
            "starting Epoch 6\n",
            "Train loss: 0.5071072168648243\n",
            "Train loss: 0.4879208234223453\n",
            "Train loss: 0.4825394231081009\n",
            "Train loss: 0.47778868452826545\n",
            "Train loss: 0.4745828438372839\n",
            "embedding_dim =  90 ; f1 =  tensor(0.7579, device='cuda:0') ; loss =  0.49561463594436644\n",
            "\n",
            "starting Epoch 7\n",
            "Train loss: 0.484724847599864\n",
            "Train loss: 0.4716353913148244\n",
            "Train loss: 0.4646631270647049\n",
            "Train loss: 0.4619178758628333\n",
            "Train loss: 0.4581011622434571\n",
            "embedding_dim =  90 ; f1 =  tensor(0.7464, device='cuda:0') ; loss =  0.4906973630189896\n",
            "\n",
            "starting Epoch 8\n",
            "Train loss: 0.47112060710787773\n",
            "Train loss: 0.45332166010683234\n",
            "Train loss: 0.4513964432477951\n",
            "Train loss: 0.4481188614866627\n",
            "Train loss: 0.44753408361048924\n",
            "embedding_dim =  90 ; f1 =  tensor(0.7349, device='cuda:0') ; loss =  0.4927653789520264\n",
            "\n",
            "starting Epoch 9\n",
            "Train loss: 0.44993671402335167\n",
            "Train loss: 0.43287652460011566\n",
            "Train loss: 0.42956188797950745\n",
            "Train loss: 0.4263568209178412\n",
            "Train loss: 0.42647104745819453\n",
            "embedding_dim =  90 ; f1 =  tensor(0.7126, device='cuda:0') ; loss =  0.5011666685342788\n",
            "\n",
            "starting Epoch 10\n",
            "Train loss: 0.43732429668307304\n",
            "Train loss: 0.42366813258691266\n",
            "Train loss: 0.4194790363311768\n",
            "Train loss: 0.4145907417162141\n",
            "Train loss: 0.4144230555920374\n",
            "embedding_dim =  90 ; f1 =  tensor(0.7087, device='cuda:0') ; loss =  0.5044202625751495\n",
            "\n",
            "starting Epoch 11\n",
            "Train loss: 0.42556736432015896\n",
            "Train loss: 0.41405629750454065\n",
            "Train loss: 0.40692262768745424\n",
            "Train loss: 0.4035522025912555\n",
            "Train loss: 0.4001813626715115\n",
            "embedding_dim =  90 ; f1 =  tensor(0.7304, device='cuda:0') ; loss =  0.4933921307325363\n",
            "\n",
            "starting Epoch 12\n",
            "Train loss: 0.40695792995393276\n",
            "Train loss: 0.39698508381843567\n",
            "Train loss: 0.3959587335586548\n",
            "Train loss: 0.39232526296999914\n",
            "Train loss: 0.3912587733495803\n",
            "embedding_dim =  90 ; f1 =  tensor(0.7236, device='cuda:0') ; loss =  0.4998098164796829\n",
            "\n",
            "starting Epoch 13\n",
            "Train loss: 0.4065342303365469\n",
            "Train loss: 0.3894621720819762\n",
            "Train loss: 0.38484513223171235\n",
            "Train loss: 0.38346037757930473\n",
            "Train loss: 0.38196268464837757\n",
            "embedding_dim =  90 ; f1 =  tensor(0.7567, device='cuda:0') ; loss =  0.48179141879081727\n",
            "\n",
            "starting Epoch 14\n",
            "Train loss: 0.3862894382327795\n",
            "Train loss: 0.37615614406990283\n",
            "Train loss: 0.3701977771520615\n",
            "Train loss: 0.36788560294393285\n",
            "Train loss: 0.36803743704443886\n",
            "embedding_dim =  90 ; f1 =  tensor(0.7477, device='cuda:0') ; loss =  0.4888246476650238\n",
            "\n",
            "starting Epoch 15\n",
            "Train loss: 0.375674344599247\n",
            "Train loss: 0.3635490184480494\n",
            "Train loss: 0.3608749830722809\n",
            "Train loss: 0.36059686941887015\n",
            "Train loss: 0.3602159001997539\n",
            "embedding_dim =  90 ; f1 =  tensor(0.7646, device='cuda:0') ; loss =  0.48583278357982634\n",
            "\n",
            "starting Epoch 16\n",
            "Train loss: 0.3598558343946934\n",
            "Train loss: 0.35333165887630347\n",
            "Train loss: 0.3503272986412048\n",
            "Train loss: 0.35027430039733204\n",
            "Train loss: 0.3485523682265055\n",
            "embedding_dim =  90 ; f1 =  tensor(0.7223, device='cuda:0') ; loss =  0.51094089448452\n",
            "\n",
            "starting Epoch 17\n",
            "Train loss: 0.3575568199157715\n",
            "Train loss: 0.34782982685349206\n",
            "Train loss: 0.3423566102981567\n",
            "Train loss: 0.34313930548838717\n",
            "Train loss: 0.3430954459167662\n",
            "embedding_dim =  90 ; f1 =  tensor(0.7387, device='cuda:0') ; loss =  0.5048833966255188\n",
            "\n",
            "starting Epoch 18\n",
            "Train loss: 0.34574276208877563\n",
            "Train loss: 0.3410378902247458\n",
            "Train loss: 0.34066890776157377\n",
            "Train loss: 0.3390512297402567\n",
            "Train loss: 0.33697874950511114\n",
            "embedding_dim =  90 ; f1 =  tensor(0.7360, device='cuda:0') ; loss =  0.5115873843431473\n",
            "\n",
            "starting Epoch 19\n",
            "Train loss: 0.3434830866754055\n",
            "Train loss: 0.33622097878745105\n",
            "Train loss: 0.33382956564426425\n",
            "Train loss: 0.330260596168575\n",
            "Train loss: 0.32939584659678595\n",
            "embedding_dim =  90 ; f1 =  tensor(0.7396, device='cuda:0') ; loss =  0.5178281486034393\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePZ_LnIqv3Tn"
      },
      "source": [
        "Так, мы видим, что та же модель, но с размерностью эмбеддинга = 80 работает лучше: f1 = 0.7642, loss =  0.49862594306468966"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOvjkHUyv2Pl"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    epoch_loss = 0\n",
        "    epoch_metric = 0\n",
        "    model.eval() \n",
        "    with torch.no_grad():\n",
        "        for i, (texts, symbols, ys) in enumerate(iterator):   \n",
        "            preds = model(texts, symbols)\n",
        "            loss = criterion(preds, ys)\n",
        "            epoch_loss += loss.item()\n",
        "            batch_metric = f1(preds.round().long(), ys.long(), ignore_index=0)\n",
        "            epoch_metric += batch_metric\n",
        "\n",
        "            if not (i + 1) % int(len(iterator)/5):\n",
        "              print(f'Val loss: {epoch_loss/i}, Val f1: {epoch_metric/i}')\n",
        "        \n",
        "    return epoch_metric / len(iterator), epoch_loss / len(iterator)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NOKhClZpxSJR",
        "outputId": "d920835b-9604-49f6-f164-27f1683076b4"
      },
      "source": [
        "model = CNN(len(word2id), len(symbol2id), 80)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
        "criterion = nn.BCELoss()  \n",
        "\n",
        "# веса модели и значения лосса храним там же, где и все остальные тензоры\n",
        "model = model.to(DEVICE)\n",
        "criterion = criterion.to(DEVICE)\n",
        "losses = []\n",
        "losses_eval = []\n",
        "f1s = []\n",
        "f1s_eval = []\n",
        "\n",
        "for y in range(20):\n",
        "    print(f'\\nstarting Epoch {y}')\n",
        "    print('Training...')\n",
        "    epoch_loss = train(model, train_iterator, optimizer, criterion)\n",
        "    losses.append(epoch_loss)\n",
        "    print('\\nEvaluating on train...')\n",
        "    f1_on_train,_ = evaluate(model, train_iterator, criterion)\n",
        "    f1s.append(f1_on_train)\n",
        "    print('\\nEvaluating on test...')\n",
        "    f1_on_test, epoch_loss_on_test = evaluate(model, val_iterator, criterion)\n",
        "    losses_eval.append(epoch_loss_on_test)\n",
        "    f1s_eval.append(f1_on_test)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "starting Epoch 0\n",
            "Training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py:298: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at  ../aten/src/ATen/native/Convolution.cpp:647.)\n",
            "  self.padding, self.dilation, self.groups)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 0.7371728122234344\n",
            "Train loss: 0.696884117343209\n",
            "Train loss: 0.6795744347572327\n",
            "Train loss: 0.6664916623884173\n",
            "Train loss: 0.6547542796248481\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.6501341722905636, Val f1: 0.7287598252296448\n",
            "Val loss: 0.6294470989342892, Val f1: 0.7071633338928223\n",
            "Val loss: 0.6231754887104034, Val f1: 0.6996847987174988\n",
            "Val loss: 0.6208362232393293, Val f1: 0.6954509615898132\n",
            "Val loss: 0.6186558511995134, Val f1: 0.6941967010498047\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.2111101746559143, Val f1: 1.4206953048706055\n",
            "Val loss: 0.8119651277860006, Val f1: 0.9353048205375671\n",
            "Val loss: 0.7320349812507629, Val f1: 0.8314833045005798\n",
            "Val loss: 0.6972817012241909, Val f1: 0.7891724109649658\n",
            "Val loss: 0.6795224878523085, Val f1: 0.7655028104782104\n",
            "\n",
            "starting Epoch 1\n",
            "Training...\n",
            "Train loss: 0.6327656507492065\n",
            "Train loss: 0.608149113077106\n",
            "Train loss: 0.5961474585533142\n",
            "Train loss: 0.5872222523191082\n",
            "Train loss: 0.5805157437210992\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.6033837907016277, Val f1: 0.7956662178039551\n",
            "Val loss: 0.5878340674169136, Val f1: 0.7656587958335876\n",
            "Val loss: 0.5807769656181335, Val f1: 0.7587534785270691\n",
            "Val loss: 0.5785051077159483, Val f1: 0.7542368173599243\n",
            "Val loss: 0.5769466019812084, Val f1: 0.7509286999702454\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.139633059501648, Val f1: 1.4876011610031128\n",
            "Val loss: 0.7618124882380167, Val f1: 0.9886274337768555\n",
            "Val loss: 0.6861196875572204, Val f1: 0.8881945610046387\n",
            "Val loss: 0.6549362625394549, Val f1: 0.8412546515464783\n",
            "Val loss: 0.6386294431156583, Val f1: 0.818170964717865\n",
            "\n",
            "starting Epoch 2\n",
            "Training...\n",
            "Train loss: 0.5768019072711468\n",
            "Train loss: 0.5584794048107031\n",
            "Train loss: 0.5493999373912811\n",
            "Train loss: 0.5463276105140572\n",
            "Train loss: 0.5429419300385884\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5833943486213684, Val f1: 0.7970624566078186\n",
            "Val loss: 0.5643080545194221, Val f1: 0.7755701541900635\n",
            "Val loss: 0.5582076215744018, Val f1: 0.7687293291091919\n",
            "Val loss: 0.5544975328801284, Val f1: 0.7657294273376465\n",
            "Val loss: 0.5524465980983916, Val f1: 0.7636653780937195\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.1028298139572144, Val f1: 1.5084118843078613\n",
            "Val loss: 0.7374213536580404, Val f1: 0.9963240027427673\n",
            "Val loss: 0.6631772756576538, Val f1: 0.8979045152664185\n",
            "Val loss: 0.6328696693692889, Val f1: 0.8534132242202759\n",
            "Val loss: 0.6165572868453132, Val f1: 0.8288633227348328\n",
            "\n",
            "starting Epoch 3\n",
            "Training...\n",
            "Train loss: 0.5558805111795664\n",
            "Train loss: 0.5366937870329077\n",
            "Train loss: 0.5296927660703659\n",
            "Train loss: 0.5256385678675637\n",
            "Train loss: 0.5212999660344351\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.54698302783072, Val f1: 0.8061618804931641\n",
            "Val loss: 0.5298207862810655, Val f1: 0.7824166417121887\n",
            "Val loss: 0.5247982132434845, Val f1: 0.7737467885017395\n",
            "Val loss: 0.5224421869462995, Val f1: 0.7699597477912903\n",
            "Val loss: 0.5203305976021857, Val f1: 0.7683387398719788\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.0514766573905945, Val f1: 1.4980487823486328\n",
            "Val loss: 0.7021069924036661, Val f1: 0.9986767768859863\n",
            "Val loss: 0.6313164949417114, Val f1: 0.8994108438491821\n",
            "Val loss: 0.602108529635838, Val f1: 0.8522889614105225\n",
            "Val loss: 0.586765468120575, Val f1: 0.8270581960678101\n",
            "\n",
            "starting Epoch 4\n",
            "Training...\n",
            "Train loss: 0.5389480926096439\n",
            "Train loss: 0.524504135052363\n",
            "Train loss: 0.5140630328655242\n",
            "Train loss: 0.5105222548121837\n",
            "Train loss: 0.5081873855420521\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5408917032182217, Val f1: 0.8213843703269958\n",
            "Val loss: 0.5273145626891743, Val f1: 0.7948361039161682\n",
            "Val loss: 0.5218452215194702, Val f1: 0.7867740988731384\n",
            "Val loss: 0.5204510457480132, Val f1: 0.7812254428863525\n",
            "Val loss: 0.51865287621816, Val f1: 0.7791897058486938\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.061169981956482, Val f1: 1.5145082473754883\n",
            "Val loss: 0.706650952498118, Val f1: 1.0110681056976318\n",
            "Val loss: 0.6343865633010864, Val f1: 0.9117076992988586\n",
            "Val loss: 0.605131983757019, Val f1: 0.8648386001586914\n",
            "Val loss: 0.5890250205993652, Val f1: 0.8392922282218933\n",
            "\n",
            "starting Epoch 5\n",
            "Training...\n",
            "Train loss: 0.5192202460020781\n",
            "Train loss: 0.504935419920719\n",
            "Train loss: 0.4972308075428009\n",
            "Train loss: 0.49300095216551826\n",
            "Train loss: 0.48964755946681615\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5110474713146687, Val f1: 0.8249075412750244\n",
            "Val loss: 0.49392162489168573, Val f1: 0.8010648488998413\n",
            "Val loss: 0.48909898459911344, Val f1: 0.7924306392669678\n",
            "Val loss: 0.48619772026787944, Val f1: 0.7882122993469238\n",
            "Val loss: 0.4842195592465855, Val f1: 0.78564453125\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.0084934830665588, Val f1: 1.503571629524231\n",
            "Val loss: 0.6714578866958618, Val f1: 0.9992562532424927\n",
            "Val loss: 0.6029440522193908, Val f1: 0.9038060307502747\n",
            "Val loss: 0.5756215878895351, Val f1: 0.8563262224197388\n",
            "Val loss: 0.5603185494740804, Val f1: 0.8329300880432129\n",
            "\n",
            "starting Epoch 6\n",
            "Training...\n",
            "Train loss: 0.49893081933259964\n",
            "Train loss: 0.48033453208027466\n",
            "Train loss: 0.4754404813051224\n",
            "Train loss: 0.47506131165063203\n",
            "Train loss: 0.4737196085708482\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.4958664681762457, Val f1: 0.8435635566711426\n",
            "Val loss: 0.48294764576536237, Val f1: 0.8149824738502502\n",
            "Val loss: 0.47937828838825225, Val f1: 0.8054673075675964\n",
            "Val loss: 0.4778046029717175, Val f1: 0.8005627393722534\n",
            "Val loss: 0.47658562198990867, Val f1: 0.7973065376281738\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.0121128559112549, Val f1: 1.5118741989135742\n",
            "Val loss: 0.6731365819772085, Val f1: 1.0107836723327637\n",
            "Val loss: 0.6029419422149658, Val f1: 0.9134097099304199\n",
            "Val loss: 0.5758598617144993, Val f1: 0.8661942481994629\n",
            "Val loss: 0.5602710048357645, Val f1: 0.8428796529769897\n",
            "\n",
            "starting Epoch 7\n",
            "Training...\n",
            "Train loss: 0.4891683477908373\n",
            "Train loss: 0.4678424298763275\n",
            "Train loss: 0.46354985117912295\n",
            "Train loss: 0.45991487689872285\n",
            "Train loss: 0.45671033149673823\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.4746254328638315, Val f1: 0.8487312197685242\n",
            "Val loss: 0.46198317047321436, Val f1: 0.8214554190635681\n",
            "Val loss: 0.4576829922199249, Val f1: 0.8122257590293884\n",
            "Val loss: 0.4560693017582395, Val f1: 0.8076037764549255\n",
            "Val loss: 0.45410108956552686, Val f1: 0.8061053156852722\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.9888593554496765, Val f1: 1.5160576105117798\n",
            "Val loss: 0.6561843852202097, Val f1: 1.0116939544677734\n",
            "Val loss: 0.5872113108634949, Val f1: 0.9150484204292297\n",
            "Val loss: 0.5614676560674395, Val f1: 0.8674686551094055\n",
            "Val loss: 0.5462710890505049, Val f1: 0.845108687877655\n",
            "\n",
            "starting Epoch 8\n",
            "Training...\n",
            "Train loss: 0.47161833941936493\n",
            "Train loss: 0.4543473278031205\n",
            "Train loss: 0.4482604748010635\n",
            "Train loss: 0.446863163318207\n",
            "Train loss: 0.445193774998188\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.4661424569785595, Val f1: 0.8515241742134094\n",
            "Val loss: 0.44756496223536407, Val f1: 0.8283427953720093\n",
            "Val loss: 0.4426191508769989, Val f1: 0.820003867149353\n",
            "Val loss: 0.4405150328999135, Val f1: 0.8162595629692078\n",
            "Val loss: 0.43906334255422863, Val f1: 0.8136875033378601\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.9758396744728088, Val f1: 1.5264836549758911\n",
            "Val loss: 0.6486170291900635, Val f1: 1.0141382217407227\n",
            "Val loss: 0.5794608294963837, Val f1: 0.9175549745559692\n",
            "Val loss: 0.5552470641476768, Val f1: 0.8695734739303589\n",
            "Val loss: 0.5400388671292199, Val f1: 0.8485321402549744\n",
            "\n",
            "starting Epoch 9\n",
            "Training...\n",
            "Train loss: 0.4498176723718643\n",
            "Train loss: 0.43603698773817584\n",
            "Train loss: 0.43091118454933164\n",
            "Train loss: 0.42892085571787253\n",
            "Train loss: 0.4275205209851265\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.44506108947098255, Val f1: 0.8566464781761169\n",
            "Val loss: 0.4302005135651791, Val f1: 0.8291910886764526\n",
            "Val loss: 0.4257154279947281, Val f1: 0.820410966873169\n",
            "Val loss: 0.42486460262270115, Val f1: 0.8160087466239929\n",
            "Val loss: 0.4244619221204803, Val f1: 0.8134189248085022\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.9634235203266144, Val f1: 1.4954955577850342\n",
            "Val loss: 0.6415708363056183, Val f1: 1.0042967796325684\n",
            "Val loss: 0.5734046161174774, Val f1: 0.9107270240783691\n",
            "Val loss: 0.5501270379338946, Val f1: 0.863116443157196\n",
            "Val loss: 0.5354765090677474, Val f1: 0.8422287106513977\n",
            "\n",
            "starting Epoch 10\n",
            "Training...\n",
            "Train loss: 0.4394506197422743\n",
            "Train loss: 0.4228408472104506\n",
            "Train loss: 0.4175425350666046\n",
            "Train loss: 0.41503637895655277\n",
            "Train loss: 0.41272951378708794\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.42878282628953457, Val f1: 0.8605407476425171\n",
            "Val loss: 0.41683881571798614, Val f1: 0.8371070027351379\n",
            "Val loss: 0.4133449685573578, Val f1: 0.8294376134872437\n",
            "Val loss: 0.4110643610135833, Val f1: 0.8254050612449646\n",
            "Val loss: 0.41006196112859816, Val f1: 0.8222814202308655\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.9557004868984222, Val f1: 1.497622013092041\n",
            "Val loss: 0.6370932360490164, Val f1: 1.0049793720245361\n",
            "Val loss: 0.5688603699207306, Val f1: 0.9103994369506836\n",
            "Val loss: 0.5462315210274288, Val f1: 0.8624463081359863\n",
            "Val loss: 0.5312773485978445, Val f1: 0.8408228754997253\n",
            "\n",
            "starting Epoch 11\n",
            "Training...\n",
            "Train loss: 0.43137481436133385\n",
            "Train loss: 0.4129303206096996\n",
            "Train loss: 0.40585060536861417\n",
            "Train loss: 0.4037919983045379\n",
            "Train loss: 0.4038668310358411\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.4177211318165064, Val f1: 0.8692368865013123\n",
            "Val loss: 0.4062326523390683, Val f1: 0.8421627283096313\n",
            "Val loss: 0.404622528553009, Val f1: 0.8314038515090942\n",
            "Val loss: 0.4024045329485367, Val f1: 0.8270365595817566\n",
            "Val loss: 0.401186357651438, Val f1: 0.8249660134315491\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.9574868977069855, Val f1: 1.483163595199585\n",
            "Val loss: 0.6384674410025278, Val f1: 1.001741647720337\n",
            "Val loss: 0.5695096611976623, Val f1: 0.9052101969718933\n",
            "Val loss: 0.5479348131588527, Val f1: 0.8578982353210449\n",
            "Val loss: 0.5330002771483527, Val f1: 0.8371441960334778\n",
            "\n",
            "starting Epoch 12\n",
            "Training...\n",
            "Train loss: 0.41415571235120296\n",
            "Train loss: 0.3984471523400509\n",
            "Train loss: 0.3932369029521942\n",
            "Train loss: 0.39023861778316216\n",
            "Train loss: 0.3897093275473231\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.4265655018389225, Val f1: 0.8378633260726929\n",
            "Val loss: 0.4146585636066668, Val f1: 0.8137709498405457\n",
            "Val loss: 0.40943790912628175, Val f1: 0.8062066435813904\n",
            "Val loss: 0.4086498627022131, Val f1: 0.8002326488494873\n",
            "Val loss: 0.4092040824748221, Val f1: 0.7963875532150269\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.989733874797821, Val f1: 1.4253723621368408\n",
            "Val loss: 0.6609424750010172, Val f1: 0.9603725671768188\n",
            "Val loss: 0.5924769163131713, Val f1: 0.8640799522399902\n",
            "Val loss: 0.5701452323368618, Val f1: 0.8169642686843872\n",
            "Val loss: 0.5551334122816721, Val f1: 0.7964061498641968\n",
            "\n",
            "starting Epoch 13\n",
            "Training...\n",
            "Train loss: 0.40843626856803894\n",
            "Train loss: 0.3891945526455388\n",
            "Train loss: 0.38305742740631105\n",
            "Train loss: 0.381670743227005\n",
            "Train loss: 0.379849116007487\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.39390758611261845, Val f1: 0.8794683814048767\n",
            "Val loss: 0.38250709663737903, Val f1: 0.8530401587486267\n",
            "Val loss: 0.37944055736064913, Val f1: 0.8451964259147644\n",
            "Val loss: 0.3792280363502787, Val f1: 0.8404340744018555\n",
            "Val loss: 0.3784039148262569, Val f1: 0.8381013870239258\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.9513800740242004, Val f1: 1.4921064376831055\n",
            "Val loss: 0.6371106803417206, Val f1: 1.0018380880355835\n",
            "Val loss: 0.5681202232837677, Val f1: 0.903853714466095\n",
            "Val loss: 0.5475334184510368, Val f1: 0.8567818999290466\n",
            "Val loss: 0.5327766074074639, Val f1: 0.8364686965942383\n",
            "\n",
            "starting Epoch 14\n",
            "Training...\n",
            "Train loss: 0.3806048706173897\n",
            "Train loss: 0.370844076980244\n",
            "Train loss: 0.3669097536802292\n",
            "Train loss: 0.3672049912943769\n",
            "Train loss: 0.3679200169586\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.4014715664088726, Val f1: 0.8613235354423523\n",
            "Val loss: 0.3915403232429967, Val f1: 0.8339837789535522\n",
            "Val loss: 0.3873132282495499, Val f1: 0.826772928237915\n",
            "Val loss: 0.38544261989308826, Val f1: 0.8228909373283386\n",
            "Val loss: 0.38373347584690365, Val f1: 0.8208668231964111\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.9806056618690491, Val f1: 1.4448928833007812\n",
            "Val loss: 0.657270093758901, Val f1: 0.9758143424987793\n",
            "Val loss: 0.5879813849925994, Val f1: 0.8798511624336243\n",
            "Val loss: 0.5670332270009177, Val f1: 0.8337978720664978\n",
            "Val loss: 0.551900198062261, Val f1: 0.8119064569473267\n",
            "\n",
            "starting Epoch 15\n",
            "Training...\n",
            "Train loss: 0.37303925305604935\n",
            "Train loss: 0.3632700208461646\n",
            "Train loss: 0.360372286438942\n",
            "Train loss: 0.3586094757514213\n",
            "Train loss: 0.3570956421040353\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.40032683685421944, Val f1: 0.8600810170173645\n",
            "Val loss: 0.3872827941721136, Val f1: 0.8358413577079773\n",
            "Val loss: 0.3848261106014252, Val f1: 0.8255820274353027\n",
            "Val loss: 0.3813556432723999, Val f1: 0.8215938210487366\n",
            "Val loss: 0.3795842042281514, Val f1: 0.8203762769699097\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.9936909675598145, Val f1: 1.4415926933288574\n",
            "Val loss: 0.6669003268082937, Val f1: 0.9713268280029297\n",
            "Val loss: 0.596746176481247, Val f1: 0.8737797141075134\n",
            "Val loss: 0.5758257806301117, Val f1: 0.8282236456871033\n",
            "Val loss: 0.5606687300735049, Val f1: 0.8060240149497986\n",
            "\n",
            "starting Epoch 16\n",
            "Training...\n",
            "Train loss: 0.3693357724696398\n",
            "Train loss: 0.35537895108714246\n",
            "Train loss: 0.3540010958909988\n",
            "Train loss: 0.3538418803642045\n",
            "Train loss: 0.35374483927374795\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.37635056860744953, Val f1: 0.8795679807662964\n",
            "Val loss: 0.3695346008647572, Val f1: 0.8485373258590698\n",
            "Val loss: 0.3677793389558792, Val f1: 0.838722288608551\n",
            "Val loss: 0.3683194406886599, Val f1: 0.8325097560882568\n",
            "Val loss: 0.3679915063437961, Val f1: 0.8303233981132507\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.9898206889629364, Val f1: 1.4533579349517822\n",
            "Val loss: 0.6656862199306488, Val f1: 0.9794083833694458\n",
            "Val loss: 0.594640451669693, Val f1: 0.8819250464439392\n",
            "Val loss: 0.5743731728621891, Val f1: 0.8362244367599487\n",
            "Val loss: 0.5592152840561337, Val f1: 0.813761830329895\n",
            "\n",
            "starting Epoch 17\n",
            "Training...\n",
            "Train loss: 0.35265097208321095\n",
            "Train loss: 0.3416490410313462\n",
            "Train loss: 0.3404873192310333\n",
            "Train loss: 0.3393469346103384\n",
            "Train loss: 0.33976847394591286\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.36108359694480896, Val f1: 0.8989816904067993\n",
            "Val loss: 0.35027429371169116, Val f1: 0.8724411725997925\n",
            "Val loss: 0.3463434147834778, Val f1: 0.8648281693458557\n",
            "Val loss: 0.3435956617789482, Val f1: 0.8604345917701721\n",
            "Val loss: 0.34336923169238226, Val f1: 0.8573800921440125\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.9578990936279297, Val f1: 1.5144857168197632\n",
            "Val loss: 0.6482189695040385, Val f1: 1.0113263130187988\n",
            "Val loss: 0.5773038029670715, Val f1: 0.9116846323013306\n",
            "Val loss: 0.5580690843718392, Val f1: 0.8647869229316711\n",
            "Val loss: 0.5426237318250868, Val f1: 0.8440792560577393\n",
            "\n",
            "starting Epoch 18\n",
            "Training...\n",
            "Train loss: 0.3454911131411791\n",
            "Train loss: 0.3348033906835498\n",
            "Train loss: 0.33388220369815824\n",
            "Train loss: 0.3324122669091865\n",
            "Train loss: 0.33287442333641504\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.3573749102652073, Val f1: 0.8948179483413696\n",
            "Val loss: 0.3466581241651015, Val f1: 0.8665425777435303\n",
            "Val loss: 0.3439661699533463, Val f1: 0.8580799698829651\n",
            "Val loss: 0.341267421619216, Val f1: 0.8557307124137878\n",
            "Val loss: 0.3407439433393024, Val f1: 0.8531818389892578\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.9766463339328766, Val f1: 1.4950999021530151\n",
            "Val loss: 0.661701112985611, Val f1: 1.0009548664093018\n",
            "Val loss: 0.5893747329711914, Val f1: 0.9001631140708923\n",
            "Val loss: 0.5699893151010785, Val f1: 0.8546044826507568\n",
            "Val loss: 0.5542391604847379, Val f1: 0.833103597164154\n",
            "\n",
            "starting Epoch 19\n",
            "Training...\n",
            "Train loss: 0.3499406538903713\n",
            "Train loss: 0.33470159046577685\n",
            "Train loss: 0.33051242113113405\n",
            "Train loss: 0.32833602312785476\n",
            "Train loss: 0.3274528118116515\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.348955973982811, Val f1: 0.9026665091514587\n",
            "Val loss: 0.3382586141427358, Val f1: 0.8731703162193298\n",
            "Val loss: 0.3354395651817322, Val f1: 0.8652044534683228\n",
            "Val loss: 0.3326236627884765, Val f1: 0.8624154925346375\n",
            "Val loss: 0.33263712305398213, Val f1: 0.8597085475921631\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.9764210879802704, Val f1: 1.5048322677612305\n",
            "Val loss: 0.6638346314430237, Val f1: 1.0056294202804565\n",
            "Val loss: 0.5910385251045227, Val f1: 0.9054684638977051\n",
            "Val loss: 0.5721294624464852, Val f1: 0.8601149320602417\n",
            "Val loss: 0.5563461614979638, Val f1: 0.838825523853302\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_pmWPDdDzmkr"
      },
      "source": [
        "def predict(model, iterator):\n",
        "    model.eval()\n",
        "    fp = []\n",
        "    fn = []\n",
        "    tp = [] \n",
        "    tn = []\n",
        "    with torch.no_grad():\n",
        "        for i, (texts, symbols, ys) in enumerate(iterator):   \n",
        "            preds = model(texts, symbols)  # делаем предсказания на тесте \n",
        "            for pred, gold, text, sym in zip(preds, ys, texts, symbols):\n",
        "              text = ' '.join([id2word[int(symbol)] for symbol in text if symbol !=0])\n",
        "              if round(pred.item()) > gold:\n",
        "                fp.append(text)\n",
        "              elif round(pred.item()) < gold:\n",
        "                fn.append(text)\n",
        "              elif round(pred.item()) == gold == 1:\n",
        "                tp.append(text)\n",
        "              elif round(pred.item()) == gold == 0:\n",
        "                tn.append(text)\n",
        "    return fp, fn, tp, tn"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNBmytlT7GHh"
      },
      "source": [
        "fp, fn, tp, tn = predict(model, val_iterator)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjqvqKVi7HX8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "839cd6e5-5c40-4690-8b1b-0d7d72c3a0a6"
      },
      "source": [
        "print('что правильно предсказываем: ', tp[:5])"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "что правильно предсказываем:  ['rt 12 декабря  виктория фестиваль добра', 'то чувство когда понимаешь что твоя презентация самая лучшая наслаждение', 'наконец нормальный выходной с в кафе понеслааась ', 'rt wylsacom запускают angry birds go в один день с gta sa ну удачи горячим парням d', 'с лизой а ты моя кароч игрушка будешь']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQpGyPHE7Q-M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "614116a4-b359-4250-c5ad-96bfff7b1baf"
      },
      "source": [
        "print('ошибочно не относим к негативным: ', fn[:5])"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ошибочно не относим к негативным:  ['завтра приду в любимых ботинках и смогу забыть эти ужасные сапоги', 'известны результаты олимпиады по району я вторая', 'rt', 'rt открытие нового центра управления в теперь в ', 'во всем надо видеть плюсы я вот например до пятницы могу есть чеснок']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bXkklMI57XRI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "812e37f2-56f3-4d12-ef16-fe25d79118c2"
      },
      "source": [
        "print('ошибочно считаем положительными: ', fp[:5])"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ошибочно считаем положительными:  ['перечитываю переписку и сново депресняк', 'мой рисунок фотке не очень получилось webcamtoy', 'нормально поесть естественно некогда работа тв германия берлин …', 'своей с толпы людей с и грустными в', 'reedasha ylianatomlinson kevin_real а до меня наконец дошло что аську']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ECNt0DB7t8X"
      },
      "source": [
        "В обоих случаях:\n",
        "\n",
        "* модели хорошо справляется в случаях, когда твиты содержат явно положительные эпитеты, напр. \"хороший\", \"замечательный\" и т.д.\n",
        "* если в твитах нет явных негативных слов, то модели часто не считает их негативными\n",
        "* то же самое происходит с положительными твитами: если в них нет явно положительных слов, то модели могут отнести такие твиты к отрицательным\n",
        "* есть какие-то слова, инерпретация которых сильно зависит от контекста. Видно, что моделям с такими словами тяжеловато, и отсюда тоже возникают ошибки "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGAkf2rX7vge"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}