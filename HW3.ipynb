{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crVQ6TXKhTM2"
      },
      "source": [
        "Импортируем необходимые для работы библиотеки"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TBKQe4Td4Ohh",
        "outputId": "a7bbd177-c6a0-4041-b03d-19999ce9dbc6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-0.6.2-py3-none-any.whl (332 kB)\n",
            "\u001b[?25l\r\u001b[K     |█                               | 10 kB 15.2 MB/s eta 0:00:01\r\u001b[K     |██                              | 20 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |███                             | 30 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |████                            | 40 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |█████                           | 51 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |██████                          | 61 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |███████                         | 71 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 81 kB 5.9 MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 92 kB 5.9 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 102 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 112 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 122 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 133 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 143 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 153 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 163 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 174 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 184 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 194 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 204 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 215 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 225 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 235 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 245 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 256 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 266 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 276 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 286 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 296 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 307 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 317 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 327 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 332 kB 5.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.3.1 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.10.0+cu111)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.3.1->torchmetrics) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->torchmetrics) (3.0.6)\n",
            "Installing collected packages: torchmetrics\n",
            "Successfully installed torchmetrics-0.6.2\n"
          ]
        }
      ],
      "source": [
        "!pip install torchmetrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q6ktzowmhE9x",
        "outputId": "6ede8d7b-35bd-4407-a1f1-1c01b9d8e545"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from sklearn.utils import shuffle\n",
        "from collections import Counter\n",
        "import gensim.downloader\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch.optim as optim\n",
        "import torchmetrics\n",
        "from torchmetrics import F1\n",
        "from torchmetrics.functional import f1, recall\n",
        "import statistics\n",
        "\n",
        "nltk.download('punkt')\n",
        "le = preprocessing.LabelEncoder()\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ybSptlT8hag3"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/Fake.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "1TSeWXNchd6P",
        "outputId": "d489589d-568d-459c-8306-8f0303aa0c15"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-09440bdd-f710-415f-8748-aacf2b90c0a0\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>subject</th>\n",
              "      <th>date</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Donald Trump Sends Out Embarrassing New Year’...</td>\n",
              "      <td>Donald Trump just couldn t wish all Americans ...</td>\n",
              "      <td>News</td>\n",
              "      <td>December 31, 2017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Drunk Bragging Trump Staffer Started Russian ...</td>\n",
              "      <td>House Intelligence Committee Chairman Devin Nu...</td>\n",
              "      <td>News</td>\n",
              "      <td>December 31, 2017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Sheriff David Clarke Becomes An Internet Joke...</td>\n",
              "      <td>On Friday, it was revealed that former Milwauk...</td>\n",
              "      <td>News</td>\n",
              "      <td>December 30, 2017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Trump Is So Obsessed He Even Has Obama’s Name...</td>\n",
              "      <td>On Christmas day, Donald Trump announced that ...</td>\n",
              "      <td>News</td>\n",
              "      <td>December 29, 2017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Pope Francis Just Called Out Donald Trump Dur...</td>\n",
              "      <td>Pope Francis used his annual Christmas Day mes...</td>\n",
              "      <td>News</td>\n",
              "      <td>December 25, 2017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23476</th>\n",
              "      <td>McPain: John McCain Furious That Iran Treated ...</td>\n",
              "      <td>21st Century Wire says As 21WIRE reported earl...</td>\n",
              "      <td>Middle-east</td>\n",
              "      <td>January 16, 2016</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23477</th>\n",
              "      <td>JUSTICE? Yahoo Settles E-mail Privacy Class-ac...</td>\n",
              "      <td>21st Century Wire says It s a familiar theme. ...</td>\n",
              "      <td>Middle-east</td>\n",
              "      <td>January 16, 2016</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23478</th>\n",
              "      <td>Sunnistan: US and Allied ‘Safe Zone’ Plan to T...</td>\n",
              "      <td>Patrick Henningsen  21st Century WireRemember ...</td>\n",
              "      <td>Middle-east</td>\n",
              "      <td>January 15, 2016</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23479</th>\n",
              "      <td>How to Blow $700 Million: Al Jazeera America F...</td>\n",
              "      <td>21st Century Wire says Al Jazeera America will...</td>\n",
              "      <td>Middle-east</td>\n",
              "      <td>January 14, 2016</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23480</th>\n",
              "      <td>10 U.S. Navy Sailors Held by Iranian Military ...</td>\n",
              "      <td>21st Century Wire says As 21WIRE predicted in ...</td>\n",
              "      <td>Middle-east</td>\n",
              "      <td>January 12, 2016</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>23481 rows × 4 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-09440bdd-f710-415f-8748-aacf2b90c0a0')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-09440bdd-f710-415f-8748-aacf2b90c0a0 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-09440bdd-f710-415f-8748-aacf2b90c0a0');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                   title  ...               date\n",
              "0       Donald Trump Sends Out Embarrassing New Year’...  ...  December 31, 2017\n",
              "1       Drunk Bragging Trump Staffer Started Russian ...  ...  December 31, 2017\n",
              "2       Sheriff David Clarke Becomes An Internet Joke...  ...  December 30, 2017\n",
              "3       Trump Is So Obsessed He Even Has Obama’s Name...  ...  December 29, 2017\n",
              "4       Pope Francis Just Called Out Donald Trump Dur...  ...  December 25, 2017\n",
              "...                                                  ...  ...                ...\n",
              "23476  McPain: John McCain Furious That Iran Treated ...  ...   January 16, 2016\n",
              "23477  JUSTICE? Yahoo Settles E-mail Privacy Class-ac...  ...   January 16, 2016\n",
              "23478  Sunnistan: US and Allied ‘Safe Zone’ Plan to T...  ...   January 15, 2016\n",
              "23479  How to Blow $700 Million: Al Jazeera America F...  ...   January 14, 2016\n",
              "23480  10 U.S. Navy Sailors Held by Iranian Military ...  ...   January 12, 2016\n",
              "\n",
              "[23481 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVnBBOlCiC6f"
      },
      "source": [
        "В качестве препроцессинга авторы только приводят текст к нижнему регистру, падят тексты до максимальной длины, и векторизуют готовой моделью ворт2века. Поэтому мы сделаем то же самое, но при паддинге посмотрим на самую длинную последовательность, потому что работаем на других данных"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "hehYTRH9jej5"
      },
      "outputs": [],
      "source": [
        "df['preprocessed_text'] = df['text'].apply(lambda x: word_tokenize(x.lower()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        },
        "id": "RW8zfTTnkzPc",
        "outputId": "26aea369-f5ef-43d6-87b9-7ce3f8724e15"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-9f7a617f-8eca-4ebc-80bc-d85e9720c329\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>subject</th>\n",
              "      <th>date</th>\n",
              "      <th>preprocessed_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Donald Trump Sends Out Embarrassing New Year’...</td>\n",
              "      <td>Donald Trump just couldn t wish all Americans ...</td>\n",
              "      <td>News</td>\n",
              "      <td>December 31, 2017</td>\n",
              "      <td>[donald, trump, just, couldn, t, wish, all, am...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Drunk Bragging Trump Staffer Started Russian ...</td>\n",
              "      <td>House Intelligence Committee Chairman Devin Nu...</td>\n",
              "      <td>News</td>\n",
              "      <td>December 31, 2017</td>\n",
              "      <td>[house, intelligence, committee, chairman, dev...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Sheriff David Clarke Becomes An Internet Joke...</td>\n",
              "      <td>On Friday, it was revealed that former Milwauk...</td>\n",
              "      <td>News</td>\n",
              "      <td>December 30, 2017</td>\n",
              "      <td>[on, friday, ,, it, was, revealed, that, forme...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Trump Is So Obsessed He Even Has Obama’s Name...</td>\n",
              "      <td>On Christmas day, Donald Trump announced that ...</td>\n",
              "      <td>News</td>\n",
              "      <td>December 29, 2017</td>\n",
              "      <td>[on, christmas, day, ,, donald, trump, announc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Pope Francis Just Called Out Donald Trump Dur...</td>\n",
              "      <td>Pope Francis used his annual Christmas Day mes...</td>\n",
              "      <td>News</td>\n",
              "      <td>December 25, 2017</td>\n",
              "      <td>[pope, francis, used, his, annual, christmas, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23476</th>\n",
              "      <td>McPain: John McCain Furious That Iran Treated ...</td>\n",
              "      <td>21st Century Wire says As 21WIRE reported earl...</td>\n",
              "      <td>Middle-east</td>\n",
              "      <td>January 16, 2016</td>\n",
              "      <td>[21st, century, wire, says, as, 21wire, report...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23477</th>\n",
              "      <td>JUSTICE? Yahoo Settles E-mail Privacy Class-ac...</td>\n",
              "      <td>21st Century Wire says It s a familiar theme. ...</td>\n",
              "      <td>Middle-east</td>\n",
              "      <td>January 16, 2016</td>\n",
              "      <td>[21st, century, wire, says, it, s, a, familiar...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23478</th>\n",
              "      <td>Sunnistan: US and Allied ‘Safe Zone’ Plan to T...</td>\n",
              "      <td>Patrick Henningsen  21st Century WireRemember ...</td>\n",
              "      <td>Middle-east</td>\n",
              "      <td>January 15, 2016</td>\n",
              "      <td>[patrick, henningsen, 21st, century, wireremem...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23479</th>\n",
              "      <td>How to Blow $700 Million: Al Jazeera America F...</td>\n",
              "      <td>21st Century Wire says Al Jazeera America will...</td>\n",
              "      <td>Middle-east</td>\n",
              "      <td>January 14, 2016</td>\n",
              "      <td>[21st, century, wire, says, al, jazeera, ameri...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23480</th>\n",
              "      <td>10 U.S. Navy Sailors Held by Iranian Military ...</td>\n",
              "      <td>21st Century Wire says As 21WIRE predicted in ...</td>\n",
              "      <td>Middle-east</td>\n",
              "      <td>January 12, 2016</td>\n",
              "      <td>[21st, century, wire, says, as, 21wire, predic...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>23481 rows × 5 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9f7a617f-8eca-4ebc-80bc-d85e9720c329')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9f7a617f-8eca-4ebc-80bc-d85e9720c329 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9f7a617f-8eca-4ebc-80bc-d85e9720c329');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                   title  ...                                  preprocessed_text\n",
              "0       Donald Trump Sends Out Embarrassing New Year’...  ...  [donald, trump, just, couldn, t, wish, all, am...\n",
              "1       Drunk Bragging Trump Staffer Started Russian ...  ...  [house, intelligence, committee, chairman, dev...\n",
              "2       Sheriff David Clarke Becomes An Internet Joke...  ...  [on, friday, ,, it, was, revealed, that, forme...\n",
              "3       Trump Is So Obsessed He Even Has Obama’s Name...  ...  [on, christmas, day, ,, donald, trump, announc...\n",
              "4       Pope Francis Just Called Out Donald Trump Dur...  ...  [pope, francis, used, his, annual, christmas, ...\n",
              "...                                                  ...  ...                                                ...\n",
              "23476  McPain: John McCain Furious That Iran Treated ...  ...  [21st, century, wire, says, as, 21wire, report...\n",
              "23477  JUSTICE? Yahoo Settles E-mail Privacy Class-ac...  ...  [21st, century, wire, says, it, s, a, familiar...\n",
              "23478  Sunnistan: US and Allied ‘Safe Zone’ Plan to T...  ...  [patrick, henningsen, 21st, century, wireremem...\n",
              "23479  How to Blow $700 Million: Al Jazeera America F...  ...  [21st, century, wire, says, al, jazeera, ameri...\n",
              "23480  10 U.S. Navy Sailors Held by Iranian Military ...  ...  [21st, century, wire, says, as, 21wire, predic...\n",
              "\n",
              "[23481 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "RsACSSGIk1Te"
      },
      "outputs": [],
      "source": [
        "df['target'] = df['subject'].astype('category').cat.codes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O05gImpmm1ZD",
        "outputId": "1eb6d413-e084-479d-fad7-1784c3bcc5fd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LabelEncoder()"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "le.fit(list(df.subject))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KkbyFRW2m23T",
        "outputId": "48f7b419-a5b4-4dda-8722-e7c7f641dd41"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['Government News', 'Middle-east', 'News', 'US_News', 'left-news',\n",
              "       'politics'], dtype='<U15')"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "le.classes_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "gJt9eFJwvlWC"
      },
      "outputs": [],
      "source": [
        "df[\"target\"] = list(le.transform(list(df.subject)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "WMGS0MhjnfSa"
      },
      "outputs": [],
      "source": [
        "df[\"preprocessed_2\"] = df['preprocessed_text'].apply(lambda x: x[:500])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 633
        },
        "id": "qomzuGe2v88J",
        "outputId": "bb73a04d-db1a-4e77-bd4b-f4594c127196"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-2b49e1c5-5850-4095-a292-e7d56b077c79\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>subject</th>\n",
              "      <th>date</th>\n",
              "      <th>preprocessed_text</th>\n",
              "      <th>target</th>\n",
              "      <th>preprocessed_2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Donald Trump Sends Out Embarrassing New Year’...</td>\n",
              "      <td>Donald Trump just couldn t wish all Americans ...</td>\n",
              "      <td>News</td>\n",
              "      <td>December 31, 2017</td>\n",
              "      <td>[donald, trump, just, couldn, t, wish, all, am...</td>\n",
              "      <td>2</td>\n",
              "      <td>[donald, trump, just, couldn, t, wish, all, am...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Drunk Bragging Trump Staffer Started Russian ...</td>\n",
              "      <td>House Intelligence Committee Chairman Devin Nu...</td>\n",
              "      <td>News</td>\n",
              "      <td>December 31, 2017</td>\n",
              "      <td>[house, intelligence, committee, chairman, dev...</td>\n",
              "      <td>2</td>\n",
              "      <td>[house, intelligence, committee, chairman, dev...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Sheriff David Clarke Becomes An Internet Joke...</td>\n",
              "      <td>On Friday, it was revealed that former Milwauk...</td>\n",
              "      <td>News</td>\n",
              "      <td>December 30, 2017</td>\n",
              "      <td>[on, friday, ,, it, was, revealed, that, forme...</td>\n",
              "      <td>2</td>\n",
              "      <td>[on, friday, ,, it, was, revealed, that, forme...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Trump Is So Obsessed He Even Has Obama’s Name...</td>\n",
              "      <td>On Christmas day, Donald Trump announced that ...</td>\n",
              "      <td>News</td>\n",
              "      <td>December 29, 2017</td>\n",
              "      <td>[on, christmas, day, ,, donald, trump, announc...</td>\n",
              "      <td>2</td>\n",
              "      <td>[on, christmas, day, ,, donald, trump, announc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Pope Francis Just Called Out Donald Trump Dur...</td>\n",
              "      <td>Pope Francis used his annual Christmas Day mes...</td>\n",
              "      <td>News</td>\n",
              "      <td>December 25, 2017</td>\n",
              "      <td>[pope, francis, used, his, annual, christmas, ...</td>\n",
              "      <td>2</td>\n",
              "      <td>[pope, francis, used, his, annual, christmas, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23476</th>\n",
              "      <td>McPain: John McCain Furious That Iran Treated ...</td>\n",
              "      <td>21st Century Wire says As 21WIRE reported earl...</td>\n",
              "      <td>Middle-east</td>\n",
              "      <td>January 16, 2016</td>\n",
              "      <td>[21st, century, wire, says, as, 21wire, report...</td>\n",
              "      <td>1</td>\n",
              "      <td>[21st, century, wire, says, as, 21wire, report...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23477</th>\n",
              "      <td>JUSTICE? Yahoo Settles E-mail Privacy Class-ac...</td>\n",
              "      <td>21st Century Wire says It s a familiar theme. ...</td>\n",
              "      <td>Middle-east</td>\n",
              "      <td>January 16, 2016</td>\n",
              "      <td>[21st, century, wire, says, it, s, a, familiar...</td>\n",
              "      <td>1</td>\n",
              "      <td>[21st, century, wire, says, it, s, a, familiar...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23478</th>\n",
              "      <td>Sunnistan: US and Allied ‘Safe Zone’ Plan to T...</td>\n",
              "      <td>Patrick Henningsen  21st Century WireRemember ...</td>\n",
              "      <td>Middle-east</td>\n",
              "      <td>January 15, 2016</td>\n",
              "      <td>[patrick, henningsen, 21st, century, wireremem...</td>\n",
              "      <td>1</td>\n",
              "      <td>[patrick, henningsen, 21st, century, wireremem...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23479</th>\n",
              "      <td>How to Blow $700 Million: Al Jazeera America F...</td>\n",
              "      <td>21st Century Wire says Al Jazeera America will...</td>\n",
              "      <td>Middle-east</td>\n",
              "      <td>January 14, 2016</td>\n",
              "      <td>[21st, century, wire, says, al, jazeera, ameri...</td>\n",
              "      <td>1</td>\n",
              "      <td>[21st, century, wire, says, al, jazeera, ameri...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23480</th>\n",
              "      <td>10 U.S. Navy Sailors Held by Iranian Military ...</td>\n",
              "      <td>21st Century Wire says As 21WIRE predicted in ...</td>\n",
              "      <td>Middle-east</td>\n",
              "      <td>January 12, 2016</td>\n",
              "      <td>[21st, century, wire, says, as, 21wire, predic...</td>\n",
              "      <td>1</td>\n",
              "      <td>[21st, century, wire, says, as, 21wire, predic...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>23481 rows × 7 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2b49e1c5-5850-4095-a292-e7d56b077c79')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2b49e1c5-5850-4095-a292-e7d56b077c79 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2b49e1c5-5850-4095-a292-e7d56b077c79');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                   title  ...                                     preprocessed_2\n",
              "0       Donald Trump Sends Out Embarrassing New Year’...  ...  [donald, trump, just, couldn, t, wish, all, am...\n",
              "1       Drunk Bragging Trump Staffer Started Russian ...  ...  [house, intelligence, committee, chairman, dev...\n",
              "2       Sheriff David Clarke Becomes An Internet Joke...  ...  [on, friday, ,, it, was, revealed, that, forme...\n",
              "3       Trump Is So Obsessed He Even Has Obama’s Name...  ...  [on, christmas, day, ,, donald, trump, announc...\n",
              "4       Pope Francis Just Called Out Donald Trump Dur...  ...  [pope, francis, used, his, annual, christmas, ...\n",
              "...                                                  ...  ...                                                ...\n",
              "23476  McPain: John McCain Furious That Iran Treated ...  ...  [21st, century, wire, says, as, 21wire, report...\n",
              "23477  JUSTICE? Yahoo Settles E-mail Privacy Class-ac...  ...  [21st, century, wire, says, it, s, a, familiar...\n",
              "23478  Sunnistan: US and Allied ‘Safe Zone’ Plan to T...  ...  [patrick, henningsen, 21st, century, wireremem...\n",
              "23479  How to Blow $700 Million: Al Jazeera America F...  ...  [21st, century, wire, says, al, jazeera, ameri...\n",
              "23480  10 U.S. Navy Sailors Held by Iranian Military ...  ...  [21st, century, wire, says, as, 21wire, predic...\n",
              "\n",
              "[23481 rows x 7 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AITW3BCLogbd",
        "outputId": "ea297711-27dd-42d3-91c5-b139bdb7113a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "500"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "len(list(df.preprocessed_2)[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "PfXHEPtmwuNq"
      },
      "outputs": [],
      "source": [
        "news_data = shuffle(df[['preprocessed_2','target']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "qRMSqhxWv9sx"
      },
      "outputs": [],
      "source": [
        "train_texts, val_texts = train_test_split(news_data, test_size=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "id": "G7gB9xgMxP44",
        "outputId": "0fb854d1-ea43-49e4-9ecd-c89939bb9ef9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-b8f79ca2-194d-4eb4-90eb-69876a366c0c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>preprocessed_2</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>18375</th>\n",
              "      <td>[when, cnn, ,, who, is, owned, by, multi-billi...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7847</th>\n",
              "      <td>[a, male, idaho, republican, with, five, daugh...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16802</th>\n",
              "      <td>[you, seriously, can, t, make, this, up, ., th...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10622</th>\n",
              "      <td>[virginia, governor, terry, mcauliffe, decided...</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8956</th>\n",
              "      <td>[president, obama, could, not, hold, back, the...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11170</th>\n",
              "      <td>[the, syrian, refugee, who, shocked, cnn, was,...</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22810</th>\n",
              "      <td>[21st, century, wire, says, the, culture, wars...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10262</th>\n",
              "      <td>[do, you, just, want, to, wring, this, guy, s,...</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20998</th>\n",
              "      <td>[all, of, a, sudden, #, blackdeathsmatter, cou...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19712</th>\n",
              "      <td>[this, is, hands, down, the, best, commentary,...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b8f79ca2-194d-4eb4-90eb-69876a366c0c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b8f79ca2-194d-4eb4-90eb-69876a366c0c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b8f79ca2-194d-4eb4-90eb-69876a366c0c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                          preprocessed_2  target\n",
              "18375  [when, cnn, ,, who, is, owned, by, multi-billi...       4\n",
              "7847   [a, male, idaho, republican, with, five, daugh...       2\n",
              "16802  [you, seriously, can, t, make, this, up, ., th...       0\n",
              "10622  [virginia, governor, terry, mcauliffe, decided...       5\n",
              "8956   [president, obama, could, not, hold, back, the...       2\n",
              "11170  [the, syrian, refugee, who, shocked, cnn, was,...       5\n",
              "22810  [21st, century, wire, says, the, culture, wars...       1\n",
              "10262  [do, you, just, want, to, wring, this, guy, s,...       5\n",
              "20998  [all, of, a, sudden, #, blackdeathsmatter, cou...       4\n",
              "19712  [this, is, hands, down, the, best, commentary,...       4"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "train_texts[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LNRybz6XxaET",
        "outputId": "cd0677ee-55f4-43f5-f10b-0ff2401941f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "всего уникальных токенов: 151903\n"
          ]
        }
      ],
      "source": [
        "vocab = Counter()\n",
        "\n",
        "for text in news_data['preprocessed_2']:\n",
        "    vocab.update(text)\n",
        "print('всего уникальных токенов:', len(vocab))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VyVhi1qsyvI7",
        "outputId": "38509c95-df3d-4f3a-d7eb-c840f14c358c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "уникальных токенов, втретившихся больше 2 раз: 49898\n"
          ]
        }
      ],
      "source": [
        "filtered_vocab = set()\n",
        "\n",
        "for word in vocab:\n",
        "    if vocab[word] > 2:\n",
        "        filtered_vocab.add(word)\n",
        "print('уникальных токенов, втретившихся больше 2 раз:', len(filtered_vocab))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "FQHdmAtsx0zN"
      },
      "outputs": [],
      "source": [
        "word2id = {'PAD':0}\n",
        "\n",
        "for word in filtered_vocab:\n",
        "    word2id[word] = len(word2id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "pkw1jFRXyFNy"
      },
      "outputs": [],
      "source": [
        "id2word = {i:word for word, i in word2id.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mByTehXGyFms",
        "outputId": "823e6d4c-b77e-43a4-c9d9-ff5099f1e0f1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 132
        }
      ],
      "source": [
        "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "DEVICE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1XiZ2w0E2JjF",
        "outputId": "f2d3a338-469a-4fa8-9656-434a1c729706"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9938"
            ]
          },
          "metadata": {},
          "execution_count": 133
        }
      ],
      "source": [
        "max(len(elem) for elem in list(df.preprocessed_text))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZBFhm2y4jzY"
      },
      "source": [
        "Чтобы колаб меня не забанил так такие длинющие паддинги, выберем длину самостоятельно"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ffqHTna4y7v",
        "outputId": "cce5fdf0-41b3-42fc-812f-c2c02b0231d9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "469.4998083556918"
            ]
          },
          "metadata": {},
          "execution_count": 134
        }
      ],
      "source": [
        "statistics.mean(len(elem) for elem in list(df.preprocessed_text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yw_eo7Qt5o3H"
      },
      "source": [
        "Возьмем длину 500"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "id": "hs2Pwg5Z0m-6"
      },
      "outputs": [],
      "source": [
        "class NewsDataset(Dataset):\n",
        "\n",
        "    def __init__(self, dataset, word2id, DEVICE):\n",
        "        self.dataset = dataset['preprocessed_2'].values\n",
        "        self.word2id = word2id\n",
        "        self.length = dataset.shape[0]\n",
        "        self.target = dataset['target'].values\n",
        "        self.device = DEVICE\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, index): \n",
        "        words = self.dataset[index]\n",
        "        ids = torch.LongTensor([self.word2id[word] for word in words if word in self.word2id])\n",
        "        y = [self.target[index]]\n",
        "        return ids, y\n",
        "\n",
        "    def collate_fn(self, batch): \n",
        "      ids, y = list(zip(*batch))\n",
        "      ids_500 = [(l[:500]) for l in ids]\n",
        "      # print(ids[0])\n",
        "      # padded_ids = nn.ConstantPad1d(500, ids).to(self.device)\n",
        "      # print(type(padded_ids))\n",
        "      padded_ids = pad_sequence(ids_500, batch_first=True).to(self.device)\n",
        "      # print(type(padded_ids))\n",
        "      y = torch.Tensor(y).to(self.device)\n",
        "      return padded_ids, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "id": "a_P3-_UD66Hi"
      },
      "outputs": [],
      "source": [
        "train_dataset = NewsDataset(train_texts, word2id, DEVICE)\n",
        "train_sampler = RandomSampler(train_dataset)\n",
        "train_iterator = DataLoader(train_dataset, collate_fn = train_dataset.collate_fn, sampler=train_sampler, batch_size=64, drop_last=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_texts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "ZYFgUd89_b4f",
        "outputId": "1636b8e2-a0f4-4466-93a1-c0dc0bfec53e"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-4ed06ebe-4175-4af8-89b2-b3f4f817cb43\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>preprocessed_2</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>18375</th>\n",
              "      <td>[when, cnn, ,, who, is, owned, by, multi-billi...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7847</th>\n",
              "      <td>[a, male, idaho, republican, with, five, daugh...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16802</th>\n",
              "      <td>[you, seriously, can, t, make, this, up, ., th...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10622</th>\n",
              "      <td>[virginia, governor, terry, mcauliffe, decided...</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8956</th>\n",
              "      <td>[president, obama, could, not, hold, back, the...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1015</th>\n",
              "      <td>[ever, since, same-sex, marriage, became, the,...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23276</th>\n",
              "      <td>[tune, in, to, the, alternate, current, radio,...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17413</th>\n",
              "      <td>[as, it, turns, out, ,, 67, year, old, hillary...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19876</th>\n",
              "      <td>[the, office, of, immigration, statistics, rep...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10893</th>\n",
              "      <td>[donald, trump, junior, was, attending, a, ral...</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>18784 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4ed06ebe-4175-4af8-89b2-b3f4f817cb43')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4ed06ebe-4175-4af8-89b2-b3f4f817cb43 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4ed06ebe-4175-4af8-89b2-b3f4f817cb43');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                          preprocessed_2  target\n",
              "18375  [when, cnn, ,, who, is, owned, by, multi-billi...       4\n",
              "7847   [a, male, idaho, republican, with, five, daugh...       2\n",
              "16802  [you, seriously, can, t, make, this, up, ., th...       0\n",
              "10622  [virginia, governor, terry, mcauliffe, decided...       5\n",
              "8956   [president, obama, could, not, hold, back, the...       2\n",
              "...                                                  ...     ...\n",
              "1015   [ever, since, same-sex, marriage, became, the,...       2\n",
              "23276  [tune, in, to, the, alternate, current, radio,...       1\n",
              "17413  [as, it, turns, out, ,, 67, year, old, hillary...       0\n",
              "19876  [the, office, of, immigration, statistics, rep...       4\n",
              "10893  [donald, trump, junior, was, attending, a, ral...       5\n",
              "\n",
              "[18784 rows x 2 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 137
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {
        "id": "prRkv8QR7KHy"
      },
      "outputs": [],
      "source": [
        "batch = next(iter(train_iterator))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fDM-fwWw7nq3",
        "outputId": "5074b1c2-c357-41fa-cb00-12592428beef"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 3611, 37801, 29216,  ...,     0,     0,     0],\n",
              "        [  746, 26162, 13887,  ...,     0,     0,     0],\n",
              "        [18855,  5988, 13780,  ...,     0,     0,     0],\n",
              "        ...,\n",
              "        [18855,  5988, 13780,  ...,     0,     0,     0],\n",
              "        [ 7781, 30733,   746,  ...,     0,     0,     0],\n",
              "        [31106, 36760, 44472,  ...,     0,     0,     0]], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 139
        }
      ],
      "source": [
        "batch[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CSFnStSN-bEO",
        "outputId": "3dd3d532-d5fb-4f13-892f-49c9d784cbf5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['nobody',\n",
              " 'with',\n",
              " 'any',\n",
              " 'brains',\n",
              " 'has',\n",
              " 'ever',\n",
              " 'accused',\n",
              " 'rush',\n",
              " 'limbaugh',\n",
              " 'of',\n",
              " 'understanding',\n",
              " 'pretty',\n",
              " 'much',\n",
              " 'anything',\n",
              " 'at',\n",
              " 'all',\n",
              " ',',\n",
              " 'but',\n",
              " 'his',\n",
              " 'latest',\n",
              " 'bout',\n",
              " 'with',\n",
              " 'just',\n",
              " 'went',\n",
              " 'way',\n",
              " 'over',\n",
              " 'the',\n",
              " 'line',\n",
              " ',',\n",
              " 'even',\n",
              " 'for',\n",
              " 'him',\n",
              " '.',\n",
              " 'while',\n",
              " 'whining',\n",
              " 'up',\n",
              " 'a',\n",
              " 'storm',\n",
              " 'about',\n",
              " 'decaying',\n",
              " 'sexual',\n",
              " 'morality',\n",
              " 'in',\n",
              " 'this',\n",
              " 'country',\n",
              " ',',\n",
              " 'he',\n",
              " 'tried',\n",
              " 'to',\n",
              " 'paint',\n",
              " 'the',\n",
              " 'left',\n",
              " 'as',\n",
              " 'hypocrites',\n",
              " 'because',\n",
              " 'we',\n",
              " 'tolerate',\n",
              " 'anything',\n",
              " 'sexual',\n",
              " 'as',\n",
              " 'long',\n",
              " 'as',\n",
              " 'there',\n",
              " 's',\n",
              " 'consent',\n",
              " '.',\n",
              " 'when',\n",
              " 'there',\n",
              " 's',\n",
              " 'no',\n",
              " 'consent',\n",
              " ',',\n",
              " 'we',\n",
              " 'send',\n",
              " 'out',\n",
              " 'the',\n",
              " 'rape',\n",
              " 'police',\n",
              " ',',\n",
              " 'and',\n",
              " 'apparently',\n",
              " ',',\n",
              " 'this',\n",
              " 'is',\n",
              " 'a',\n",
              " 'you',\n",
              " 'ever',\n",
              " 'heard',\n",
              " 'anything',\n",
              " 'so',\n",
              " 'completely',\n",
              " 'asinine',\n",
              " 'in',\n",
              " 'your',\n",
              " 'life',\n",
              " '?',\n",
              " 'what',\n",
              " 'people',\n",
              " 'do',\n",
              " 'in',\n",
              " 'their',\n",
              " 'bedrooms',\n",
              " 'is',\n",
              " 'their',\n",
              " 'business',\n",
              " ',',\n",
              " 'but',\n",
              " 'all',\n",
              " 'bets',\n",
              " 'are',\n",
              " 'off',\n",
              " 'when',\n",
              " 'there',\n",
              " 's',\n",
              " 'no',\n",
              " 'consent',\n",
              " '.',\n",
              " 'sex',\n",
              " 'must',\n",
              " 'be',\n",
              " 'consensual',\n",
              " 'or',\n",
              " 'it',\n",
              " 's',\n",
              " 'rape',\n",
              " '.',\n",
              " 'end',\n",
              " 'of',\n",
              " 'story',\n",
              " '.',\n",
              " 'however',\n",
              " ',',\n",
              " 'limbaugh',\n",
              " 'actually',\n",
              " 'sees',\n",
              " 'that',\n",
              " 'as',\n",
              " 'evidence',\n",
              " 'of',\n",
              " 'major',\n",
              " 'hypocrisy',\n",
              " 'when',\n",
              " 'it',\n",
              " 'comes',\n",
              " 'to',\n",
              " 'trump',\n",
              " 's',\n",
              " 'damning',\n",
              " 'hot',\n",
              " 'mic',\n",
              " 'moment',\n",
              " ',',\n",
              " 'where',\n",
              " 'he',\n",
              " 'admitted',\n",
              " 'to',\n",
              " 'sexually',\n",
              " 'assaulting',\n",
              " 'women',\n",
              " '.',\n",
              " 'he',\n",
              " 'said',\n",
              " ':',\n",
              " 'you',\n",
              " 'know',\n",
              " 'what',\n",
              " 'the',\n",
              " 'magic',\n",
              " 'word',\n",
              " ',',\n",
              " 'the',\n",
              " 'only',\n",
              " 'thing',\n",
              " 'that',\n",
              " 'matters',\n",
              " 'in',\n",
              " 'american',\n",
              " 'sexual',\n",
              " 'mores',\n",
              " 'today',\n",
              " 'is',\n",
              " '?',\n",
              " 'one',\n",
              " 'thing',\n",
              " '.',\n",
              " 'you',\n",
              " 'can',\n",
              " 'do',\n",
              " 'anything',\n",
              " ',',\n",
              " 'the',\n",
              " 'left',\n",
              " 'will',\n",
              " 'promote',\n",
              " 'and',\n",
              " 'understand',\n",
              " 'and',\n",
              " 'tolerate',\n",
              " 'anything',\n",
              " ',',\n",
              " 'as',\n",
              " 'long',\n",
              " 'as',\n",
              " 'there',\n",
              " 'is',\n",
              " 'one',\n",
              " 'element',\n",
              " '.',\n",
              " 'do',\n",
              " 'you',\n",
              " 'know',\n",
              " 'what',\n",
              " 'it',\n",
              " 'is',\n",
              " '?',\n",
              " 'consent',\n",
              " '.',\n",
              " 'if',\n",
              " 'there',\n",
              " 'is',\n",
              " 'consent',\n",
              " 'on',\n",
              " 'both',\n",
              " 'or',\n",
              " 'all',\n",
              " 'three',\n",
              " 'or',\n",
              " 'all',\n",
              " 'four',\n",
              " ',',\n",
              " 'however',\n",
              " 'many',\n",
              " 'are',\n",
              " 'involved',\n",
              " 'in',\n",
              " 'the',\n",
              " 'sex',\n",
              " 'act',\n",
              " ',',\n",
              " 'it',\n",
              " 's',\n",
              " 'perfectly',\n",
              " 'fine',\n",
              " '.',\n",
              " 'whatever',\n",
              " 'it',\n",
              " 'is',\n",
              " '.',\n",
              " 'but',\n",
              " 'if',\n",
              " 'the',\n",
              " 'left',\n",
              " 'ever',\n",
              " 'senses',\n",
              " 'and',\n",
              " 'smells',\n",
              " 'that',\n",
              " 'there',\n",
              " 's',\n",
              " 'no',\n",
              " 'consent',\n",
              " 'in',\n",
              " 'part',\n",
              " 'of',\n",
              " 'the',\n",
              " 'equation',\n",
              " 'then',\n",
              " 'here',\n",
              " 'come',\n",
              " 'the',\n",
              " 'rape',\n",
              " 'police',\n",
              " '.',\n",
              " 'but',\n",
              " 'consent',\n",
              " 'is',\n",
              " 'the',\n",
              " 'magic',\n",
              " 'key',\n",
              " 'to',\n",
              " 'the',\n",
              " 'left',\n",
              " '.',\n",
              " 'listen',\n",
              " 'to',\n",
              " 'this',\n",
              " 'vile',\n",
              " ',',\n",
              " 'pathetic',\n",
              " ',',\n",
              " 'diseased',\n",
              " 'beer',\n",
              " 'turd',\n",
              " 'of',\n",
              " 'a',\n",
              " 'man',\n",
              " 'below',\n",
              " ',',\n",
              " 'because',\n",
              " 'holy',\n",
              " 'shit',\n",
              " ':',\n",
              " 'of',\n",
              " 'course',\n",
              " ',',\n",
              " 'this',\n",
              " 'is',\n",
              " 'also',\n",
              " 'the',\n",
              " 'man',\n",
              " 'that',\n",
              " 'said',\n",
              " 'we',\n",
              " 'would',\n",
              " 'legalize',\n",
              " 'rape',\n",
              " 'if',\n",
              " 'we',\n",
              " 'could',\n",
              " 'because',\n",
              " 'we',\n",
              " 'd',\n",
              " 'just',\n",
              " 'call',\n",
              " 'it',\n",
              " 'the',\n",
              " 'civil',\n",
              " 'rights',\n",
              " 'act',\n",
              " 'of',\n",
              " '2016.',\n",
              " 'that',\n",
              " 's',\n",
              " 'kind',\n",
              " 'of',\n",
              " 'odd',\n",
              " ',',\n",
              " 'though',\n",
              " ',',\n",
              " 'considering',\n",
              " 'limbaugh',\n",
              " 'is',\n",
              " 'now',\n",
              " 'jumping',\n",
              " 'on',\n",
              " 'us',\n",
              " 'for',\n",
              " 'sending',\n",
              " 'out',\n",
              " 'the',\n",
              " 'rape',\n",
              " 'police',\n",
              " 'whenever',\n",
              " 'we',\n",
              " 'see',\n",
              " 'instances',\n",
              " 'of',\n",
              " 'he',\n",
              " 'brought',\n",
              " 'this',\n",
              " 'up',\n",
              " ',',\n",
              " 'he',\n",
              " 'was',\n",
              " 'discussing',\n",
              " 'bathroom',\n",
              " 'bills',\n",
              " 'intended',\n",
              " 'to',\n",
              " 'keep',\n",
              " 'transgender',\n",
              " 'people',\n",
              " 'out',\n",
              " 'of',\n",
              " 'the',\n",
              " 'bathrooms',\n",
              " 'that',\n",
              " 'correspond',\n",
              " 'with',\n",
              " 'their',\n",
              " 'identities',\n",
              " ':',\n",
              " 'now',\n",
              " '[',\n",
              " 'civil',\n",
              " 'rights',\n",
              " ']',\n",
              " 'is',\n",
              " 'being',\n",
              " 'expanded',\n",
              " '.',\n",
              " 'civil',\n",
              " 'rights',\n",
              " ',',\n",
              " 'gay',\n",
              " 'marriage',\n",
              " ';',\n",
              " 'civil',\n",
              " 'rights',\n",
              " ',',\n",
              " 'transgender',\n",
              " 'bathroom',\n",
              " 'rights',\n",
              " ';',\n",
              " 'civil',\n",
              " 'rights',\n",
              " ',',\n",
              " 'whatever',\n",
              " 'the',\n",
              " 'left',\n",
              " 'wants',\n",
              " '.',\n",
              " 'except',\n",
              " 'over',\n",
              " 'here',\n",
              " ',',\n",
              " 'rape',\n",
              " 'has',\n",
              " 'become',\n",
              " 'a',\n",
              " 'safety',\n",
              " 'issue',\n",
              " '.',\n",
              " 'sexual',\n",
              " 'assault',\n",
              " 'is',\n",
              " 'a',\n",
              " 'safety',\n",
              " 'issue',\n",
              " ',',\n",
              " 'but',\n",
              " 'only',\n",
              " 'when',\n",
              " 'we',\n",
              " 're',\n",
              " 'talking',\n",
              " 'about',\n",
              " 'people',\n",
              " 'that',\n",
              " 'limbaugh',\n",
              " 'and',\n",
              " 'his',\n",
              " 'loyal',\n",
              " 'fans',\n",
              " 'hate',\n",
              " '.',\n",
              " 'it',\n",
              " 's',\n",
              " 'just',\n",
              " 'words',\n",
              " 'when',\n",
              " 'donald',\n",
              " 'trump',\n",
              " 'brags',\n",
              " 'about',\n",
              " 'actually',\n",
              " 'doing',\n",
              " 'another',\n",
              " 'way',\n",
              " ',',\n",
              " 'these',\n",
              " 'people',\n",
              " 'are',\n",
              " 'terrified',\n",
              " 'of',\n",
              " 'sexual',\n",
              " 'predators',\n",
              " 'posing',\n",
              " 'as',\n",
              " 'transgender',\n",
              " 'people',\n",
              " 'and',\n",
              " 'invading',\n",
              " 'women',\n",
              " 's',\n",
              " 'public',\n",
              " 'restrooms',\n",
              " ',',\n",
              " 'despite',\n",
              " 'the',\n",
              " 'fact',\n",
              " 'that',\n",
              " 'there',\n",
              " 's',\n",
              " 'no',\n",
              " 'evidence',\n",
              " 'this',\n",
              " 'happens',\n",
              " '.',\n",
              " 'however',\n",
              " ',',\n",
              " 'their',\n",
              " 'lord',\n",
              " 'and',\n",
              " 'savior',\n",
              " ',',\n",
              " 'PAD',\n",
              " 'PAD',\n",
              " 'PAD']"
            ]
          },
          "metadata": {},
          "execution_count": 140
        }
      ],
      "source": [
        "\n",
        "[id2word[int(i)] for i in batch[0][0]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "taooUFelC55W",
        "outputId": "f3f8d368-6ac2-485e-e111-173831763757"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[2.],\n",
              "        [2.],\n",
              "        [3.],\n",
              "        [2.],\n",
              "        [4.],\n",
              "        [2.],\n",
              "        [3.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [5.],\n",
              "        [0.],\n",
              "        [5.],\n",
              "        [5.],\n",
              "        [4.],\n",
              "        [2.],\n",
              "        [5.],\n",
              "        [5.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [4.],\n",
              "        [2.],\n",
              "        [4.],\n",
              "        [5.],\n",
              "        [4.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [4.],\n",
              "        [5.],\n",
              "        [5.],\n",
              "        [2.],\n",
              "        [0.],\n",
              "        [5.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [5.],\n",
              "        [2.],\n",
              "        [4.],\n",
              "        [2.],\n",
              "        [5.],\n",
              "        [0.],\n",
              "        [5.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [5.],\n",
              "        [2.],\n",
              "        [0.],\n",
              "        [5.],\n",
              "        [2.],\n",
              "        [4.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [5.],\n",
              "        [3.],\n",
              "        [2.],\n",
              "        [4.],\n",
              "        [2.],\n",
              "        [1.],\n",
              "        [4.],\n",
              "        [5.]], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 141
        }
      ],
      "source": [
        "batch[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "id": "Lz6jFKhJDCd1"
      },
      "outputs": [],
      "source": [
        "val_dataset = NewsDataset(val_texts, word2id, DEVICE)\n",
        "val_sampler = SequentialSampler(val_dataset)\n",
        "val_iterator = DataLoader(val_dataset, collate_fn = val_dataset.collate_fn, sampler=val_sampler, batch_size=64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_2xdUv5CDM7j",
        "outputId": "c66d7e1c-7744-4be5-a2cd-d3dd449f727a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([64, 500])"
            ]
          },
          "metadata": {},
          "execution_count": 143
        }
      ],
      "source": [
        "test_batch = next(iter(val_iterator))\n",
        "test_batch[0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05j_BqAQyS_J",
        "outputId": "3f8790bb-4951-4278-f876-1a0ac63f6ee7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
          ]
        }
      ],
      "source": [
        "w2v_vectors = gensim.downloader.load('word2vec-google-news-300')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BWiyqtEADSy2",
        "outputId": "c9bd36bb-3c54-484b-c966-c58af1ef1c17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
            "  import sys\n"
          ]
        }
      ],
      "source": [
        "weights = np.zeros((len(word2id), 300))\n",
        "count = 0\n",
        "for word, i in word2id.items():\n",
        "    if word == 'PAD':\n",
        "        continue   \n",
        "    try:\n",
        "        weights[i] = w2v_vectors.wv[word]    \n",
        "    except KeyError:\n",
        "      count += 1\n",
        "      # oov словам сопоставляем случайный вектор\n",
        "      weights[i] = np.random.normal(-0.25,0.25,300)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class C_LSTM(nn.Module):\n",
        "    \n",
        "    def __init__(self, filter_lengths, dropout_layer, vocab_size, embedding_dim, weights, n_filters,\n",
        "                 hidden_dim, output_dim, dropout):\n",
        "        \n",
        "        super().__init__()\n",
        "\n",
        "        self.emb_layer = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.emb_layer.from_pretrained(torch.tensor(weights), freeze=True)\n",
        "        self.filter_lengths = filter_lengths\n",
        "        self.dropout_layer = dropout_layer\n",
        "\n",
        "        self.bigrams = nn.Conv1d(in_channels=embedding_dim, out_channels=n_filters,\n",
        "                                  kernel_size=2)\n",
        "        self.trigrams = nn.Conv1d(in_channels=embedding_dim, out_channels=n_filters,\n",
        "                                    kernel_size=3)\n",
        "        \n",
        "        self.fourgrams = nn.Conv1d(in_channels=embedding_dim, out_channels=n_filters,\n",
        "                                   kernel_size=4)\n",
        "      \n",
        "        self.lstm = nn.LSTM(input_size=n_filters, hidden_size=hidden_dim,\n",
        "                            batch_first=True)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.relu = nn.ReLU()\n",
        "        # self.out = nn.Softmax(dim=1)\n",
        "        self.linear = nn.Linear(in_features=hidden_dim, out_features=output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_emb = self.emb_layer(x)\n",
        "        if self.dropout_layer == 0:\n",
        "          x_emb = self.dropout(x_emb)\n",
        "\n",
        "        x_emb = x_emb.transpose(1,2)\n",
        "\n",
        "        max_len_feat = (500 - max(self.filter_lengths)) + 1\n",
        "        conv = []\n",
        "        for f_len in self.filter_lengths:\n",
        "          if f_len == 2:\n",
        "            f_map = self.bigrams(x_emb)\n",
        "            f_map = self.relu(f_map)\n",
        "            f_map = f_map[:, :, :max_len_feat]\n",
        "            conv.append(f_map)\n",
        "\n",
        "          elif f_len == 3:\n",
        "            f_map = self.trigrams(x_emb)\n",
        "            f_map = self.relu(f_map)\n",
        "            f_map = f_map[:, :, :max_len_feat]\n",
        "            conv.append(f_map)\n",
        "          \n",
        "          else:\n",
        "            f_map = self.fourgrams(x_emb)\n",
        "            f_map = self.relu(f_map)\n",
        "            f_map = f_map[:, :, :max_len_feat]\n",
        "            conv.append(f_map)\n",
        "\n",
        "\n",
        "        if len(self.filter_lengths) != 1:\n",
        "            concat_ngrams = torch.cat(conv, -1)\n",
        "        else:\n",
        "            concat_ngrams = f_map\n",
        "\n",
        "        concat_ngrams = concat_ngrams.transpose(1, 2)\n",
        "        output, (hidden, cell) = self.lstm(concat_ngrams)\n",
        "        if self.dropout_layer == 1:\n",
        "          output = self.dropout(output)\n",
        "        \n",
        "        logits = self.linear(torch.squeeze(hidden, 0))  \n",
        "\n",
        "\n",
        "        return logits"
      ],
      "metadata": {
        "id": "Gr-X8qIAmfCE"
      },
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 181,
      "metadata": {
        "id": "zq18YEGqZoK6"
      },
      "outputs": [],
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "    epoch_loss = 0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for i, (texts, ys) in enumerate(iterator): \n",
        "        optimizer.zero_grad()\n",
        "        preds = model(texts)  \n",
        "        ys = ys.to(torch.int64)\n",
        "        loss = criterion(preds, ys.squeeze())  \n",
        "        loss.backward() \n",
        "        optimizer.step() \n",
        "        epoch_loss += loss.item()\n",
        "        if not (i + 1) % int(len(iterator)/5):\n",
        "            print(f'Train loss: {epoch_loss/i}')      \n",
        "    return  epoch_loss / len(iterator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 182,
      "metadata": {
        "id": "zxBZQreXZsU0"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    epoch_loss = 0\n",
        "    epoch_metric = 0\n",
        "    model.eval() \n",
        "    metric = torchmetrics.Accuracy()\n",
        "    metric.to(DEVICE)\n",
        "    with torch.no_grad():\n",
        "        for i, (texts, ys) in enumerate(iterator):   \n",
        "            preds = model(texts)\n",
        "            # print(preds)\n",
        "            # print(ys)\n",
        "            ys = ys.to(torch.int64)\n",
        "            loss = criterion(preds, ys.squeeze())\n",
        "            epoch_loss += loss.item()\n",
        "            batch_metric = metric(preds, ys.squeeze())\n",
        "            epoch_metric += batch_metric\n",
        "\n",
        "            if not (i + 1) % int(len(iterator)/5):\n",
        "              print(f'Val loss: {epoch_loss/i}, Val accuracy: {epoch_metric/i}')\n",
        "        \n",
        "    return epoch_metric / len(iterator), epoch_loss / len(iterator)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def experiment(filter_lengths, dropout_layer):\n",
        "    \n",
        "    model = C_LSTM(filter_lengths=filter_lengths, dropout_layer=dropout_layer,\n",
        "                   vocab_size=len(word2id), embedding_dim=100, weights=weights,\n",
        "               n_filters=150, hidden_dim=150, output_dim=6, dropout=0.5)\n",
        "\n",
        "    model = model.to(DEVICE)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    criterion = criterion.to(DEVICE)\n",
        "\n",
        "    losses = []\n",
        "    losses_eval = []\n",
        "    accs = []\n",
        "    accs_eval = []\n",
        "\n",
        "    print(\"Пармеметры:\")\n",
        "    print(f'\\n#######\\n\\n длины фильтров: {filter_lengths}')\n",
        "    print(f'\\nСлой, на котором применяется дропаут: {dropout_layer}')\n",
        "    for i in range(5):\n",
        "        print(f'\\nstarting Epoch {i}')\n",
        "        print('Training...')\n",
        "        epoch_loss = train(model, train_iterator, optimizer, criterion)\n",
        "        losses.append(epoch_loss)\n",
        "        print('\\nEvaluating on train...')\n",
        "        acc_on_train,_ = evaluate(model, train_iterator, criterion)\n",
        "        accs.append(acc_on_train)\n",
        "        print('\\nEvaluating on test...')\n",
        "        acc_on_test, epoch_loss_on_test = evaluate(model, val_iterator, criterion)\n",
        "        losses_eval.append(epoch_loss_on_test)\n",
        "        accs_eval.append(acc_on_test)\n"
      ],
      "metadata": {
        "id": "xfCA1cLlPgiR"
      },
      "execution_count": 183,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 0 - делаем дропаут после эмбеддинг слоя\n",
        "# 1 - делаем дропаут перед линейным слоем\n",
        "params = [[[2], 0], [[2], 1], [[3], 0], [[3], 1], [[4], 0], [[4], 1],\n",
        "          [[2, 3], 0], [[2, 3], 1], [[2, 4], 0], [[2, 4], 1],\n",
        "          [[3, 4], 0], [[3, 4], 1], [[2, 3, 4], 0], [[2, 3, 4], 0]] \n",
        "\n",
        "\n",
        "for param in params:\n",
        "    experiment(filter_lengths=param[0], dropout_layer=param[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "elXhwvuCRPxT",
        "outputId": "9ae5e054-f003-49c7-f125-b0a37ac42774"
      },
      "execution_count": 185,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Пармеметры:\n",
            "\n",
            "#######\n",
            "\n",
            " длины фильтров: [2]\n",
            "\n",
            "Слой, на котором применяется дропаут: 0\n",
            "\n",
            "starting Epoch 0\n",
            "Training...\n",
            "Train loss: 1.5069096673998916\n",
            "Train loss: 1.4788073290949282\n",
            "Train loss: 1.4660652503801908\n",
            "Train loss: 1.457874445688157\n",
            "Train loss: 1.454728629762326\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 1.4666895029837625, Val accuracy: 0.38843202590942383\n",
            "Val loss: 1.4386199536530868, Val accuracy: 0.39211955666542053\n",
            "Val loss: 1.4264575780471624, Val accuracy: 0.3978504240512848\n",
            "Val loss: 1.4231054199722422, Val accuracy: 0.39948591589927673\n",
            "Val loss: 1.4256289067152874, Val accuracy: 0.3982482850551605\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.5522707242232103, Val accuracy: 0.4086538553237915\n",
            "Val loss: 1.4855808637760304, Val accuracy: 0.394097238779068\n",
            "Val loss: 1.4542613058555416, Val accuracy: 0.3982469439506531\n",
            "Val loss: 1.4463361349972812, Val accuracy: 0.3911931812763214\n",
            "Val loss: 1.4398125237312869, Val accuracy: 0.38813406229019165\n",
            "\n",
            "starting Epoch 1\n",
            "Training...\n",
            "Train loss: 1.4161044338293243\n",
            "Train loss: 1.3686312644377998\n",
            "Train loss: 1.3172731055000615\n",
            "Train loss: 1.2555449372762209\n",
            "Train loss: 1.2148905905885268\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 1.036157512873934, Val accuracy: 0.625548243522644\n",
            "Val loss: 1.0326903809671817, Val accuracy: 0.6153532266616821\n",
            "Val loss: 1.031406163824776, Val accuracy: 0.6092846393585205\n",
            "Val loss: 1.033387608858414, Val accuracy: 0.6038960814476013\n",
            "Val loss: 1.0324331759169028, Val accuracy: 0.6047253608703613\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.155462654737326, Val accuracy: 0.6237980723381042\n",
            "Val loss: 1.0936439567142062, Val accuracy: 0.5960648059844971\n",
            "Val loss: 1.0609305704512246, Val accuracy: 0.602134108543396\n",
            "Val loss: 1.0574241074648771, Val accuracy: 0.5971590876579285\n",
            "Val loss: 1.0504019554110542, Val accuracy: 0.5978261232376099\n",
            "\n",
            "starting Epoch 2\n",
            "Training...\n",
            "Train loss: 1.0600417078587048\n",
            "Train loss: 1.0458423982495848\n",
            "Train loss: 1.0369899965435094\n",
            "Train loss: 1.0289951423545936\n",
            "Train loss: 1.0255545218923101\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.9844019778987818, Val accuracy: 0.6123903393745422\n",
            "Val loss: 0.9885680328244748, Val accuracy: 0.6024456024169922\n",
            "Val loss: 0.9761164412333098, Val accuracy: 0.6095556020736694\n",
            "Val loss: 0.9771488311486843, Val accuracy: 0.6096455454826355\n",
            "Val loss: 0.9723159726515773, Val accuracy: 0.6115916967391968\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.097176460119394, Val accuracy: 0.625\n",
            "Val loss: 1.0479275186856587, Val accuracy: 0.5995370149612427\n",
            "Val loss: 1.013575125031355, Val accuracy: 0.6051828861236572\n",
            "Val loss: 1.0070363239808515, Val accuracy: 0.6005681753158569\n",
            "Val loss: 1.0004140354584956, Val accuracy: 0.6028079986572266\n",
            "\n",
            "starting Epoch 3\n",
            "Training...\n",
            "Train loss: 1.006338642354597\n",
            "Train loss: 1.0025986039120218\n",
            "Train loss: 0.9959571178937923\n",
            "Train loss: 0.9821991652121276\n",
            "Train loss: 0.9700255612601046\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.9133110516949704, Val accuracy: 0.6433662176132202\n",
            "Val loss: 0.9017111441363459, Val accuracy: 0.631657600402832\n",
            "Val loss: 0.8989901573671771, Val accuracy: 0.6297868490219116\n",
            "Val loss: 0.8967053472221672, Val accuracy: 0.6298701167106628\n",
            "Val loss: 0.8924315019049859, Val accuracy: 0.630136251449585\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.0032946788347685, Val accuracy: 0.645432710647583\n",
            "Val loss: 0.9715073572264777, Val accuracy: 0.6180555820465088\n",
            "Val loss: 0.9437307453737026, Val accuracy: 0.6196646094322205\n",
            "Val loss: 0.9398705179041082, Val accuracy: 0.6144886016845703\n",
            "Val loss: 0.9310920990031698, Val accuracy: 0.6173007488250732\n",
            "\n",
            "starting Epoch 4\n",
            "Training...\n",
            "Train loss: 0.8997966019730819\n",
            "Train loss: 0.8929712943408801\n",
            "Train loss: 0.8877443323245627\n",
            "Train loss: 0.8875975399822383\n",
            "Train loss: 0.8797867545619555\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.8724741203743115, Val accuracy: 0.6529605388641357\n",
            "Val loss: 0.8632113337516785, Val accuracy: 0.647554337978363\n",
            "Val loss: 0.8618459649857758, Val accuracy: 0.6451408863067627\n",
            "Val loss: 0.8663609105271178, Val accuracy: 0.6432629823684692\n",
            "Val loss: 0.8657034608732045, Val accuracy: 0.6438689827919006\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.9809079812123225, Val accuracy: 0.6514423489570618\n",
            "Val loss: 0.948948085308075, Val accuracy: 0.6267361044883728\n",
            "Val loss: 0.9258015301169419, Val accuracy: 0.6261432766914368\n",
            "Val loss: 0.9229965036565607, Val accuracy: 0.6204545497894287\n",
            "Val loss: 0.916544182576995, Val accuracy: 0.6254529356956482\n",
            "Пармеметры:\n",
            "\n",
            "#######\n",
            "\n",
            " длины фильтров: [2]\n",
            "\n",
            "Слой, на котором применяется дропаут: 1\n",
            "\n",
            "starting Epoch 0\n",
            "Training...\n",
            "Train loss: 1.5086615796674763\n",
            "Train loss: 1.4759464481602544\n",
            "Train loss: 1.4636291871870184\n",
            "Train loss: 1.4537178974647027\n",
            "Train loss: 1.432640272440795\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 1.3571846422396208, Val accuracy: 0.5361841917037964\n",
            "Val loss: 1.347195895858433, Val accuracy: 0.527309775352478\n",
            "Val loss: 1.3415772391192486, Val accuracy: 0.5195989608764648\n",
            "Val loss: 1.3307002473187137, Val accuracy: 0.5199540257453918\n",
            "Val loss: 1.3240379486941962, Val accuracy: 0.5201665163040161\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.435923200387221, Val accuracy: 0.546875\n",
            "Val loss: 1.3701976802614, Val accuracy: 0.5289351940155029\n",
            "Val loss: 1.3404424888331716, Val accuracy: 0.5316310524940491\n",
            "Val loss: 1.3342669768766924, Val accuracy: 0.5235795378684998\n",
            "Val loss: 1.326264123985733, Val accuracy: 0.5269474983215332\n",
            "\n",
            "starting Epoch 1\n",
            "Training...\n",
            "Train loss: 1.2701988188843978\n",
            "Train loss: 1.1721429747083913\n",
            "Train loss: 1.1463232812164836\n",
            "Train loss: 1.1199643405484947\n",
            "Train loss: 1.1092943083043743\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.9966872177625957, Val accuracy: 0.6225329041481018\n",
            "Val loss: 1.0014202843541684, Val accuracy: 0.6084238886833191\n",
            "Val loss: 1.000462184751654, Val accuracy: 0.6062138676643372\n",
            "Val loss: 1.0051126880047125, Val accuracy: 0.6026109457015991\n",
            "Val loss: 1.0018968883270212, Val accuracy: 0.6053200960159302\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.1321351436468272, Val accuracy: 0.6165865659713745\n",
            "Val loss: 1.0698882782900776, Val accuracy: 0.5983796119689941\n",
            "Val loss: 1.0387175722820003, Val accuracy: 0.5998475551605225\n",
            "Val loss: 1.0356574459509416, Val accuracy: 0.5914772748947144\n",
            "Val loss: 1.0284532965093418, Val accuracy: 0.5964673757553101\n",
            "\n",
            "starting Epoch 2\n",
            "Training...\n",
            "Train loss: 1.008296039020806\n",
            "Train loss: 0.982132407375004\n",
            "Train loss: 0.9815626161635955\n",
            "Train loss: 0.979214317354805\n",
            "Train loss: 0.9755967139785265\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.9722058082881727, Val accuracy: 0.6376096606254578\n",
            "Val loss: 0.9511106527369956, Val accuracy: 0.6383152008056641\n",
            "Val loss: 0.9428129223729834, Val accuracy: 0.6402637362480164\n",
            "Val loss: 0.9427336170559838, Val accuracy: 0.638798713684082\n",
            "Val loss: 0.9421501009109523, Val accuracy: 0.6384623646736145\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.0808265163348272, Val accuracy: 0.6442307829856873\n",
            "Val loss: 1.0178398158815172, Val accuracy: 0.6267361044883728\n",
            "Val loss: 0.9924209335955178, Val accuracy: 0.6269054412841797\n",
            "Val loss: 0.9863962986252525, Val accuracy: 0.6227272748947144\n",
            "Val loss: 0.9800489475761635, Val accuracy: 0.6259058117866516\n",
            "\n",
            "starting Epoch 3\n",
            "Training...\n",
            "Train loss: 0.9303999480448271\n",
            "Train loss: 0.9311542863431184\n",
            "Train loss: 0.9265774977689534\n",
            "Train loss: 0.9283004348928278\n",
            "Train loss: 0.9202569304453048\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.8806597193082174, Val accuracy: 0.6784539818763733\n",
            "Val loss: 0.8672160920889481, Val accuracy: 0.6657608151435852\n",
            "Val loss: 0.8704350049785107, Val accuracy: 0.6601336598396301\n",
            "Val loss: 0.8705508982464348, Val accuracy: 0.6591585278511047\n",
            "Val loss: 0.8707338352929349, Val accuracy: 0.6578720211982727\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.0372438980982854, Val accuracy: 0.6526442766189575\n",
            "Val loss: 0.9758784528131839, Val accuracy: 0.640625\n",
            "Val loss: 0.9522254975830636, Val accuracy: 0.6398627758026123\n",
            "Val loss: 0.9451598286628723, Val accuracy: 0.6355113387107849\n",
            "Val loss: 0.9373463487279587, Val accuracy: 0.6379076242446899\n",
            "\n",
            "starting Epoch 4\n",
            "Training...\n",
            "Train loss: 0.9020716025118243\n",
            "Train loss: 0.8835526575212893\n",
            "Train loss: 0.8843079495292179\n",
            "Train loss: 0.8794945481019619\n",
            "Train loss: 0.8761856489940498\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.8921982112683748, Val accuracy: 0.6592653393745422\n",
            "Val loss: 0.8716397798579673, Val accuracy: 0.6607336401939392\n",
            "Val loss: 0.8598422125584817, Val accuracy: 0.6611271500587463\n",
            "Val loss: 0.8536727621957854, Val accuracy: 0.6624729633331299\n",
            "Val loss: 0.8513682162885435, Val accuracy: 0.66181880235672\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.037975902740772, Val accuracy: 0.6598557829856873\n",
            "Val loss: 0.9670098070745115, Val accuracy: 0.647569477558136\n",
            "Val loss: 0.9502409565739516, Val accuracy: 0.6413871645927429\n",
            "Val loss: 0.9422333348881115, Val accuracy: 0.637499988079071\n",
            "Val loss: 0.9371289766353109, Val accuracy: 0.6390398740768433\n",
            "Пармеметры:\n",
            "\n",
            "#######\n",
            "\n",
            " длины фильтров: [3]\n",
            "\n",
            "Слой, на котором применяется дропаут: 0\n",
            "\n",
            "starting Epoch 0\n",
            "Training...\n",
            "Train loss: 1.5036103997314185\n",
            "Train loss: 1.4849744506504226\n",
            "Train loss: 1.4721755692035476\n",
            "Train loss: 1.459151206594525\n",
            "Train loss: 1.4459726315468653\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 1.4037967920303345, Val accuracy: 0.5169956088066101\n",
            "Val loss: 1.3904200999633125, Val accuracy: 0.5173912644386292\n",
            "Val loss: 1.3835116328531607, Val accuracy: 0.5177022814750671\n",
            "Val loss: 1.3825152008048385, Val accuracy: 0.5175865888595581\n",
            "Val loss: 1.3795776606431056, Val accuracy: 0.5171388387680054\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.5041710413419282, Val accuracy: 0.5420673489570618\n",
            "Val loss: 1.4418581591712103, Val accuracy: 0.5219907164573669\n",
            "Val loss: 1.408508076900389, Val accuracy: 0.5259146094322205\n",
            "Val loss: 1.4009020285172895, Val accuracy: 0.5184658765792847\n",
            "Val loss: 1.393619177997976, Val accuracy: 0.5197011232376099\n",
            "\n",
            "starting Epoch 1\n",
            "Training...\n",
            "Train loss: 1.3986240039791977\n",
            "Train loss: 1.3570625274077706\n",
            "Train loss: 1.3064500451777024\n",
            "Train loss: 1.2569864020719157\n",
            "Train loss: 1.2162279546467079\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 1.042558723374417, Val accuracy: 0.6239035129547119\n",
            "Val loss: 1.0307563517404639, Val accuracy: 0.6135869026184082\n",
            "Val loss: 1.0289474133122174, Val accuracy: 0.6096459627151489\n",
            "Val loss: 1.0245258730727356, Val accuracy: 0.6099161505699158\n",
            "Val loss: 1.029948214964883, Val accuracy: 0.605914831161499\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.1571560685451214, Val accuracy: 0.6201923489570618\n",
            "Val loss: 1.0991407566600375, Val accuracy: 0.5954861044883728\n",
            "Val loss: 1.0655490159988403, Val accuracy: 0.6032774448394775\n",
            "Val loss: 1.061706291545521, Val accuracy: 0.5977272391319275\n",
            "Val loss: 1.0546942353248596, Val accuracy: 0.5987318754196167\n",
            "\n",
            "starting Epoch 2\n",
            "Training...\n",
            "Train loss: 1.0714553354079264\n",
            "Train loss: 1.0675406466359678\n",
            "Train loss: 1.0534273365329456\n",
            "Train loss: 1.0512160248570628\n",
            "Train loss: 1.0415169720418724\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 1.0123806595802307, Val accuracy: 0.6181469559669495\n",
            "Val loss: 1.0064116851143214, Val accuracy: 0.615217387676239\n",
            "Val loss: 0.998838703411852, Val accuracy: 0.6147037148475647\n",
            "Val loss: 1.0050360462366243, Val accuracy: 0.6099837422370911\n",
            "Val loss: 1.0019800747554608, Val accuracy: 0.6085099577903748\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.13380956191283, Val accuracy: 0.6237980723381042\n",
            "Val loss: 1.073896050453186, Val accuracy: 0.5966435074806213\n",
            "Val loss: 1.040818715967783, Val accuracy: 0.6025152206420898\n",
            "Val loss: 1.0343016862869263, Val accuracy: 0.5980113744735718\n",
            "Val loss: 1.027341191319452, Val accuracy: 0.6003170609474182\n",
            "\n",
            "starting Epoch 3\n",
            "Training...\n",
            "Train loss: 1.0370057423909504\n",
            "Train loss: 1.0259418513463892\n",
            "Train loss: 1.03022397460276\n",
            "Train loss: 1.0305372154041803\n",
            "Train loss: 1.025163322171538\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 1.0204918133585077, Val accuracy: 0.6170504689216614\n",
            "Val loss: 1.0095640514207922, Val accuracy: 0.6142662763595581\n",
            "Val loss: 1.0030180033231746, Val accuracy: 0.6144328117370605\n",
            "Val loss: 1.0011194611524608, Val accuracy: 0.612554132938385\n",
            "Val loss: 1.0029290646417743, Val accuracy: 0.6100237965583801\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.1518145570388207, Val accuracy: 0.6117788553237915\n",
            "Val loss: 1.081691304842631, Val accuracy: 0.5943287014961243\n",
            "Val loss: 1.0485506813700607, Val accuracy: 0.602134108543396\n",
            "Val loss: 1.0410657741806724, Val accuracy: 0.5980113744735718\n",
            "Val loss: 1.0345462983933047, Val accuracy: 0.5991848111152649\n",
            "\n",
            "starting Epoch 4\n",
            "Training...\n",
            "Train loss: 1.0210656596903216\n",
            "Train loss: 1.021590649045032\n",
            "Train loss: 1.0126356258557712\n",
            "Train loss: 1.0078352181426375\n",
            "Train loss: 1.0032507331726048\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.9877107551223353, Val accuracy: 0.6373355388641357\n",
            "Val loss: 0.9724400712096173, Val accuracy: 0.6346467137336731\n",
            "Val loss: 0.9696435053224508, Val accuracy: 0.6289739608764648\n",
            "Val loss: 0.9683327878708448, Val accuracy: 0.6277056336402893\n",
            "Val loss: 0.9687566363275257, Val accuracy: 0.6270004510879517\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.1135602639271662, Val accuracy: 0.6334134936332703\n",
            "Val loss: 1.051759174576512, Val accuracy: 0.6059027910232544\n",
            "Val loss: 1.0196016401779362, Val accuracy: 0.6101371645927429\n",
            "Val loss: 1.0122729128057306, Val accuracy: 0.6085227131843567\n",
            "Val loss: 1.004525215729423, Val accuracy: 0.6129981875419617\n",
            "Пармеметры:\n",
            "\n",
            "#######\n",
            "\n",
            " длины фильтров: [3]\n",
            "\n",
            "Слой, на котором применяется дропаут: 1\n",
            "\n",
            "starting Epoch 0\n",
            "Training...\n",
            "Train loss: 1.5095067358853524\n",
            "Train loss: 1.4708061726196953\n",
            "Train loss: 1.4617657103290447\n",
            "Train loss: 1.4356184475349658\n",
            "Train loss: 1.3876797084577355\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 1.087854708495893, Val accuracy: 0.6080043911933899\n",
            "Val loss: 1.087210186149763, Val accuracy: 0.593478262424469\n",
            "Val loss: 1.0793030448042589, Val accuracy: 0.5929371118545532\n",
            "Val loss: 1.0766643507656082, Val accuracy: 0.5916531085968018\n",
            "Val loss: 1.0765542255965896, Val accuracy: 0.5885596871376038\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.2202285115535443, Val accuracy: 0.6009615659713745\n",
            "Val loss: 1.1499615201243647, Val accuracy: 0.5787037014961243\n",
            "Val loss: 1.115484026873984, Val accuracy: 0.5838414430618286\n",
            "Val loss: 1.1074910619042135, Val accuracy: 0.5803977251052856\n",
            "Val loss: 1.1008815238441245, Val accuracy: 0.5835598111152649\n",
            "\n",
            "starting Epoch 1\n",
            "Training...\n",
            "Train loss: 1.1226755589769597\n",
            "Train loss: 1.0710799020269643\n",
            "Train loss: 1.0478662669314125\n",
            "Train loss: 1.036923508365433\n",
            "Train loss: 1.02478468727488\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.9535183843813444, Val accuracy: 0.6365131735801697\n",
            "Val loss: 0.9481921315193176, Val accuracy: 0.6283966898918152\n",
            "Val loss: 0.9479057678597511, Val accuracy: 0.6259934902191162\n",
            "Val loss: 0.9453036093608641, Val accuracy: 0.6274350881576538\n",
            "Val loss: 0.9464817997906034, Val accuracy: 0.6251622438430786\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.1054862233308644, Val accuracy: 0.6165865659713745\n",
            "Val loss: 1.0293916530079312, Val accuracy: 0.6087962985038757\n",
            "Val loss: 1.0033099462346333, Val accuracy: 0.6097560524940491\n",
            "Val loss: 1.0014796300367876, Val accuracy: 0.6042613387107849\n",
            "Val loss: 0.9957221452740656, Val accuracy: 0.608016312122345\n",
            "\n",
            "starting Epoch 2\n",
            "Training...\n",
            "Train loss: 0.9579353416175173\n",
            "Train loss: 0.9502630700235781\n",
            "Train loss: 0.9375131777945281\n",
            "Train loss: 0.9294362963536085\n",
            "Train loss: 0.9136551305084493\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.8135824224405122, Val accuracy: 0.6707785129547119\n",
            "Val loss: 0.8037552543308424, Val accuracy: 0.6641303896903992\n",
            "Val loss: 0.7961097900578052, Val accuracy: 0.6633850932121277\n",
            "Val loss: 0.794659190621727, Val accuracy: 0.6630140542984009\n",
            "Val loss: 0.7911887430814724, Val accuracy: 0.665333092212677\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.960887060715602, Val accuracy: 0.6742788553237915\n",
            "Val loss: 0.9164125190840827, Val accuracy: 0.6527777910232544\n",
            "Val loss: 0.8920328660709101, Val accuracy: 0.6532012224197388\n",
            "Val loss: 0.8873544053597884, Val accuracy: 0.6457386016845703\n",
            "Val loss: 0.8769352246021879, Val accuracy: 0.64673912525177\n",
            "\n",
            "starting Epoch 3\n",
            "Training...\n",
            "Train loss: 0.8143915408536008\n",
            "Train loss: 0.8140638398087543\n",
            "Train loss: 0.802684549651394\n",
            "Train loss: 0.794482538710425\n",
            "Train loss: 0.7896766943090102\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.7453906849810952, Val accuracy: 0.6850329041481018\n",
            "Val loss: 0.7404894372691279, Val accuracy: 0.678260862827301\n",
            "Val loss: 0.7340021123086786, Val accuracy: 0.6788294911384583\n",
            "Val loss: 0.7347032639371368, Val accuracy: 0.6757981777191162\n",
            "Val loss: 0.7277105085577519, Val accuracy: 0.680092990398407\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.9375652579160837, Val accuracy: 0.6778846383094788\n",
            "Val loss: 0.8877532261389273, Val accuracy: 0.6585648059844971\n",
            "Val loss: 0.8632104731187588, Val accuracy: 0.6585365533828735\n",
            "Val loss: 0.8648761088197882, Val accuracy: 0.6514204144477844\n",
            "Val loss: 0.8572711892749952, Val accuracy: 0.6551177501678467\n",
            "\n",
            "starting Epoch 4\n",
            "Training...\n",
            "Train loss: 0.7267892799879375\n",
            "Train loss: 0.7182919903941777\n",
            "Train loss: 0.7168434510341269\n",
            "Train loss: 0.7175334403009126\n",
            "Train loss: 0.720714365012918\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.7314054903231169, Val accuracy: 0.6918859481811523\n",
            "Val loss: 0.7261224860730379, Val accuracy: 0.6902173757553101\n",
            "Val loss: 0.7202574433274351, Val accuracy: 0.6861451864242554\n",
            "Val loss: 0.7227416555860858, Val accuracy: 0.6853355169296265\n",
            "Val loss: 0.7197328155222236, Val accuracy: 0.6867971420288086\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.9329109008495624, Val accuracy: 0.6875\n",
            "Val loss: 0.8975681772938481, Val accuracy: 0.6608796119689941\n",
            "Val loss: 0.8629187040212678, Val accuracy: 0.6612042188644409\n",
            "Val loss: 0.860571516643871, Val accuracy: 0.6553977131843567\n",
            "Val loss: 0.8475355229515961, Val accuracy: 0.6594203114509583\n",
            "Пармеметры:\n",
            "\n",
            "#######\n",
            "\n",
            " длины фильтров: [4]\n",
            "\n",
            "Слой, на котором применяется дропаут: 0\n",
            "\n",
            "starting Epoch 0\n",
            "Training...\n",
            "Train loss: 1.4937769216403627\n",
            "Train loss: 1.4694083016851673\n",
            "Train loss: 1.4576404287635936\n",
            "Train loss: 1.4306540127956506\n",
            "Train loss: 1.3816473997587977\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 1.1170314937307124, Val accuracy: 0.5718201994895935\n",
            "Val loss: 1.0947526257971059, Val accuracy: 0.572554349899292\n",
            "Val loss: 1.0807876531788378, Val accuracy: 0.5785765647888184\n",
            "Val loss: 1.0842250206253745, Val accuracy: 0.5744724273681641\n",
            "Val loss: 1.0783452731927785, Val accuracy: 0.5774221420288086\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.2300040125846863, Val accuracy: 0.5709134936332703\n",
            "Val loss: 1.1578491021085668, Val accuracy: 0.5601851940155029\n",
            "Val loss: 1.1175237079946005, Val accuracy: 0.5697408318519592\n",
            "Val loss: 1.1127225052226672, Val accuracy: 0.5627840757369995\n",
            "Val loss: 1.1029689735260562, Val accuracy: 0.5692934989929199\n",
            "\n",
            "starting Epoch 1\n",
            "Training...\n",
            "Train loss: 1.120657083235289\n",
            "Train loss: 1.0872434833775395\n",
            "Train loss: 1.0728889741649517\n",
            "Train loss: 1.0655321127924569\n",
            "Train loss: 1.0618911039045524\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 1.0262507921771, Val accuracy: 0.6200658082962036\n",
            "Val loss: 1.0131422296814296, Val accuracy: 0.620516300201416\n",
            "Val loss: 1.0140944471249003, Val accuracy: 0.6130779981613159\n",
            "Val loss: 1.012576042831718, Val accuracy: 0.6111336350440979\n",
            "Val loss: 1.0112971496417036, Val accuracy: 0.6079692840576172\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.148306287251986, Val accuracy: 0.6201923489570618\n",
            "Val loss: 1.0829283837918882, Val accuracy: 0.5960648059844971\n",
            "Val loss: 1.0480784657524853, Val accuracy: 0.6028963327407837\n",
            "Val loss: 1.0413009936159308, Val accuracy: 0.5988636016845703\n",
            "Val loss: 1.0363741776217585, Val accuracy: 0.6005434989929199\n",
            "\n",
            "starting Epoch 2\n",
            "Training...\n",
            "Train loss: 1.0512048413878994\n",
            "Train loss: 1.032387379978014\n",
            "Train loss: 1.028479313230239\n",
            "Train loss: 1.022979613248404\n",
            "Train loss: 1.0197328395084526\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 1.016788309080559, Val accuracy: 0.6101973652839661\n",
            "Val loss: 0.9878782204959704, Val accuracy: 0.6137228012084961\n",
            "Val loss: 0.9816155557687571, Val accuracy: 0.6173229813575745\n",
            "Val loss: 0.9830804229814769, Val accuracy: 0.614989161491394\n",
            "Val loss: 0.9784894818665659, Val accuracy: 0.6166738867759705\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.1093549453295195, Val accuracy: 0.6225962042808533\n",
            "Val loss: 1.050547911061181, Val accuracy: 0.5983796119689941\n",
            "Val loss: 1.018961942777401, Val accuracy: 0.6059451103210449\n",
            "Val loss: 1.0157658457756042, Val accuracy: 0.6002840995788574\n",
            "Val loss: 1.007991116979848, Val accuracy: 0.6037137508392334\n",
            "\n",
            "starting Epoch 3\n",
            "Training...\n",
            "Train loss: 0.9906998423107883\n",
            "Train loss: 0.9834499949994294\n",
            "Train loss: 0.9826260167739295\n",
            "Train loss: 0.9780232697338253\n",
            "Train loss: 0.9722726334750034\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.9794652765257317, Val accuracy: 0.6356908082962036\n",
            "Val loss: 0.9546658666237541, Val accuracy: 0.6320651769638062\n",
            "Val loss: 0.9461405115320504, Val accuracy: 0.6295158863067627\n",
            "Val loss: 0.9423981714042234, Val accuracy: 0.6291937232017517\n",
            "Val loss: 0.9416279048243196, Val accuracy: 0.6283521056175232\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.0899591629321759, Val accuracy: 0.6201923489570618\n",
            "Val loss: 1.028860248901226, Val accuracy: 0.5995370149612427\n",
            "Val loss: 0.9956217655321447, Val accuracy: 0.609375\n",
            "Val loss: 0.9926760922778737, Val accuracy: 0.6048295497894287\n",
            "Val loss: 0.9863350866497427, Val accuracy: 0.6102808117866516\n",
            "\n",
            "starting Epoch 4\n",
            "Training...\n",
            "Train loss: 0.9569020198102582\n",
            "Train loss: 0.938136460988418\n",
            "Train loss: 0.9181753900009773\n",
            "Train loss: 0.9064760311341389\n",
            "Train loss: 0.8991430086248061\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.8129968371307641, Val accuracy: 0.6784539818763733\n",
            "Val loss: 0.8141900010730909, Val accuracy: 0.6638586521148682\n",
            "Val loss: 0.814697437548224, Val accuracy: 0.6625722646713257\n",
            "Val loss: 0.8163375660970613, Val accuracy: 0.6598349809646606\n",
            "Val loss: 0.8110725293934964, Val accuracy: 0.660575270652771\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.9334820050459641, Val accuracy: 0.6646634936332703\n",
            "Val loss: 0.9047697959122835, Val accuracy: 0.640625\n",
            "Val loss: 0.8813758826837307, Val accuracy: 0.6455792188644409\n",
            "Val loss: 0.8782587300647389, Val accuracy: 0.6414772272109985\n",
            "Val loss: 0.8701651234557664, Val accuracy: 0.6444746255874634\n",
            "Пармеметры:\n",
            "\n",
            "#######\n",
            "\n",
            " длины фильтров: [4]\n",
            "\n",
            "Слой, на котором применяется дропаут: 1\n",
            "\n",
            "starting Epoch 0\n",
            "Training...\n",
            "Train loss: 1.4985655901724833\n",
            "Train loss: 1.4748108542483787\n",
            "Train loss: 1.4609264397207713\n",
            "Train loss: 1.4463206400602926\n",
            "Train loss: 1.3893113767399508\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 1.0888103257145798, Val accuracy: 0.6118420958518982\n",
            "Val loss: 1.06079915502797, Val accuracy: 0.6088314652442932\n",
            "Val loss: 1.0626844250397876, Val accuracy: 0.6036849617958069\n",
            "Val loss: 1.065765642500543, Val accuracy: 0.6009199023246765\n",
            "Val loss: 1.063355077302992, Val accuracy: 0.6014814376831055\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.1924125598027155, Val accuracy: 0.6201923489570618\n",
            "Val loss: 1.1306470654628895, Val accuracy: 0.5925925970077515\n",
            "Val loss: 1.0971708414031238, Val accuracy: 0.5998475551605225\n",
            "Val loss: 1.0925899256359448, Val accuracy: 0.594318151473999\n",
            "Val loss: 1.086175649062447, Val accuracy: 0.59714674949646\n",
            "\n",
            "starting Epoch 1\n",
            "Training...\n",
            "Train loss: 1.0403978155370344\n",
            "Train loss: 1.0209283294885054\n",
            "Train loss: 1.0024155436912714\n",
            "Train loss: 0.9957363935776087\n",
            "Train loss: 0.9830991378292493\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.8922082593566493, Val accuracy: 0.6507675647735596\n",
            "Val loss: 0.8809794809507288, Val accuracy: 0.6448369026184082\n",
            "Val loss: 0.8780357379444762, Val accuracy: 0.638818621635437\n",
            "Val loss: 0.873526183299688, Val accuracy: 0.6394751071929932\n",
            "Val loss: 0.8719716383511632, Val accuracy: 0.6407331228256226\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.032009482383728, Val accuracy: 0.6382212042808533\n",
            "Val loss: 0.9825281854029055, Val accuracy: 0.6116898059844971\n",
            "Val loss: 0.9509123243936678, Val accuracy: 0.6227133870124817\n",
            "Val loss: 0.9460838036103683, Val accuracy: 0.6198863387107849\n",
            "Val loss: 0.9407137019046838, Val accuracy: 0.6204710006713867\n",
            "\n",
            "starting Epoch 2\n",
            "Training...\n",
            "Train loss: 0.9248271845934684\n",
            "Train loss: 0.9004278442134028\n",
            "Train loss: 0.8878639498887034\n",
            "Train loss: 0.8734198168758706\n",
            "Train loss: 0.8556307739864996\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.7598798808298612, Val accuracy: 0.6828399300575256\n",
            "Val loss: 0.7474684549414593, Val accuracy: 0.6792119145393372\n",
            "Val loss: 0.7498052251132238, Val accuracy: 0.6748554706573486\n",
            "Val loss: 0.7476862047141765, Val accuracy: 0.6766098737716675\n",
            "Val loss: 0.7471507047286908, Val accuracy: 0.6779303550720215\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.9256907609792856, Val accuracy: 0.6742788553237915\n",
            "Val loss: 0.8803061423478303, Val accuracy: 0.6533564925193787\n",
            "Val loss: 0.855451704525366, Val accuracy: 0.6554877758026123\n",
            "Val loss: 0.8545892379500649, Val accuracy: 0.6499999761581421\n",
            "Val loss: 0.8454416592915853, Val accuracy: 0.6524003744125366\n",
            "\n",
            "starting Epoch 3\n",
            "Training...\n",
            "Train loss: 0.7746541479177642\n",
            "Train loss: 0.7565029735150545\n",
            "Train loss: 0.7517417183501183\n",
            "Train loss: 0.7537729830452891\n",
            "Train loss: 0.7495395010111654\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.673457498090309, Val accuracy: 0.7053179740905762\n",
            "Val loss: 0.6669198170952175, Val accuracy: 0.7029891014099121\n",
            "Val loss: 0.6671638363014066, Val accuracy: 0.7002348303794861\n",
            "Val loss: 0.6696803625269886, Val accuracy: 0.697510838508606\n",
            "Val loss: 0.6693727284360509, Val accuracy: 0.6968533992767334\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.8536094885606033, Val accuracy: 0.6935096383094788\n",
            "Val loss: 0.8196622044951828, Val accuracy: 0.6736111044883728\n",
            "Val loss: 0.8077653399327906, Val accuracy: 0.6707316637039185\n",
            "Val loss: 0.8039858644658869, Val accuracy: 0.6656249761581421\n",
            "Val loss: 0.8001243428907533, Val accuracy: 0.6650815606117249\n",
            "\n",
            "starting Epoch 4\n",
            "Training...\n",
            "Train loss: 0.687372042421709\n",
            "Train loss: 0.6863244380639947\n",
            "Train loss: 0.6809880352778241\n",
            "Train loss: 0.6757905074270256\n",
            "Train loss: 0.6761935294499447\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.6353258924526081, Val accuracy: 0.7146381735801697\n",
            "Val loss: 0.6272649941237076, Val accuracy: 0.7130434513092041\n",
            "Val loss: 0.6255363398204649, Val accuracy: 0.7066473960876465\n",
            "Val loss: 0.6186416645844778, Val accuracy: 0.7074540257453918\n",
            "Val loss: 0.6196731218623455, Val accuracy: 0.7065311670303345\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.7951678404441247, Val accuracy: 0.7115384936332703\n",
            "Val loss: 0.7872951759232415, Val accuracy: 0.678819477558136\n",
            "Val loss: 0.7825640236459127, Val accuracy: 0.6730182766914368\n",
            "Val loss: 0.790939969366247, Val accuracy: 0.6673295497894287\n",
            "Val loss: 0.7922672733016636, Val accuracy: 0.6675724983215332\n",
            "Пармеметры:\n",
            "\n",
            "#######\n",
            "\n",
            " длины фильтров: [2, 3]\n",
            "\n",
            "Слой, на котором применяется дропаут: 0\n",
            "\n",
            "starting Epoch 0\n",
            "Training...\n",
            "Train loss: 1.5087197851716427\n",
            "Train loss: 1.4763192964636762\n",
            "Train loss: 1.430032725968113\n",
            "Train loss: 1.3550065810030156\n",
            "Train loss: 1.29861321663774\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 1.0225702649668644, Val accuracy: 0.6288377046585083\n",
            "Val loss: 1.034487661589747, Val accuracy: 0.6081521511077881\n",
            "Val loss: 1.0410365758603708, Val accuracy: 0.6032333970069885\n",
            "Val loss: 1.0446588944047044, Val accuracy: 0.600581705570221\n",
            "Val loss: 1.0455774355512177, Val accuracy: 0.5992106795310974\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.162589050256289, Val accuracy: 0.618990421295166\n",
            "Val loss: 1.096261238610303, Val accuracy: 0.5925925970077515\n",
            "Val loss: 1.066482160149551, Val accuracy: 0.597942054271698\n",
            "Val loss: 1.0639255415309559, Val accuracy: 0.5928977131843567\n",
            "Val loss: 1.0600038956904756, Val accuracy: 0.5942029356956482\n",
            "\n",
            "starting Epoch 1\n",
            "Training...\n",
            "Train loss: 1.074601551942658\n",
            "Train loss: 1.0540736094765042\n",
            "Train loss: 1.0377715336794109\n",
            "Train loss: 1.028511376349957\n",
            "Train loss: 1.0115753746362706\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.9458087107591462, Val accuracy: 0.6365131735801697\n",
            "Val loss: 0.9299466651418935, Val accuracy: 0.6317934393882751\n",
            "Val loss: 0.9368030084350895, Val accuracy: 0.6225613951683044\n",
            "Val loss: 0.9332545588026832, Val accuracy: 0.6201298832893372\n",
            "Val loss: 0.9309357681076419, Val accuracy: 0.6212694644927979\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.0609801228229816, Val accuracy: 0.6274038553237915\n",
            "Val loss: 1.0063462522294786, Val accuracy: 0.6087962985038757\n",
            "Val loss: 0.9760956749683474, Val accuracy: 0.6150914430618286\n",
            "Val loss: 0.9743596878918734, Val accuracy: 0.609943151473999\n",
            "Val loss: 0.9664132145867832, Val accuracy: 0.6114130616188049\n",
            "\n",
            "starting Epoch 2\n",
            "Training...\n",
            "Train loss: 0.9786769935959264\n",
            "Train loss: 0.9579239456549935\n",
            "Train loss: 0.9521146547587621\n",
            "Train loss: 0.950486741540752\n",
            "Train loss: 0.9447984206635234\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.9176114504797417, Val accuracy: 0.6387061476707458\n",
            "Val loss: 0.9233231865841409, Val accuracy: 0.6277173757553101\n",
            "Val loss: 0.9239850199291472, Val accuracy: 0.6215679049491882\n",
            "Val loss: 0.9184549332181097, Val accuracy: 0.6211444735527039\n",
            "Val loss: 0.9203157086685867, Val accuracy: 0.6182958483695984\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.0481877097716699, Val accuracy: 0.6322115659713745\n",
            "Val loss: 0.9950162061938533, Val accuracy: 0.6087962985038757\n",
            "Val loss: 0.9681888309920706, Val accuracy: 0.6074694991111755\n",
            "Val loss: 0.9634339863603766, Val accuracy: 0.6034090518951416\n",
            "Val loss: 0.9555050030998562, Val accuracy: 0.6068840622901917\n",
            "\n",
            "starting Epoch 3\n",
            "Training...\n",
            "Train loss: 0.9256408653761211\n",
            "Train loss: 0.9349983376005422\n",
            "Train loss: 0.9276522311861115\n",
            "Train loss: 0.9198550940592052\n",
            "Train loss: 0.9157275237839114\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.9066098060524255, Val accuracy: 0.640076756477356\n",
            "Val loss: 0.889496706361356, Val accuracy: 0.6388586759567261\n",
            "Val loss: 0.8857584368286794, Val accuracy: 0.6366509795188904\n",
            "Val loss: 0.8812211429401909, Val accuracy: 0.6370400190353394\n",
            "Val loss: 0.8797223339031311, Val accuracy: 0.6373810768127441\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.005588554419004, Val accuracy: 0.640625\n",
            "Val loss: 0.9663348153785423, Val accuracy: 0.6140046119689941\n",
            "Val loss: 0.9381184752394514, Val accuracy: 0.6181402206420898\n",
            "Val loss: 0.93152968341654, Val accuracy: 0.6144886016845703\n",
            "Val loss: 0.9229638498762379, Val accuracy: 0.61888587474823\n",
            "\n",
            "starting Epoch 4\n",
            "Training...\n",
            "Train loss: 0.9069799268454836\n",
            "Train loss: 0.8966232227242511\n",
            "Train loss: 0.8882062007926103\n",
            "Train loss: 0.8825894186507056\n",
            "Train loss: 0.8862013942642608\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.8471173194416782, Val accuracy: 0.6521381735801697\n",
            "Val loss: 0.8459511534027432, Val accuracy: 0.6426630020141602\n",
            "Val loss: 0.8433720424685175, Val accuracy: 0.6407153010368347\n",
            "Val loss: 0.8351570633582739, Val accuracy: 0.6432629823684692\n",
            "Val loss: 0.8392489725743199, Val accuracy: 0.6403546929359436\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.9724970643336956, Val accuracy: 0.6538462042808533\n",
            "Val loss: 0.9347147433846085, Val accuracy: 0.6290509104728699\n",
            "Val loss: 0.9047418105893019, Val accuracy: 0.6326219439506531\n",
            "Val loss: 0.9005861412395131, Val accuracy: 0.6264204382896423\n",
            "Val loss: 0.8909171405045883, Val accuracy: 0.6286231875419617\n",
            "Пармеметры:\n",
            "\n",
            "#######\n",
            "\n",
            " длины фильтров: [2, 3]\n",
            "\n",
            "Слой, на котором применяется дропаут: 1\n",
            "\n",
            "starting Epoch 0\n",
            "Training...\n",
            "Train loss: 1.4764624248471176\n",
            "Train loss: 1.4299257423566736\n",
            "Train loss: 1.3526288295067803\n",
            "Train loss: 1.2906404821387618\n",
            "Train loss: 1.2463928939561943\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 1.0174452798408375, Val accuracy: 0.6214364171028137\n",
            "Val loss: 1.024644894703575, Val accuracy: 0.6063858270645142\n",
            "Val loss: 1.0268073333481145, Val accuracy: 0.606033205986023\n",
            "Val loss: 1.0177791451478932, Val accuracy: 0.6091044545173645\n",
            "Val loss: 1.018749905086306, Val accuracy: 0.6056985259056091\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.1590232161375194, Val accuracy: 0.6201923489570618\n",
            "Val loss: 1.0988162049540766, Val accuracy: 0.5949074029922485\n",
            "Val loss: 1.0663453776661942, Val accuracy: 0.6002286076545715\n",
            "Val loss: 1.059615136276592, Val accuracy: 0.5963068008422852\n",
            "Val loss: 1.053381591603376, Val accuracy: 0.5978261232376099\n",
            "\n",
            "starting Epoch 1\n",
            "Training...\n",
            "Train loss: 1.0390425585863883\n",
            "Train loss: 1.0330802207407743\n",
            "Train loss: 1.0087909681259553\n",
            "Train loss: 1.0094835012506096\n",
            "Train loss: 1.0090125044324407\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 1.0167281962277597, Val accuracy: 0.6118420958518982\n",
            "Val loss: 0.993277386997057, Val accuracy: 0.6141303777694702\n",
            "Val loss: 0.9832625247839558, Val accuracy: 0.6122651696205139\n",
            "Val loss: 0.9835616566918113, Val accuracy: 0.6099837422370911\n",
            "Val loss: 0.976414566634023, Val accuracy: 0.6115376353263855\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.141394936121427, Val accuracy: 0.6201923489570618\n",
            "Val loss: 1.0843117590303775, Val accuracy: 0.5960648059844971\n",
            "Val loss: 1.046974975888322, Val accuracy: 0.6006097197532654\n",
            "Val loss: 1.0423402612859554, Val accuracy: 0.5965908765792847\n",
            "Val loss: 1.0334648299908293, Val accuracy: 0.5989583730697632\n",
            "\n",
            "starting Epoch 2\n",
            "Training...\n",
            "Train loss: 0.9599667651611462\n",
            "Train loss: 0.9491556530413421\n",
            "Train loss: 0.936260891443043\n",
            "Train loss: 0.930007086429761\n",
            "Train loss: 0.9196522149247695\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.8323987670112074, Val accuracy: 0.671326756477356\n",
            "Val loss: 0.8249732934910318, Val accuracy: 0.6642662882804871\n",
            "Val loss: 0.8157098034902804, Val accuracy: 0.6663655638694763\n",
            "Val loss: 0.8162646402012218, Val accuracy: 0.6678841710090637\n",
            "Val loss: 0.8155770303881292, Val accuracy: 0.6656574606895447\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.979505181312561, Val accuracy: 0.665865421295166\n",
            "Val loss: 0.9387970986189665, Val accuracy: 0.640625\n",
            "Val loss: 0.9110619789216576, Val accuracy: 0.644817054271698\n",
            "Val loss: 0.9030542135238647, Val accuracy: 0.6431818008422852\n",
            "Val loss: 0.897519083990567, Val accuracy: 0.645380437374115\n",
            "\n",
            "starting Epoch 3\n",
            "Training...\n",
            "Train loss: 0.8328348178612558\n",
            "Train loss: 0.8224019055781158\n",
            "Train loss: 0.8235710046194882\n",
            "Train loss: 0.8229717960605374\n",
            "Train loss: 0.8177632406921123\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.7475442739955166, Val accuracy: 0.6962719559669495\n",
            "Val loss: 0.7430247830308002, Val accuracy: 0.6934782266616821\n",
            "Val loss: 0.7380708490492981, Val accuracy: 0.6892160177230835\n",
            "Val loss: 0.7438416517142094, Val accuracy: 0.6828327775001526\n",
            "Val loss: 0.741204611157876, Val accuracy: 0.6821475028991699\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.9189778428811294, Val accuracy: 0.6742788553237915\n",
            "Val loss: 0.8817089111716659, Val accuracy: 0.6527777910232544\n",
            "Val loss: 0.868870652303463, Val accuracy: 0.6520578861236572\n",
            "Val loss: 0.8559690919789401, Val accuracy: 0.6505681872367859\n",
            "Val loss: 0.8517438536104949, Val accuracy: 0.6535326242446899\n",
            "\n",
            "starting Epoch 4\n",
            "Training...\n",
            "Train loss: 0.7520292474512469\n",
            "Train loss: 0.7295231072799019\n",
            "Train loss: 0.7253714093583168\n",
            "Train loss: 0.7179241590685659\n",
            "Train loss: 0.7199344818567323\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.6895075586804172, Val accuracy: 0.6998355388641357\n",
            "Val loss: 0.6769423718037813, Val accuracy: 0.699456512928009\n",
            "Val loss: 0.6749672116227232, Val accuracy: 0.698970377445221\n",
            "Val loss: 0.6749072024574527, Val accuracy: 0.698051929473877\n",
            "Val loss: 0.6739675689321076, Val accuracy: 0.6968533992767334\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.9249478395168598, Val accuracy: 0.6754807829856873\n",
            "Val loss: 0.876868259023737, Val accuracy: 0.6527777910232544\n",
            "Val loss: 0.8663471588274327, Val accuracy: 0.649009108543396\n",
            "Val loss: 0.8512744957750494, Val accuracy: 0.6497159004211426\n",
            "Val loss: 0.8442270487978838, Val accuracy: 0.65013587474823\n",
            "Пармеметры:\n",
            "\n",
            "#######\n",
            "\n",
            " длины фильтров: [2, 4]\n",
            "\n",
            "Слой, на котором применяется дропаут: 0\n",
            "\n",
            "starting Epoch 0\n",
            "Training...\n",
            "Train loss: 1.4986954718305354\n",
            "Train loss: 1.4701249412868334\n",
            "Train loss: 1.4471787951585184\n",
            "Train loss: 1.4189545901822838\n",
            "Train loss: 1.3684546867043914\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 1.0638011704411423, Val accuracy: 0.6121162176132202\n",
            "Val loss: 1.0662019543025805, Val accuracy: 0.5986412763595581\n",
            "Val loss: 1.0709031661810902, Val accuracy: 0.5899566411972046\n",
            "Val loss: 1.0606699113721971, Val accuracy: 0.5957792401313782\n",
            "Val loss: 1.0644582959607398, Val accuracy: 0.5910467505455017\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.1872559189796448, Val accuracy: 0.6165865659713745\n",
            "Val loss: 1.1249874300426908, Val accuracy: 0.59375\n",
            "Val loss: 1.0938137784236814, Val accuracy: 0.5941310524940491\n",
            "Val loss: 1.0897484508427706, Val accuracy: 0.5877840518951416\n",
            "Val loss: 1.0831614007120547, Val accuracy: 0.5894474983215332\n",
            "\n",
            "starting Epoch 1\n",
            "Training...\n",
            "Train loss: 1.1019560684237564\n",
            "Train loss: 1.0631571329158285\n",
            "Train loss: 1.0557164139830308\n",
            "Train loss: 1.0490151269611343\n",
            "Train loss: 1.046188206202431\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 1.0314403490016335, Val accuracy: 0.6049890518188477\n",
            "Val loss: 1.0237943234650986, Val accuracy: 0.6039401888847351\n",
            "Val loss: 1.0146100858732454, Val accuracy: 0.6027817726135254\n",
            "Val loss: 1.0133360177923592, Val accuracy: 0.6017992496490479\n",
            "Val loss: 1.0143644382796897, Val accuracy: 0.6006163954734802\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.1451589327592115, Val accuracy: 0.6045673489570618\n",
            "Val loss: 1.0776890494205333, Val accuracy: 0.5862268805503845\n",
            "Val loss: 1.0492570254860856, Val accuracy: 0.5918444991111755\n",
            "Val loss: 1.0453616012226452, Val accuracy: 0.5860795378684998\n",
            "Val loss: 1.037607477195021, Val accuracy: 0.5892210006713867\n",
            "\n",
            "starting Epoch 2\n",
            "Training...\n",
            "Train loss: 1.0389707193040012\n",
            "Train loss: 1.028374465652134\n",
            "Train loss: 1.0217152327471386\n",
            "Train loss: 1.0161694855917067\n",
            "Train loss: 1.008687216724079\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.9728297350699442, Val accuracy: 0.6332237124443054\n",
            "Val loss: 0.9606030464172364, Val accuracy: 0.6343749761581421\n",
            "Val loss: 0.9691825467727088, Val accuracy: 0.6248193383216858\n",
            "Val loss: 0.966643179133857, Val accuracy: 0.6245265007019043\n",
            "Val loss: 0.9646350914631748, Val accuracy: 0.6233239769935608\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.087947354866908, Val accuracy: 0.6274038553237915\n",
            "Val loss: 1.028477085961236, Val accuracy: 0.6099537014961243\n",
            "Val loss: 0.9987968162792485, Val accuracy: 0.6150914430618286\n",
            "Val loss: 0.9951492970640009, Val accuracy: 0.6073863506317139\n",
            "Val loss: 0.9870448077934376, Val accuracy: 0.6102808117866516\n",
            "\n",
            "starting Epoch 3\n",
            "Training...\n",
            "Train loss: 0.9771731464486373\n",
            "Train loss: 0.9707838794459467\n",
            "Train loss: 0.9662280382448538\n",
            "Train loss: 0.9586570902820274\n",
            "Train loss: 0.9517431525210608\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.8661061577629625, Val accuracy: 0.6485745906829834\n",
            "Val loss: 0.8606573156688524, Val accuracy: 0.6379075646400452\n",
            "Val loss: 0.8553786829027826, Val accuracy: 0.6392702460289001\n",
            "Val loss: 0.8471237322984836, Val accuracy: 0.6448187232017517\n",
            "Val loss: 0.8474901075181664, Val accuracy: 0.6434364318847656\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.9597731828689575, Val accuracy: 0.6574519276618958\n",
            "Val loss: 0.9212431907653809, Val accuracy: 0.6342592835426331\n",
            "Val loss: 0.899834394454956, Val accuracy: 0.6360518336296082\n",
            "Val loss: 0.9000976432453502, Val accuracy: 0.6295454502105713\n",
            "Val loss: 0.8905692065971486, Val accuracy: 0.6322463750839233\n",
            "\n",
            "starting Epoch 4\n",
            "Training...\n",
            "Train loss: 0.8727235637213054\n",
            "Train loss: 0.8627321813417518\n",
            "Train loss: 0.8525496781561416\n",
            "Train loss: 0.8420839344526266\n",
            "Train loss: 0.8401676512300762\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.8147148395839491, Val accuracy: 0.6699561476707458\n",
            "Val loss: 0.7908611831457718, Val accuracy: 0.6754075884819031\n",
            "Val loss: 0.789324109264881, Val accuracy: 0.6743135452270508\n",
            "Val loss: 0.7924822259258915, Val accuracy: 0.668695867061615\n",
            "Val loss: 0.7886232601730057, Val accuracy: 0.6697664856910706\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.9113752796099737, Val accuracy: 0.6850962042808533\n",
            "Val loss: 0.8823102028281601, Val accuracy: 0.6556712985038757\n",
            "Val loss: 0.8627106082148668, Val accuracy: 0.6577743887901306\n",
            "Val loss: 0.8601877212524414, Val accuracy: 0.653124988079071\n",
            "Val loss: 0.8482214281524437, Val accuracy: 0.6560235619544983\n",
            "Пармеметры:\n",
            "\n",
            "#######\n",
            "\n",
            " длины фильтров: [2, 4]\n",
            "\n",
            "Слой, на котором применяется дропаут: 1\n",
            "\n",
            "starting Epoch 0\n",
            "Training...\n",
            "Train loss: 1.4925164548974288\n",
            "Train loss: 1.4724424496940944\n",
            "Train loss: 1.4504722019151457\n",
            "Train loss: 1.4138394903827023\n",
            "Train loss: 1.3498806095453282\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 1.0767468517286736, Val accuracy: 0.5855263471603394\n",
            "Val loss: 1.0574175497759943, Val accuracy: 0.5868206024169922\n",
            "Val loss: 1.057035852374369, Val accuracy: 0.5854407548904419\n",
            "Val loss: 1.0582710287787698, Val accuracy: 0.5840097665786743\n",
            "Val loss: 1.0524047833825478, Val accuracy: 0.5862889289855957\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.1811676575587347, Val accuracy: 0.598557710647583\n",
            "Val loss: 1.1236661473910015, Val accuracy: 0.5752314925193787\n",
            "Val loss: 1.0904901260282935, Val accuracy: 0.5819359421730042\n",
            "Val loss: 1.088988586989316, Val accuracy: 0.5738636255264282\n",
            "Val loss: 1.0804513373236726, Val accuracy: 0.5763134360313416\n",
            "\n",
            "starting Epoch 1\n",
            "Training...\n",
            "Train loss: 1.0986617458494086\n",
            "Train loss: 1.0554908425911613\n",
            "Train loss: 1.0418738023394105\n",
            "Train loss: 1.0346222233462643\n",
            "Train loss: 1.024685100700616\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.9366232313607868, Val accuracy: 0.6340460777282715\n",
            "Val loss: 0.9328315143999846, Val accuracy: 0.626630425453186\n",
            "Val loss: 0.9302294443797514, Val accuracy: 0.6240065097808838\n",
            "Val loss: 0.936478162998761, Val accuracy: 0.6200622320175171\n",
            "Val loss: 0.9353181022673742, Val accuracy: 0.6203503608703613\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.0706513432356028, Val accuracy: 0.6274038553237915\n",
            "Val loss: 1.0305290553304884, Val accuracy: 0.6001157164573669\n",
            "Val loss: 1.0016113257989652, Val accuracy: 0.6051828861236572\n",
            "Val loss: 1.0006637020544573, Val accuracy: 0.6002840995788574\n",
            "Val loss: 0.9908557931582133, Val accuracy: 0.6034873127937317\n",
            "\n",
            "starting Epoch 2\n",
            "Training...\n",
            "Train loss: 0.9331094727181551\n",
            "Train loss: 0.8961750206740006\n",
            "Train loss: 0.8787379874659411\n",
            "Train loss: 0.8692794011268781\n",
            "Train loss: 0.8626465393185203\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.802200226407302, Val accuracy: 0.6707785129547119\n",
            "Val loss: 0.7992393405541129, Val accuracy: 0.663451075553894\n",
            "Val loss: 0.7968164355079562, Val accuracy: 0.6620303392410278\n",
            "Val loss: 0.7883688730078858, Val accuracy: 0.6651109457015991\n",
            "Val loss: 0.7845696095364316, Val accuracy: 0.6654952764511108\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.9349747896194458, Val accuracy: 0.671875\n",
            "Val loss: 0.914200155823319, Val accuracy: 0.6412037014961243\n",
            "Val loss: 0.8961402352263288, Val accuracy: 0.6417682766914368\n",
            "Val loss: 0.8935732581398704, Val accuracy: 0.6369317770004272\n",
            "Val loss: 0.8824035922686259, Val accuracy: 0.6417572498321533\n",
            "\n",
            "starting Epoch 3\n",
            "Training...\n",
            "Train loss: 0.7997942974692897\n",
            "Train loss: 0.8010140133940655\n",
            "Train loss: 0.7898823489688035\n",
            "Train loss: 0.7802095027455004\n",
            "Train loss: 0.7757700348601622\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.7268904159989273, Val accuracy: 0.6949013471603394\n",
            "Val loss: 0.7142103729040726, Val accuracy: 0.6902173757553101\n",
            "Val loss: 0.7070560708555872, Val accuracy: 0.6865968108177185\n",
            "Val loss: 0.7099433338745332, Val accuracy: 0.6841856241226196\n",
            "Val loss: 0.7061650170380681, Val accuracy: 0.684418261051178\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.8984026175278884, Val accuracy: 0.6947115659713745\n",
            "Val loss: 0.874500866289492, Val accuracy: 0.6597222089767456\n",
            "Val loss: 0.8539839837609268, Val accuracy: 0.6573932766914368\n",
            "Val loss: 0.8532766342163086, Val accuracy: 0.6519886255264282\n",
            "Val loss: 0.8396688881127731, Val accuracy: 0.6573822498321533\n",
            "\n",
            "starting Epoch 4\n",
            "Training...\n",
            "Train loss: 0.7538711606410512\n",
            "Train loss: 0.7232652741929759\n",
            "Train loss: 0.7351920513403898\n",
            "Train loss: 0.7301426687281886\n",
            "Train loss: 0.7280130130609427\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.6689813769700235, Val accuracy: 0.7061403393745422\n",
            "Val loss: 0.6643652192924334, Val accuracy: 0.6959238648414612\n",
            "Val loss: 0.6648857102573262, Val accuracy: 0.6964414715766907\n",
            "Val loss: 0.6648424214872963, Val accuracy: 0.6946699023246765\n",
            "Val loss: 0.662563250654709, Val accuracy: 0.6953936219215393\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.8768886740391071, Val accuracy: 0.6802884936332703\n",
            "Val loss: 0.8590753674507141, Val accuracy: 0.6516203880310059\n",
            "Val loss: 0.8356142451123494, Val accuracy: 0.6554877758026123\n",
            "Val loss: 0.834204020283439, Val accuracy: 0.6502840518951416\n",
            "Val loss: 0.8213559523008872, Val accuracy: 0.6542119979858398\n",
            "Пармеметры:\n",
            "\n",
            "#######\n",
            "\n",
            " длины фильтров: [3, 4]\n",
            "\n",
            "Слой, на котором применяется дропаут: 0\n",
            "\n",
            "starting Epoch 0\n",
            "Training...\n",
            "Train loss: 1.4912616181791873\n",
            "Train loss: 1.4211897881134696\n",
            "Train loss: 1.3174957102433795\n",
            "Train loss: 1.2526328254055668\n",
            "Train loss: 1.2107178119227135\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 1.0424546202023823, Val accuracy: 0.6137609481811523\n",
            "Val loss: 1.0403681418170099, Val accuracy: 0.604347825050354\n",
            "Val loss: 1.0446429728083528, Val accuracy: 0.5983561873435974\n",
            "Val loss: 1.0372162543850028, Val accuracy: 0.5998376607894897\n",
            "Val loss: 1.0341366207723386, Val accuracy: 0.6000216603279114\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.1505662294534535, Val accuracy: 0.6213942766189575\n",
            "Val loss: 1.0859648960608024, Val accuracy: 0.5972222089767456\n",
            "Val loss: 1.0568415842405179, Val accuracy: 0.6028963327407837\n",
            "Val loss: 1.0532912416891618, Val accuracy: 0.5968749523162842\n",
            "Val loss: 1.0491009829700857, Val accuracy: 0.5991848111152649\n",
            "\n",
            "starting Epoch 1\n",
            "Training...\n",
            "Train loss: 1.0448032920820671\n",
            "Train loss: 1.039640676456949\n",
            "Train loss: 1.0263067821546785\n",
            "Train loss: 1.0181738654256383\n",
            "Train loss: 1.0030612896057973\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.9911375286286337, Val accuracy: 0.6173245906829834\n",
            "Val loss: 0.9836631261784098, Val accuracy: 0.614402174949646\n",
            "Val loss: 0.9752702588979909, Val accuracy: 0.6123554706573486\n",
            "Val loss: 0.972408795253539, Val accuracy: 0.6132305264472961\n",
            "Val loss: 0.9711055732928345, Val accuracy: 0.6098616123199463\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.09328869672922, Val accuracy: 0.6225962042808533\n",
            "Val loss: 1.0335345290325306, Val accuracy: 0.5960648059844971\n",
            "Val loss: 1.0037942048979969, Val accuracy: 0.6017530560493469\n",
            "Val loss: 1.0015185963023792, Val accuracy: 0.5982954502105713\n",
            "Val loss: 0.9976722580799158, Val accuracy: 0.6000905632972717\n",
            "\n",
            "starting Epoch 2\n",
            "Training...\n",
            "Train loss: 1.0023900605084604\n",
            "Train loss: 0.9790345445923183\n",
            "Train loss: 0.965485901846362\n",
            "Train loss: 0.9560404236698563\n",
            "Train loss: 0.9427162561449625\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.8651202383794283, Val accuracy: 0.6589912176132202\n",
            "Val loss: 0.8533474626748458, Val accuracy: 0.6547554135322571\n",
            "Val loss: 0.8511851023387358, Val accuracy: 0.6547145843505859\n",
            "Val loss: 0.85142005676831, Val accuracy: 0.6508387327194214\n",
            "Val loss: 0.8558791552035454, Val accuracy: 0.6468966603279114\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.9750392299432021, Val accuracy: 0.65625\n",
            "Val loss: 0.9369067328947561, Val accuracy: 0.6307870149612427\n",
            "Val loss: 0.9155780934705967, Val accuracy: 0.6310975551605225\n",
            "Val loss: 0.9119783087210221, Val accuracy: 0.6281249523162842\n",
            "Val loss: 0.9016282264737115, Val accuracy: 0.6338315606117249\n",
            "\n",
            "starting Epoch 3\n",
            "Training...\n",
            "Train loss: 0.8748902793516192\n",
            "Train loss: 0.8610041089679884\n",
            "Train loss: 0.8629439524832488\n",
            "Train loss: 0.8645478182540828\n",
            "Train loss: 0.8677048184054945\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.8209208647410074, Val accuracy: 0.6658443212509155\n",
            "Val loss: 0.8250555219857589, Val accuracy: 0.6603260636329651\n",
            "Val loss: 0.821827870228387, Val accuracy: 0.6576047539710999\n",
            "Val loss: 0.8172047171757851, Val accuracy: 0.65625\n",
            "Val loss: 0.8132055744167843, Val accuracy: 0.65625\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.9369237377093389, Val accuracy: 0.6622596383094788\n",
            "Val loss: 0.9016687119448626, Val accuracy: 0.6412037014961243\n",
            "Val loss: 0.8817841032656227, Val accuracy: 0.6421493887901306\n",
            "Val loss: 0.8778529427268288, Val accuracy: 0.6392045021057129\n",
            "Val loss: 0.866683846798496, Val accuracy: 0.6447011232376099\n",
            "\n",
            "starting Epoch 4\n",
            "Training...\n",
            "Train loss: 0.8367816429389151\n",
            "Train loss: 0.8194984233897665\n",
            "Train loss: 0.8244862828640579\n",
            "Train loss: 0.8239110579222312\n",
            "Train loss: 0.8248929802109214\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.786812026249735, Val accuracy: 0.671326756477356\n",
            "Val loss: 0.7869387471157572, Val accuracy: 0.66236412525177\n",
            "Val loss: 0.7769166985688182, Val accuracy: 0.6653720736503601\n",
            "Val loss: 0.7708575862310666, Val accuracy: 0.6673430800437927\n",
            "Val loss: 0.7681573478820827, Val accuracy: 0.6679282188415527\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.8831990819710952, Val accuracy: 0.6887019276618958\n",
            "Val loss: 0.8608592748641968, Val accuracy: 0.6545138955116272\n",
            "Val loss: 0.8447151242232904, Val accuracy: 0.65625\n",
            "Val loss: 0.8388426217165861, Val accuracy: 0.6528409123420715\n",
            "Val loss: 0.8306230608967767, Val accuracy: 0.6557971239089966\n",
            "Пармеметры:\n",
            "\n",
            "#######\n",
            "\n",
            " длины фильтров: [3, 4]\n",
            "\n",
            "Слой, на котором применяется дропаут: 1\n",
            "\n",
            "starting Epoch 0\n",
            "Training...\n",
            "Train loss: 1.486900017972578\n",
            "Train loss: 1.471598796222521\n",
            "Train loss: 1.406236116596729\n",
            "Train loss: 1.3275389534570439\n",
            "Train loss: 1.2696993324170889\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 1.0233022643808733, Val accuracy: 0.6107456088066101\n",
            "Val loss: 1.0075125492137411, Val accuracy: 0.609918475151062\n",
            "Val loss: 1.0042609410478889, Val accuracy: 0.6094653010368347\n",
            "Val loss: 1.0093022195291725, Val accuracy: 0.6074134111404419\n",
            "Val loss: 1.009713135582353, Val accuracy: 0.6080774664878845\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.155648960517003, Val accuracy: 0.6177884936332703\n",
            "Val loss: 1.0923846253642329, Val accuracy: 0.5931712985038757\n",
            "Val loss: 1.0577213865954702, Val accuracy: 0.602134108543396\n",
            "Val loss: 1.0519903952425176, Val accuracy: 0.5951704382896423\n",
            "Val loss: 1.043712451838065, Val accuracy: 0.5975996255874634\n",
            "\n",
            "starting Epoch 1\n",
            "Training...\n",
            "Train loss: 1.0093144199304414\n",
            "Train loss: 0.9883200992708621\n",
            "Train loss: 0.9828223865156229\n",
            "Train loss: 0.9722794286616436\n",
            "Train loss: 0.9609169914648195\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.9083844078214545, Val accuracy: 0.6515899300575256\n",
            "Val loss: 0.9155795382416767, Val accuracy: 0.6394021511077881\n",
            "Val loss: 0.9225099028190437, Val accuracy: 0.6366509795188904\n",
            "Val loss: 0.9171777246318338, Val accuracy: 0.6378517150878906\n",
            "Val loss: 0.9136900004631096, Val accuracy: 0.6401925086975098\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.0590407664959247, Val accuracy: 0.6442307829856873\n",
            "Val loss: 1.0084224387451455, Val accuracy: 0.6226851940155029\n",
            "Val loss: 0.9841948355116495, Val accuracy: 0.6211889982223511\n",
            "Val loss: 0.9777320341630416, Val accuracy: 0.6178976893424988\n",
            "Val loss: 0.970016814660335, Val accuracy: 0.6204710006713867\n",
            "\n",
            "starting Epoch 2\n",
            "Training...\n",
            "Train loss: 0.8557534406059667\n",
            "Train loss: 0.8358684218448141\n",
            "Train loss: 0.8269490823580351\n",
            "Train loss: 0.8196702565981712\n",
            "Train loss: 0.8115998754039355\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.7382596563874629, Val accuracy: 0.6927083134651184\n",
            "Val loss: 0.72677444945211, Val accuracy: 0.6876358389854431\n",
            "Val loss: 0.7319131090461863, Val accuracy: 0.6832550168037415\n",
            "Val loss: 0.7316588024040321, Val accuracy: 0.682494580745697\n",
            "Val loss: 0.7288921058796681, Val accuracy: 0.6827422380447388\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.8878382169283353, Val accuracy: 0.686298131942749\n",
            "Val loss: 0.8605388447090432, Val accuracy: 0.6608796119689941\n",
            "Val loss: 0.8399960529513475, Val accuracy: 0.65625\n",
            "Val loss: 0.8328542286699468, Val accuracy: 0.653124988079071\n",
            "Val loss: 0.8228970195936121, Val accuracy: 0.654891312122345\n",
            "\n",
            "starting Epoch 3\n",
            "Training...\n",
            "Train loss: 0.792722529486606\n",
            "Train loss: 0.7465620367423348\n",
            "Train loss: 0.7409657222687165\n",
            "Train loss: 0.7344544989206058\n",
            "Train loss: 0.7352275037848001\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.695228491958819, Val accuracy: 0.6954495906829834\n",
            "Val loss: 0.6946096342542897, Val accuracy: 0.6956521272659302\n",
            "Val loss: 0.6957958604214508, Val accuracy: 0.6930996775627136\n",
            "Val loss: 0.6996248788905867, Val accuracy: 0.6892586350440979\n",
            "Val loss: 0.6992645219329319, Val accuracy: 0.6901492476463318\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.9125453417117779, Val accuracy: 0.6850962042808533\n",
            "Val loss: 0.8730321791436937, Val accuracy: 0.6620370149612427\n",
            "Val loss: 0.8592368887691963, Val accuracy: 0.6547256112098694\n",
            "Val loss: 0.8580365961248224, Val accuracy: 0.6499999761581421\n",
            "Val loss: 0.8445932726929153, Val accuracy: 0.6524003744125366\n",
            "\n",
            "starting Epoch 4\n",
            "Training...\n",
            "Train loss: 0.7056357065836588\n",
            "Train loss: 0.6837678137032882\n",
            "Train loss: 0.6703516875388306\n",
            "Train loss: 0.669110576569776\n",
            "Train loss: 0.671568493529587\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.6629837761845505, Val accuracy: 0.7124451994895935\n",
            "Val loss: 0.6524640459081401, Val accuracy: 0.7029891014099121\n",
            "Val loss: 0.6461884735981164, Val accuracy: 0.7030346393585205\n",
            "Val loss: 0.6428122743641659, Val accuracy: 0.7046807408332825\n",
            "Val loss: 0.6421488169774052, Val accuracy: 0.7033953666687012\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.8925596888248737, Val accuracy: 0.6995192766189575\n",
            "Val loss: 0.8642178729728416, Val accuracy: 0.6689814925193787\n",
            "Val loss: 0.8388417886524666, Val accuracy: 0.6642529964447021\n",
            "Val loss: 0.8290486671707847, Val accuracy: 0.6624999642372131\n",
            "Val loss: 0.817193147064983, Val accuracy: 0.6653079986572266\n",
            "Пармеметры:\n",
            "\n",
            "#######\n",
            "\n",
            " длины фильтров: [2, 3, 4]\n",
            "\n",
            "Слой, на котором применяется дропаут: 0\n",
            "\n",
            "starting Epoch 0\n",
            "Training...\n",
            "Train loss: 1.4858731470609967\n",
            "Train loss: 1.449369686582814\n",
            "Train loss: 1.432830281340318\n",
            "Train loss: 1.4277290730249315\n",
            "Train loss: 1.4142239267025851\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 1.3565395443063033, Val accuracy: 0.5205591917037964\n",
            "Val loss: 1.3371831520743993, Val accuracy: 0.5188858509063721\n",
            "Val loss: 1.3405122591580958, Val accuracy: 0.5118316411972046\n",
            "Val loss: 1.345175845798476, Val accuracy: 0.5085227489471436\n",
            "Val loss: 1.3393208811439858, Val accuracy: 0.5095155835151672\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.4685494532951942, Val accuracy: 0.525240421295166\n",
            "Val loss: 1.3998030909785517, Val accuracy: 0.5121527910232544\n",
            "Val loss: 1.3688983044973233, Val accuracy: 0.515625\n",
            "Val loss: 1.3612293070012873, Val accuracy: 0.5082386136054993\n",
            "Val loss: 1.3511744979499043, Val accuracy: 0.5097373127937317\n",
            "\n",
            "starting Epoch 1\n",
            "Training...\n",
            "Train loss: 1.3889462696878534\n",
            "Train loss: 1.3786438081575476\n",
            "Train loss: 1.3755534963111657\n",
            "Train loss: 1.3218024672367872\n",
            "Train loss: 1.2763578714383927\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 1.0821276265278197, Val accuracy: 0.5789473652839661\n",
            "Val loss: 1.0799770168636156, Val accuracy: 0.5714673399925232\n",
            "Val loss: 1.0739115900386964, Val accuracy: 0.5708995461463928\n",
            "Val loss: 1.0748848050703734, Val accuracy: 0.5702110528945923\n",
            "Val loss: 1.071263423427991, Val accuracy: 0.571961522102356\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.1962031859617968, Val accuracy: 0.5757212042808533\n",
            "Val loss: 1.1290534094527915, Val accuracy: 0.5596064925193787\n",
            "Val loss: 1.0967827889977433, Val accuracy: 0.5621188879013062\n",
            "Val loss: 1.0913090760057622, Val accuracy: 0.5582386255264282\n",
            "Val loss: 1.083097579686538, Val accuracy: 0.5627264380455017\n",
            "\n",
            "starting Epoch 2\n",
            "Training...\n",
            "Train loss: 1.0914447077533656\n",
            "Train loss: 1.0686596310657004\n",
            "Train loss: 1.0628538159276708\n",
            "Train loss: 1.052878113020034\n",
            "Train loss: 1.0517541093809795\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.990310972197014, Val accuracy: 0.6334978342056274\n",
            "Val loss: 1.006828262494958, Val accuracy: 0.6195651888847351\n",
            "Val loss: 1.0140415657462412, Val accuracy: 0.6116329431533813\n",
            "Val loss: 1.0143994103262435, Val accuracy: 0.6098484992980957\n",
            "Val loss: 1.0173122746721683, Val accuracy: 0.6058067083358765\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.1415137373484099, Val accuracy: 0.6262019276618958\n",
            "Val loss: 1.082882123964804, Val accuracy: 0.5995370149612427\n",
            "Val loss: 1.0493413689659863, Val accuracy: 0.6051828861236572\n",
            "Val loss: 1.0449742848222905, Val accuracy: 0.5999999642372131\n",
            "Val loss: 1.0386039882466414, Val accuracy: 0.601902186870575\n",
            "\n",
            "starting Epoch 3\n",
            "Training...\n",
            "Train loss: 1.0239425113326626\n",
            "Train loss: 1.0254172366598377\n",
            "Train loss: 1.0246859475367331\n",
            "Train loss: 1.0213671594987184\n",
            "Train loss: 1.0236103924500488\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.9990035159546032, Val accuracy: 0.6233552694320679\n",
            "Val loss: 0.9927312778389972, Val accuracy: 0.616304337978363\n",
            "Val loss: 0.9946402514601029, Val accuracy: 0.6113619804382324\n",
            "Val loss: 0.9946164161095887, Val accuracy: 0.611201286315918\n",
            "Val loss: 0.9900111054879159, Val accuracy: 0.6114835739135742\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.1242534518241882, Val accuracy: 0.6153846383094788\n",
            "Val loss: 1.0644805343062789, Val accuracy: 0.5972222089767456\n",
            "Val loss: 1.0330288104894685, Val accuracy: 0.6036584973335266\n",
            "Val loss: 1.0289421103217384, Val accuracy: 0.5985795259475708\n",
            "Val loss: 1.0198210400083791, Val accuracy: 0.6009963750839233\n",
            "\n",
            "starting Epoch 4\n",
            "Training...\n",
            "Train loss: 1.0029314285830448\n",
            "Train loss: 0.9959103299223858\n",
            "Train loss: 0.9966033597212995\n",
            "Train loss: 1.000153217738841\n",
            "Train loss: 0.9911602859266077\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.9753513221155133, Val accuracy: 0.6277412176132202\n",
            "Val loss: 0.9590710178665492, Val accuracy: 0.626902163028717\n",
            "Val loss: 0.9546030635778615, Val accuracy: 0.6243677735328674\n",
            "Val loss: 0.9439417541801155, Val accuracy: 0.6272321343421936\n",
            "Val loss: 0.9437560521607581, Val accuracy: 0.6279195547103882\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.0731998773721547, Val accuracy: 0.645432710647583\n",
            "Val loss: 1.035959921501301, Val accuracy: 0.6197916865348816\n",
            "Val loss: 1.0036580795195045, Val accuracy: 0.6219512224197388\n",
            "Val loss: 0.9946843190626664, Val accuracy: 0.6190340518951416\n",
            "Val loss: 0.9856853977493618, Val accuracy: 0.6204710006713867\n",
            "Пармеметры:\n",
            "\n",
            "#######\n",
            "\n",
            " длины фильтров: [2, 3, 4]\n",
            "\n",
            "Слой, на котором применяется дропаут: 0\n",
            "\n",
            "starting Epoch 0\n",
            "Training...\n",
            "Train loss: 1.505866814077946\n",
            "Train loss: 1.473098113225854\n",
            "Train loss: 1.468383354258675\n",
            "Train loss: 1.459459373445222\n",
            "Train loss: 1.4515107766979707\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 1.4157003603483502, Val accuracy: 0.5180920958518982\n",
            "Val loss: 1.4036384769107983, Val accuracy: 0.5141304135322571\n",
            "Val loss: 1.3971939927580728, Val accuracy: 0.513818621635437\n",
            "Val loss: 1.396938120131885, Val accuracy: 0.5127164721488953\n",
            "Val loss: 1.3942811938718116, Val accuracy: 0.5131379961967468\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.5193098508394682, Val accuracy: 0.5348557829856873\n",
            "Val loss: 1.4546598902455083, Val accuracy: 0.5173611044883728\n",
            "Val loss: 1.4235710109152444, Val accuracy: 0.5201981663703918\n",
            "Val loss: 1.414859086816961, Val accuracy: 0.5130681395530701\n",
            "Val loss: 1.4079353135565054, Val accuracy: 0.5149456858634949\n",
            "\n",
            "starting Epoch 1\n",
            "Training...\n",
            "Train loss: 1.3857155536350452\n",
            "Train loss: 1.3808250987011454\n",
            "Train loss: 1.3662482734360446\n",
            "Train loss: 1.3547453684208197\n",
            "Train loss: 1.327320065052864\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 1.1339933527143378, Val accuracy: 0.5904605388641357\n",
            "Val loss: 1.1181835646214693, Val accuracy: 0.588179349899292\n",
            "Val loss: 1.1106073456692558, Val accuracy: 0.5855310559272766\n",
            "Val loss: 1.1071694212042409, Val accuracy: 0.5848890542984009\n",
            "Val loss: 1.106686659750229, Val accuracy: 0.5836397409439087\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.2287789308107817, Val accuracy: 0.5913462042808533\n",
            "Val loss: 1.1689600238093623, Val accuracy: 0.5723379850387573\n",
            "Val loss: 1.1389974166707295, Val accuracy: 0.578125\n",
            "Val loss: 1.13482727245851, Val accuracy: 0.5721590518951416\n",
            "Val loss: 1.1252336847609368, Val accuracy: 0.5772191882133484\n",
            "\n",
            "starting Epoch 2\n",
            "Training...\n",
            "Train loss: 1.1430115720681977\n",
            "Train loss: 1.108863778217979\n",
            "Train loss: 1.0845668991177069\n",
            "Train loss: 1.076064878211909\n",
            "Train loss: 1.073111888034121\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 1.0106444181057446, Val accuracy: 0.6123903393745422\n",
            "Val loss: 1.01207528995431, Val accuracy: 0.606249988079071\n",
            "Val loss: 1.016551561093744, Val accuracy: 0.6007044911384583\n",
            "Val loss: 1.014443641101127, Val accuracy: 0.5975378751754761\n",
            "Val loss: 1.01742844433108, Val accuracy: 0.5958585739135742\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.1544206279974718, Val accuracy: 0.5925480723381042\n",
            "Val loss: 1.0916391655250832, Val accuracy: 0.5775462985038757\n",
            "Val loss: 1.0551907609148723, Val accuracy: 0.5834603309631348\n",
            "Val loss: 1.0508501854809849, Val accuracy: 0.578693151473999\n",
            "Val loss: 1.0450845196627188, Val accuracy: 0.5826539993286133\n",
            "\n",
            "starting Epoch 3\n",
            "Training...\n",
            "Train loss: 1.0367177358844823\n",
            "Train loss: 1.023348739872808\n",
            "Train loss: 1.0245884908417058\n",
            "Train loss: 1.0227135120532214\n",
            "Train loss: 1.0242836533120758\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.9670991719814769, Val accuracy: 0.624451756477356\n",
            "Val loss: 0.9773230526758276, Val accuracy: 0.6158967018127441\n",
            "Val loss: 0.9757752738936099, Val accuracy: 0.6132586598396301\n",
            "Val loss: 0.976315449842643, Val accuracy: 0.6101866960525513\n",
            "Val loss: 0.9759038443383874, Val accuracy: 0.6093209385871887\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.0969526309233446, Val accuracy: 0.6153846383094788\n",
            "Val loss: 1.0469865865177579, Val accuracy: 0.5983796119689941\n",
            "Val loss: 1.018561403925826, Val accuracy: 0.6017530560493469\n",
            "Val loss: 1.0148306163874539, Val accuracy: 0.5980113744735718\n",
            "Val loss: 1.0077596695526787, Val accuracy: 0.6005434989929199\n",
            "\n",
            "starting Epoch 4\n",
            "Training...\n",
            "Train loss: 0.9851161450670477\n",
            "Train loss: 0.9888546373533166\n",
            "Train loss: 0.990366290070418\n",
            "Train loss: 0.9888930524582471\n",
            "Train loss: 0.9822613832034867\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.946027490130642, Val accuracy: 0.6211622953414917\n",
            "Val loss: 0.9376983544100885, Val accuracy: 0.6203804016113281\n",
            "Val loss: 0.9332173611387352, Val accuracy: 0.6196712255477905\n",
            "Val loss: 0.9292672211989695, Val accuracy: 0.6188446879386902\n",
            "Val loss: 0.9275054791394401, Val accuracy: 0.6177011728286743\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.047694742679596, Val accuracy: 0.6322115659713745\n",
            "Val loss: 1.0126830560189706, Val accuracy: 0.5983796119689941\n",
            "Val loss: 0.9789420060995149, Val accuracy: 0.6040396094322205\n",
            "Val loss: 0.9755173802375794, Val accuracy: 0.6017045378684998\n",
            "Val loss: 0.9715747695038284, Val accuracy: 0.60326087474823\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Выводы\n",
        "Так как эксперимент очень большой, я решила запустить всего по 5 эпох, чтобы колаб меня не забанил "
      ],
      "metadata": {
        "id": "Hy3_NKeSpyUw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Исходя из результатов эксперимента:\n",
        "\n",
        "1) лучше всего справилась модель с одним Conv1d слоем, где длина фильтра равна четырем, и с дропаутом перед линейным слоем;\n",
        "\n",
        "2) в среднем лучше справлялись модели с двумя Conv1d слоями;\n",
        "\n",
        "3) модели с дропаутом перед линейным слоем работают лучше, чем модели с дропаутом после эмбеддинг-слоя"
      ],
      "metadata": {
        "id": "YsnhNIXluVK1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "u2y1PrFuuTaw"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "HW3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}